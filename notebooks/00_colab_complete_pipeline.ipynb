{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tathan08/review-rater/blob/main/notebooks/00_colab_complete_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6da5986",
      "metadata": {
        "id": "c6da5986"
      },
      "source": [
        "# Pipeline Architecture 2.0 - Training Phase\n",
        "\n",
        "## Data Flow Overview\n",
        "\n",
        "```\n",
        "data/raw ‚Üí (external processing) ‚Üí data/clean\n",
        "data/clean ‚Üí (00_ipynb + gemini) ‚Üí data/pseudo-label  \n",
        "data/pseudo-label ‚Üí data/testing + data/training\n",
        "data/clean ‚Üí data/training (combined)\n",
        "HuggingFace model trained on data/training with feedback loop against gemini\n",
        "Trained models ‚Üí models/saved_models\n",
        "```\n",
        "\n",
        "## Directory Structure\n",
        "\n",
        "### Data Directories\n",
        "- **`data/raw`**: Raw input data (processed externally)\n",
        "- **`data/clean`**: Cleaned/processed data from data/raw\n",
        "- **`data/pseudo-label`**: Pseudo-labeled data generated by Gemini from data/clean\n",
        "- **`data/training`**: Training data (combination of data/clean + data/pseudo-label)\n",
        "- **`data/testing`**: Testing data split from data/pseudo-label\n",
        "- **`data/actual`**: Production data for inference (used by 01_inference_pipeline.ipynb)\n",
        "\n",
        "### Model Directories\n",
        "- **`models/saved_models`**: Trained models ready for production\n",
        "- **`models/cache`**: Model cache files\n",
        "\n",
        "### Results Directories\n",
        "- **`results/predictions`**: Training predictions and evaluations\n",
        "- **`results/inference`**: Production inference results\n",
        "\n",
        "## Pipeline Components\n",
        "\n",
        "### Training Phase (This Notebook - 00_ipynb)\n",
        "1. **Environment Setup**: Install packages, configure GPU\n",
        "2. **Data Processing**: Create directory structure, load sample data\n",
        "3. **Gemini Pseudo-Labeling**: Generate high-quality labels for training\n",
        "4. **HuggingFace Training**: Train models with feedback loop against Gemini\n",
        "5. **Model Export**: Save trained models to models/saved_models\n",
        "\n",
        "### Inference Phase (01_ipynb)\n",
        "1. **Load Trained Models**: From models/saved_models\n",
        "2. **Process Production Data**: From data/actual\n",
        "3. **Generate Predictions**: Using trained pipeline\n",
        "4. **Save Results**: To results/inference\n",
        "\n",
        "## Integration Points\n",
        "- **Spam Detection**: Pipeline ready for spam detection model integration\n",
        "- **Feedback Loop**: HuggingFace model iteratively improved against Gemini predictions\n",
        "- **Production Ready**: Complete separation of training and inference phases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b975af5e",
      "metadata": {
        "id": "b975af5e"
      },
      "source": [
        "# Review Classification Pipeline - Complete Google Colab Implementation\n",
        "\n",
        "This notebook implements the complete review classification pipeline for detecting Google review policy violations, fully configured for Google Colab.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "### Phase 1: Environment Setup and Data Structure\n",
        "- Install all required packages (transformers, torch, google-generativeai, etc.)\n",
        "- Create proper directory structure (data/clean, data/pseudo-label, etc.)\n",
        "- Load sample data for demonstration\n",
        "\n",
        "### Phase 2: Core Pipeline Components\n",
        "- **Ollama Pipeline**: Local LLM classification (for reference, not runnable in Colab)\n",
        "- **HuggingFace Pipeline**: Zero-shot classification using pre-trained models\n",
        "- **Gemini Pseudo-Labeling**: High-quality label generation for training data\n",
        "- **Ensemble Method**: Combines multiple approaches for best results\n",
        "\n",
        "### Phase 3: Future Spam Detection Integration\n",
        "- Pipeline output will be piped into a spam detection model\n",
        "- Structured JSON output format for downstream processing\n",
        "- Confidence scoring for reliable filtering\n",
        "\n",
        "### Phase 4: Evaluation and Analysis\n",
        "- Comprehensive performance metrics\n",
        "- Policy category accuracy assessment\n",
        "- Model comparison and improvement recommendations\n",
        "\n",
        "**Key Features:**\n",
        "- **Policy Categories**: No_Ads, Irrelevant, Rant_No_Visit detection\n",
        "- **Zero Setup**: Everything configured for Google Colab\n",
        "- **Extensible**: Ready for spam detection integration\n",
        "- **Production Ready**: Structured output and comprehensive evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5462d932",
      "metadata": {
        "id": "5462d932"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f35169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96f35169",
        "outputId": "56d0a3d5-8f95-402a-c056-d6adb529d84e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for the complete pipeline\n",
        "!pip install -q transformers==4.43.3 torch pandas scikit-learn\n",
        "!pip install -q google-generativeai tqdm datasets accelerate\n",
        "!pip install -q ipywidgets matplotlib seaborn wordcloud\n",
        "\n",
        "print(\"‚úÖ Core packages installed successfully!\")\n",
        "\n",
        "# Check GPU availability and setup device\n",
        "import torch\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"Using CPU - models will run slower but still functional\")\n",
        "\n",
        "# Set environment for optimal performance\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid warnings\n",
        "print(\"Environment configured for optimal performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d42837",
      "metadata": {
        "id": "77d42837"
      },
      "source": [
        "## 2. Project Structure Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0558d7e",
      "metadata": {
        "id": "c0558d7e"
      },
      "outputs": [],
      "source": [
        "# Create complete directory structure matching the actual pipeline\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create all necessary directories (matching actual pipeline structure)\n",
        "directories = [\n",
        "    # Source code structure\n",
        "    'src/config', 'src/core', 'src/pseudo_labelling', 'src/pipeline', 'src/integration',\n",
        "\n",
        "    # Data directories (matching actual structure)\n",
        "    'data/raw',           # For raw input data\n",
        "    'data/clean',         # For cleaned/processed data (from data/raw)\n",
        "    'data/pseudo-label',  # For pseudo-labeled data from Gemini (from data/clean)\n",
        "    'data/training',      # For training data split (from data/clean + data/pseudo-label)\n",
        "    'data/testing',       # For testing data split (from data/pseudo-label)\n",
        "    'data/actual',        # For actual production data to be processed by 01_inference_pipeline.ipynb\n",
        "    'data/sample',        # For sample data\n",
        "\n",
        "    # Results directories\n",
        "    'results/predictions',   # All predictions\n",
        "    'results/evaluations',   # For evaluation results\n",
        "    'results/reports',       # For generated reports\n",
        "\n",
        "    # Other directories\n",
        "    'models/saved_models',   # For trained models\n",
        "    'models/cache',          # For model cache\n",
        "    'logs/pipeline_logs',    # For pipeline logs\n",
        "    'prompts',               # Prompt engineering\n",
        "    'docs'                   # Documentation\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    # Create __init__.py files for Python packages\n",
        "    if directory.startswith('src/'):\n",
        "        with open(f'{directory}/__init__.py', 'w') as f:\n",
        "            f.write('# Review Classification Pipeline Package\\n')\n",
        "\n",
        "print(\"‚úÖ Complete directory structure created!\")\n",
        "print(f\"Created {len(directories)} directories\")\n",
        "\n",
        "# Verify critical directories exist\n",
        "critical_dirs = ['data/clean', 'data/pseudo-label', 'data/sample', 'results/predictions']\n",
        "for dir_name in critical_dirs:\n",
        "    if os.path.exists(dir_name):\n",
        "        print(f\"‚úÖ {dir_name}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {dir_name} - MISSING!\")\n",
        "\n",
        "print(\"\\nDirectory structure matches production pipeline!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc18cdbb",
      "metadata": {
        "id": "bc18cdbb"
      },
      "source": [
        "## 3. Sample Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8856980d",
      "metadata": {
        "id": "8856980d"
      },
      "outputs": [],
      "source": [
        "# Load actual sample data from the production pipeline\n",
        "sample_data = {\n",
        "    'id': [1, 2, 3, 4, 5],\n",
        "    'text': [\n",
        "        \"Use my promo code EAT10 for 10% off! DM me on WhatsApp.\",\n",
        "        \"Great laksa; broth was rich and staff friendly. Will return.\",\n",
        "        \"Crypto is the future. Buy BTC now! Nothing to do with this cafe.\",\n",
        "        \"Overpriced scammers. Society is doomed.\",\n",
        "        \"Visited on 18 Aug, ordered set A; cashier fixed a double-charge.\"\n",
        "    ],\n",
        "    'gold_label': ['REJECT', 'APPROVE', 'REJECT', 'REJECT', 'APPROVE'],\n",
        "    'gold_category': ['No_Ads', 'None', 'Irrelevant', 'Rant_No_Visit', 'None']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "df.to_csv('data/sample/sample_reviews.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Production sample data loaded!\")\n",
        "print(\"\\nSample Data Overview:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "print(f\"APPROVE: {len(df[df['gold_label'] == 'APPROVE'])} reviews\")\n",
        "print(f\"REJECT:  {len(df[df['gold_label'] == 'REJECT'])} reviews\")\n",
        "\n",
        "print(f\"\\nCategory Distribution:\")\n",
        "for category in df['gold_category'].value_counts().index:\n",
        "    count = df['gold_category'].value_counts()[category]\n",
        "    print(f\"{category}: {count} reviews\")\n",
        "\n",
        "print(f\"\\nThis data demonstrates all policy violation types:\")\n",
        "print(\"‚Ä¢ No_Ads: Promotional codes and contact solicitation\")\n",
        "print(\"‚Ä¢ Irrelevant: Off-topic content unrelated to business\")\n",
        "print(\"‚Ä¢ Rant_No_Visit: Generic negative comments without visit evidence\")\n",
        "print(\"‚Ä¢ None: Legitimate reviews that should be approved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1014facd",
      "metadata": {
        "id": "1014facd"
      },
      "source": [
        "## 4. Configuration Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a97baccb",
      "metadata": {
        "id": "a97baccb"
      },
      "outputs": [],
      "source": [
        "# Create configuration classes matching the actual pipeline\n",
        "config_code = '''\n",
        "\"\"\"\n",
        "Pipeline Configuration Classes - Matching Production Structure\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for model settings\"\"\"\n",
        "    # HuggingFace models (matching actual pipeline)\n",
        "    hf_sentiment_model: str = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "    hf_toxicity_model: str = \"unitary/toxic-bert\"\n",
        "    hf_zero_shot_model: str = \"facebook/bart-large-mnli\"\n",
        "\n",
        "    # Gemini configuration\n",
        "    gemini_model: str = \"gemini-2.5-flash-lite\"\n",
        "\n",
        "    # Confidence thresholds (matching actual pipeline)\n",
        "    sentiment_threshold: float = 0.7\n",
        "    toxicity_threshold: float = 0.5\n",
        "    zero_shot_threshold: float = 0.7\n",
        "    ensemble_tau: float = 0.55\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    \"\"\"Configuration for data paths and settings\"\"\"\n",
        "    data_dir: str = \"data\"\n",
        "    raw_data_dir: str = \"data/raw\"\n",
        "    processed_data_dir: str = \"data/clean\"  # Matches actual structure\n",
        "    sample_data_dir: str = \"data/sample\"\n",
        "    pseudo_label_dir: str = \"data/pseudo-label\"  # Matches actual structure\n",
        "    training_dir: str = \"data/training\"\n",
        "    testing_dir: str = \"data/testing\"\n",
        "\n",
        "    # Default input file\n",
        "    sample_reviews_file: str = \"data/sample/sample_reviews.csv\"\n",
        "\n",
        "@dataclass\n",
        "class OutputConfig:\n",
        "    \"\"\"Configuration for output paths\"\"\"\n",
        "    results_dir: str = \"results\"\n",
        "    predictions_dir: str = \"results/predictions\"\n",
        "    evaluations_dir: str = \"results/evaluations\"\n",
        "    reports_dir: str = \"results/reports\"\n",
        "\n",
        "    # Default output files (matching actual pipeline)\n",
        "    hf_predictions: str = \"results/predictions/predictions_hf.csv\"\n",
        "    ensemble_predictions: str = \"results/predictions/predictions_ens.csv\"\n",
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    \"\"\"Main pipeline configuration combining all components\"\"\"\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    output: OutputConfig = field(default_factory=OutputConfig)\n",
        "\n",
        "    # Gemini configuration\n",
        "    gemini_api_key: str = \"\"\n",
        "\n",
        "    # Pipeline settings\n",
        "    batch_size: int = 32\n",
        "    max_workers: int = 4\n",
        "    cache_predictions: bool = True\n",
        "    verbose_logging: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create directories if they don't exist\"\"\"\n",
        "        directories = [\n",
        "            self.data.raw_data_dir,\n",
        "            self.data.processed_data_dir,\n",
        "            self.data.sample_data_dir,\n",
        "            self.data.pseudo_label_dir,\n",
        "            self.data.training_dir,\n",
        "            self.data.testing_dir,\n",
        "            self.output.predictions_dir,\n",
        "            self.output.evaluations_dir,\n",
        "            self.output.reports_dir\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Global configuration instance\n",
        "config = PipelineConfig()\n",
        "'''\n",
        "\n",
        "with open('src/config/pipeline_config.py', 'w') as f:\n",
        "    f.write(config_code)\n",
        "\n",
        "print(\"‚úÖ Configuration created matching production pipeline!\")\n",
        "\n",
        "# Test configuration\n",
        "exec(config_code)\n",
        "test_config = PipelineConfig()\n",
        "print(f\"Data directory: {test_config.data.sample_data_dir}\")\n",
        "print(f\"HF Zero-shot model: {test_config.model.hf_zero_shot_model}\")\n",
        "print(f\"Ensemble tau: {test_config.model.ensemble_tau}\")\n",
        "print(f\"Predictions output: {test_config.output.hf_predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "becf61f4",
      "metadata": {
        "id": "becf61f4"
      },
      "source": [
        "## 5. Constants and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b648a5",
      "metadata": {
        "id": "f5b648a5"
      },
      "outputs": [],
      "source": [
        "# Create constants and prompts matching the actual pipeline\n",
        "constants_code = '''\n",
        "\"\"\"\n",
        "Core Constants - Matching Production Pipeline\n",
        "\"\"\"\n",
        "\n",
        "# Policy Categories (matching actual pipeline)\n",
        "POLICY_CATEGORIES = {\n",
        "    'NO_ADS': 'No_Ads',\n",
        "    'IRRELEVANT': 'Irrelevant',\n",
        "    'RANT_NO_VISIT': 'Rant_No_Visit',\n",
        "    'NONE': 'None'\n",
        "}\n",
        "\n",
        "# Label Types (matching actual pipeline)\n",
        "LABELS = {\n",
        "    'APPROVE': 'APPROVE',\n",
        "    'REJECT': 'REJECT'\n",
        "}\n",
        "\n",
        "# Default Models (matching actual pipeline)\n",
        "DEFAULT_MODELS = {\n",
        "    'SENTIMENT': \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    'TOXIC': \"unitary/toxic-bert\",\n",
        "    'ZERO_SHOT': \"facebook/bart-large-mnli\",\n",
        "    'GEMINI_DEFAULT': \"gemini-2.5-flash-lite\"\n",
        "}\n",
        "\n",
        "# Zero-shot Classification Labels (matching actual pipeline)\n",
        "ZERO_SHOT_LABELS = [\n",
        "    \"an advertisement or promotional solicitation for this business (promo code, referral, links, contact to buy)\",\n",
        "    \"off-topic or unrelated to this business (e.g., politics, crypto, chain messages, personal stories not about this place)\",\n",
        "    \"a generic negative rant about this business without evidence of a visit (short insults, 'scam', 'overpriced', 'worst ever')\",\n",
        "    \"a relevant on-topic description of a visit or experience at this business\"\n",
        "]\n",
        "\n",
        "# Mapping zero-shot labels to policy categories\n",
        "ZERO_SHOT_TO_POLICY = {\n",
        "    ZERO_SHOT_LABELS[0]: POLICY_CATEGORIES['NO_ADS'],\n",
        "    ZERO_SHOT_LABELS[1]: POLICY_CATEGORIES['IRRELEVANT'],\n",
        "    ZERO_SHOT_LABELS[2]: POLICY_CATEGORIES['RANT_NO_VISIT'],\n",
        "    ZERO_SHOT_LABELS[3]: POLICY_CATEGORIES['NONE']\n",
        "}\n",
        "\n",
        "# Confidence Thresholds\n",
        "CONFIDENCE_THRESHOLDS = {\n",
        "    'HIGH': 0.8,\n",
        "    'MEDIUM': 0.6,\n",
        "    'LOW': 0.4,\n",
        "    'DEFAULT': 0.55\n",
        "}\n",
        "'''\n",
        "\n",
        "with open('src/core/constants.py', 'w') as f:\n",
        "    f.write(constants_code)\n",
        "\n",
        "# Create prompt templates (matching actual pipeline)\n",
        "prompts_code = '''\n",
        "\"\"\"\n",
        "Policy Prompts - Matching Production Pipeline\n",
        "\"\"\"\n",
        "\n",
        "# JSON schema all prompts must return\n",
        "TEMPLATE_JSON = \"\"\"Return ONLY JSON with no extra text:\n",
        "{\"label\":\"<APPROVE|REJECT>\",\"category\":\"<No_Ads|Irrelevant|Rant_No_Visit|None>\",\n",
        " \"rationale\":\"<short>\",\"confidence\":<0.0-1.0>,\n",
        " \"flags\":{\"links\":false,\"coupon\":false,\"visit_claimed\":false}}\n",
        "\"\"\"\n",
        "\n",
        "# ===== 1) NO ADS / PROMOTIONAL =====\n",
        "NO_ADS_SYSTEM = \"\"\"You are a content policy checker for location reviews.\n",
        "If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0. Do not reject for other policies.\n",
        "Reject ONLY if the review contains clear advertising or promotional solicitation:\n",
        "- referral/promo/coupon codes, price lists, booking/ordering links, contact-for-order (DM me / WhatsApp / Telegram / email / call), affiliate pitches.\n",
        "Do NOT mark generic off-topic content (e.g., crypto/politics) as Ads unless it includes explicit solicitation to buy or contact.\n",
        "Approve normal experiences even if positive or mentioning 'cheap' or 'good deal'.\n",
        "Output the required JSON only.\n",
        "\"\"\"\n",
        "\n",
        "# ===== 2) IRRELEVANT CONTENT =====\n",
        "IRRELEVANT_SYSTEM = \"\"\"You are checking ONLY for the 'Irrelevant' policy.\n",
        "\n",
        "Decision rule (mutually exclusive):\n",
        "- If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0.\n",
        "- Do not reject for other policies (e.g., Ads or Rant_No_Visit).\n",
        "\n",
        "Reject as Irrelevant when the text is off-topic and unrelated to THIS venue/service:\n",
        "- unrelated politics/news/crypto hype/chain messages/personal stories\n",
        "- generic advice not tied to this place (e.g., 'buy BTC now', 'vote X'), etc.\n",
        "- content about another business or location without discussing this one\n",
        "\n",
        "Return ONLY JSON with fields: label, category, rationale, confidence (0.0‚Äì1.0), flags.\n",
        "\"\"\"\n",
        "\n",
        "# ===== 3) RANTS WITHOUT VISIT =====\n",
        "RANT_NO_VISIT_SYSTEM = \"\"\"Reject generic rants or accusations clearly targeting THIS place but with no evidence of a visit.\n",
        "These rants are often:\n",
        "- Short and emotional (e.g., 'Terrible place', 'Worst ever', 'Overpriced scammers')\n",
        "- Broad accusations ('scam', 'rip-off', 'fraud')\n",
        "- Negative judgments about pricing, quality, or character of the venue\n",
        "Reject them even if the reviewer does not explicitly say 'this place/restaurant' ‚Äî assume negativity is directed at the business being reviewed.\n",
        "Approve only if the reviewer provides concrete evidence of a visit (date, food ordered, staff interaction).\n",
        "Output JSON only.\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(system_text: str, review_text: str, fewshots):\n",
        "    demo = \"\\\\n\\\\n\".join(\n",
        "        [f\"Review:\\\\n{r}\\\\nExpected JSON:\\\\n{j}\" for r,j in fewshots]\n",
        "    )\n",
        "    return f\"\"\"{system_text}\n",
        "\n",
        "{TEMPLATE_JSON}\n",
        "\n",
        "{demo}\n",
        "\n",
        "Now classify this review. Return ONLY JSON.\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\"\n",
        "'''\n",
        "\n",
        "with open('prompts/policy_prompts.py', 'w') as f:\n",
        "    f.write(prompts_code)\n",
        "\n",
        "print(\"‚úÖ Constants and prompts created matching production pipeline!\")\n",
        "\n",
        "# Test constants\n",
        "exec(constants_code)\n",
        "print(f\"Policy categories: {list(POLICY_CATEGORIES.values())}\")\n",
        "print(f\"Zero-shot model: {DEFAULT_MODELS['ZERO_SHOT']}\")\n",
        "print(f\"Default confidence threshold: {CONFIDENCE_THRESHOLDS['DEFAULT']}\")\n",
        "print(f\"Zero-shot labels configured: {len(ZERO_SHOT_LABELS)} categories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6015eaed",
      "metadata": {
        "id": "6015eaed"
      },
      "source": [
        "## 6. Gemini API Configuration and Pseudo-Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33aa5b43",
      "metadata": {
        "id": "33aa5b43"
      },
      "outputs": [],
      "source": [
        "# Gemini API Key Setup and Pseudo-Labeling Implementation (deduped + cache-aware)\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Config & cache paths ---\n",
        "PSEUDO_DIR = \"data/pseudo-label\"\n",
        "os.makedirs(PSEUDO_DIR, exist_ok=True)\n",
        "PSEUDO_PATH = os.path.join(PSEUDO_DIR, \"gemini_pseudo_labels.csv\")\n",
        "\n",
        "def load_cached_pseudo_labels():\n",
        "    if os.path.exists(PSEUDO_PATH):\n",
        "        print(f\"üì¶ Loading cached pseudo-labels from {PSEUDO_PATH}\")\n",
        "        return pd.read_csv(PSEUDO_PATH)\n",
        "    return None\n",
        "\n",
        "# --- API key (Colab secrets first, manual fallback) ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"‚úÖ Gemini API key loaded from Colab secrets\")\n",
        "    api_source = \"secrets\"\n",
        "except Exception as e:\n",
        "    print(f\"Colab secrets not found: {e}\")\n",
        "    print(\"Please enter your Gemini API key manually:\")\n",
        "    print(\"1) https://aistudio.google.com/app/apikey  2) Create key  3) Paste here\")\n",
        "    GEMINI_API_KEY = input(\"Enter your Gemini API key: \").strip()\n",
        "    api_source = \"manual\"\n",
        "\n",
        "# --- Configure & test Gemini ---\n",
        "if GEMINI_API_KEY:\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    try:\n",
        "        _test_model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "        _ = _test_model.generate_content(\"Test: Say 'API working'\")\n",
        "        print(\"‚úÖ Gemini API test successful!\")\n",
        "        print(f\"   Source: {api_source}\")\n",
        "        print(f\"   Model: gemini-2.5-flash-lite\")\n",
        "        gemini_available = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Gemini test failed: {e}\")\n",
        "        print(\"Check your API key and quota limits\")\n",
        "        gemini_available = False\n",
        "else:\n",
        "    print(\"No API key provided ‚Äî skipping Gemini pseudo-labeling.\")\n",
        "    gemini_available = False\n",
        "\n",
        "print(\"\\nConfiguration Summary:\")\n",
        "print(f\"   Gemini Available: {'‚úÖ Yes' if gemini_available else '‚ùå No'}\")\n",
        "print(f\"   Pipeline Mode: {'Full' if gemini_available else 'HuggingFace Only'}\")\n",
        "\n",
        "# --- Pseudo-label helpers ---\n",
        "def classify_with_gemini(text: str, model_name=\"gemini-2.0-flash-exp\"):\n",
        "    \"\"\"Call Gemini to classify a review into policy categories.\"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel(model_name)\n",
        "        prompt = f\"\"\"\n",
        "You are a Google review policy expert. Classify this review and provide a JSON response.\n",
        "\n",
        "Review: \"{text}\"\n",
        "\n",
        "Policy Categories:\n",
        "- No_Ads: Contains advertisements, promotional content, promo codes, contact information\n",
        "- Irrelevant: Off-topic content (politics, crypto, unrelated businesses)\n",
        "- Rant_No_Visit: Generic negative rants without evidence of visiting\n",
        "- None: Legitimate review about an actual visit/experience\n",
        "\n",
        "Respond with JSON only:\n",
        "{{\"label\": \"APPROVE\" or \"REJECT\", \"category\": \"policy_name\", \"confidence\": 0.0-1.0, \"rationale\": \"brief_explanation\"}}\n",
        "\"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "        resp = response.text.strip()\n",
        "\n",
        "        # unwrap code fences if present\n",
        "        if resp.startswith('```json'):\n",
        "            resp = resp.replace('```json', '').replace('```', '').strip()\n",
        "        elif resp.startswith('```'):\n",
        "            resp = resp.replace('```', '').strip()\n",
        "\n",
        "        # try strict JSON\n",
        "        try:\n",
        "            obj = json.loads(resp)\n",
        "            if all(k in obj for k in ('label','category','confidence')):\n",
        "                return {\n",
        "                    \"label\": obj['label'],\n",
        "                    \"category\": obj['category'],\n",
        "                    \"confidence\": float(obj['confidence']),\n",
        "                    \"rationale\": obj.get('rationale','Gemini classification')\n",
        "                }\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        # fallback: detect REJECT/APPROVE heuristically\n",
        "        if 'REJECT' in resp.upper():\n",
        "            if 'No_Ads' in resp or 'advert' in resp.lower():\n",
        "                cat = 'No_Ads'\n",
        "            elif 'Irrelevant' in resp or 'off-topic' in resp.lower():\n",
        "                cat = 'Irrelevant'\n",
        "            elif 'Rant_No_Visit' in resp or 'rant' in resp.lower():\n",
        "                cat = 'Rant_No_Visit'\n",
        "            else:\n",
        "                cat = 'No_Ads'\n",
        "            return {\"label\":\"REJECT\",\"category\":cat,\"confidence\":0.7,\"rationale\":\"Parsed from text\"}\n",
        "        else:\n",
        "            return {\"label\":\"APPROVE\",\"category\":\"None\",\"confidence\":0.7,\"rationale\":\"Parsed from text\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gemini API error: {e}\")\n",
        "        return {\"label\":\"APPROVE\",\"category\":\"None\",\"confidence\":0.0,\"rationale\":f\"API error: {e}\"}\n",
        "\n",
        "def generate_pseudo_labels_with_gemini(unlabeled_df, confidence_threshold=0.8, max_labels=50):\n",
        "    \"\"\"Generate high-confidence pseudo-labels and save to cache.\"\"\"\n",
        "    if not gemini_available:\n",
        "        print(\"‚ùå Gemini not available for pseudo-labeling\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(\"Generating pseudo-labels for training data‚Ä¶\")\n",
        "    print(f\"  Confidence threshold: {confidence_threshold}\")\n",
        "    print(f\"  Max labels: {max_labels}\")\n",
        "\n",
        "    rows = []\n",
        "    processed = 0\n",
        "    total = min(len(unlabeled_df), max_labels)\n",
        "    print(f\"Processing {total} reviews‚Ä¶\")\n",
        "\n",
        "    for _, row in tqdm(unlabeled_df.head(total).iterrows(), total=total, desc=\"Generating pseudo-labels\"):\n",
        "        txt = str(row['text'])\n",
        "        res = classify_with_gemini(txt)\n",
        "        if res['confidence'] >= confidence_threshold:\n",
        "            rows.append({\n",
        "                'id': row.get('id', processed + 100),\n",
        "                'text': txt,\n",
        "                'pred_label': res['label'],\n",
        "                'pred_category': res['category'],\n",
        "                'confidence': res['confidence'],\n",
        "                'rationale': res['rationale'],\n",
        "                'source': 'gemini_pseudo'\n",
        "            })\n",
        "        processed += 1\n",
        "        time.sleep(0.1)  # rate limit\n",
        "\n",
        "    out = pd.DataFrame(rows)\n",
        "    if len(out) > 0:\n",
        "        out.to_csv(PSEUDO_PATH, index=False)\n",
        "        print(f\"‚úÖ Generated {len(out)} pseudo-labels\")\n",
        "        print(f\"üíæ Cached to {PSEUDO_PATH}\")\n",
        "        try:\n",
        "            print(f\"Label distribution: {out['pred_label'].value_counts().to_dict()}\")\n",
        "            print(f\"Category distribution: {out['pred_category'].value_counts().to_dict()}\")\n",
        "            print(f\"Average confidence: {out['confidence'].mean():.3f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        print(\"‚ùå No high-confidence pseudo-labels generated\")\n",
        "    return out\n",
        "\n",
        "# --- Use cache if present; otherwise, generate ---\n",
        "cached = load_cached_pseudo_labels()\n",
        "if cached is not None and len(cached) > 0:\n",
        "    pseudo_labels_df = cached\n",
        "    print(f\"‚úÖ Using cached pseudo-labels (n={len(pseudo_labels_df)})\")\n",
        "else:\n",
        "    if gemini_available:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"GENERATING TRAINING DATA WITH PSEUDO-LABELS\")\n",
        "        print(\"=\"*60)\n",
        "        unlabeled_df = pd.DataFrame({\n",
        "            'id': list(range(101, 121)),\n",
        "            'text': [\n",
        "                \"Amazing food and service, definitely coming back!\",\n",
        "                \"Visit our website for exclusive deals and discounts - use code SAVE20\",\n",
        "                \"The worst experience ever, everything was terrible, total scam\",\n",
        "                \"Staff was friendly, food was fresh and tasty, good value for money\",\n",
        "                \"This place is overpriced, never going back, waste of money\",\n",
        "                \"Great atmosphere, perfect for family dinner, ordered the set meal and dessert\",\n",
        "                \"Follow my Instagram @foodie123 for more reviews and promos\",\n",
        "                \"Bitcoin is going to the moon! Buy now before it's too late!\",\n",
        "                \"Had the chicken rice here yesterday, portion was generous and taste was authentic\",\n",
        "                \"DM me for discount codes! Also selling crypto courses online\",\n",
        "                \"Terrible service, rude staff, food was cold when it arrived\",\n",
        "                \"The laksa here reminds me of my grandmother's cooking, very nostalgic\",\n",
        "                \"Check out my YouTube channel for food reviews and crypto tips\",\n",
        "                \"Went here for lunch with colleagues, everyone enjoyed their meals\",\n",
        "                \"Overpriced tourist trap, locals know better places nearby\",\n",
        "                \"Made reservation for 6pm, got seated immediately, excellent service throughout\",\n",
        "                \"Politics in this country is corrupt, restaurants like this are part of the problem\",\n",
        "                \"Their signature dish was perfectly seasoned, will definitely recommend to friends\",\n",
        "                \"Worst restaurant in Singapore, total ripoff, avoid at all costs\",\n",
        "                \"Celebrated my birthday here last week, staff even brought out a cake\"\n",
        "            ]\n",
        "        })\n",
        "        pseudo_labels_df = generate_pseudo_labels_with_gemini(\n",
        "            unlabeled_df, confidence_threshold=0.7, max_labels=20\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"SKIPPING PSEUDO-LABELING (Gemini not available)\")\n",
        "        print(\"=\"*60)\n",
        "        pseudo_labels_df = pd.DataFrame()\n",
        "\n",
        "print(f\"\\nTraining data preparation: {'‚úÖ Complete' if len(pseudo_labels_df) > 0 else '‚ùå Skipped'}\")\n",
        "print(f\"Ready for HuggingFace model training: {'‚úÖ Yes' if len(pseudo_labels_df) > 0 else '‚ö†Ô∏è Pre-trained only'}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f255cad8",
      "metadata": {
        "id": "f255cad8"
      },
      "source": [
        "## 7. HuggingFace Model Training with Pseudo-Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2235ac4d",
      "metadata": {
        "id": "2235ac4d",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers -q\n",
        "!pip install -qU \"transformers>=4.45.0\" \"accelerate>=0.34.0\" \"huggingface_hub>=0.23.0\"\n",
        "\n",
        "# 7. HuggingFace Model Training with Pseudo-Labels\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding, pipeline as hf_pipeline,\n",
        "    __version__ as HF_VER,   # <-- for version-aware args\n",
        ")\n",
        "from packaging.version import parse                # <-- for version-aware args\n",
        "from datasets import Dataset\n",
        "import torch, numpy as np, pandas as pd, re, os\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"HUGGINGFACE MODEL TRAINING WITH PSEUDO-LABELS (no sentiment, fused policy logic)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ---- import your constants (as provided)\n",
        "from src.core.constants import (\n",
        "    DEFAULT_MODELS, ZERO_SHOT_LABELS, ZERO_SHOT_TO_POLICY,\n",
        "    POLICY_CATEGORIES, LABELS, CONFIDENCE_THRESHOLDS\n",
        ")\n",
        "\n",
        "import os, pandas as pd\n",
        "\n",
        "\n",
        "PSEUDO_DIR = \"data/pseudo-label\"\n",
        "PSEUDO_PATH = os.path.join(PSEUDO_DIR, \"gemini_pseudo_labels.csv\")\n",
        "os.makedirs(PSEUDO_DIR, exist_ok=True)\n",
        "\n",
        "def load_cached_pseudo_labels(path=PSEUDO_PATH):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"üì¶ Loading cached pseudo-labels from {path}\")\n",
        "        df_cached = pd.read_csv(path)\n",
        "        need = {\"id\",\"text\",\"pred_label\",\"pred_category\",\"confidence\"}\n",
        "        missing = need - set(df_cached.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Cached pseudo-label file missing columns: {missing}\")\n",
        "        print(f\"‚úÖ Loaded {len(df_cached)} pseudo-labeled rows\")\n",
        "        return df_cached\n",
        "    print(f\"‚ÑπÔ∏è No cache at {path}.\")\n",
        "    return pd.DataFrame(columns=[\"id\",\"text\",\"pred_label\",\"pred_category\",\"confidence\"])\n",
        "\n",
        "# make pseudo_labels_df available for training-mode decision\n",
        "pseudo_labels_df = load_cached_pseudo_labels()\n",
        "\n",
        "# decide mode only AFTER loading the cache\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "has_training_data = len(pseudo_labels_df) > 0\n",
        "training_mode = \"fine-tuning\" if has_training_data else \"pre-trained\"\n",
        "print(f\"Mode: {training_mode}  |  Pseudo-label rows: {len(pseudo_labels_df)}\")\n",
        "\n",
        "# ---- models (NO sentiment)\n",
        "BASE_MODEL      = \"distilbert-base-uncased\"\n",
        "TOXIC_MODEL     = DEFAULT_MODELS['TOXIC']\n",
        "ZERO_SHOT_MODEL = DEFAULT_MODELS['ZERO_SHOT']\n",
        "\n",
        "# ========= new policy helpers (ad evidence + toxicity gate) =========\n",
        "AD_PATTERNS = [\n",
        "    r\"https?://\", r\"\\bwww\\.\", r\"\\.[a-z]{2,6}\\b\",\n",
        "    r\"\\b(?:\\+?\\d[\\s\\-()]*){7,}\\b\",\n",
        "    r\"\\bpromo(?:\\s*code)?\\b\", r\"\\bdiscount\\b\", r\"\\bcoupon\\b\",\n",
        "    r\"\\breferral\\b\", r\"\\buse\\s*code\\b\", r\"\\benter\\s*code\\b\",\n",
        "    r\"\\bwhatsapp\\b\",\n",
        "    r\"\\bdm\\s+(?:me|us)\\b\",\n",
        "    r\"\\bcontact\\s+(?:us|me)\\b\", r\"\\bcall\\s+(?:us|me)\\b\",\n",
        "]\n",
        "AD_REGEX = re.compile(\"|\".join(AD_PATTERNS), flags=re.IGNORECASE)\n",
        "TOX_TO_RANT = {\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\"}\n",
        "TOX_TO_IRRELEVANT = {\"identity_hate\"}\n",
        "\n",
        "def ad_evidence(text: str):\n",
        "    t = text or \"\"\n",
        "    m = AD_REGEX.search(t)\n",
        "    return bool(m), (m.group(0) if m else \"\")\n",
        "\n",
        "def tox_top_label(tox_output):\n",
        "    \"\"\"\n",
        "    Normalize HF toxicity pipeline outputs into (label, score).\n",
        "    Handles dict, list[dict], list[list[dict]].\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(tox_output, dict):\n",
        "            candidates = [tox_output]\n",
        "        elif isinstance(tox_output, list):\n",
        "            if len(tox_output) and isinstance(tox_output[0], dict):\n",
        "                candidates = tox_output\n",
        "            elif len(tox_output) and isinstance(tox_output[0], list):\n",
        "                candidates = tox_output[0]\n",
        "            else:\n",
        "                candidates = []\n",
        "        else:\n",
        "            candidates = []\n",
        "        if not candidates:\n",
        "            return \"NONE\", 0.0\n",
        "        best = max(candidates, key=lambda d: float(d.get(\"score\", 0.0)))\n",
        "        return best.get(\"label\", \"NONE\"), float(best.get(\"score\", 0.0))\n",
        "    except Exception:\n",
        "        return \"NONE\", 0.0\n",
        "\n",
        "\n",
        "def policy_zero_shot_fused(zshot, toxic, text: str,\n",
        "                           tau_irrelevant=0.55, tau_rant=0.55,\n",
        "                           tau_ads=0.70, tox_tau=0.50, ads_margin=0.10):\n",
        "    \"\"\"\n",
        "    1) Toxicity gate:\n",
        "         - identity_hate -> Irrelevant\n",
        "         - toxic/insult/obscene/threat -> Rant_No_Visit\n",
        "    2) Else if Irrelevant/Rant over thresholds -> that category\n",
        "    3) Else if Ads high + ad evidence + margin -> No_Ads\n",
        "    4) Else -> None\n",
        "    Returns: (pred_label 'APPROVE'/'REJECT', pred_category)\n",
        "    \"\"\"\n",
        "    # zero-shot on your label set\n",
        "    zs_res = zshot(text, candidate_labels=ZERO_SHOT_LABELS,\n",
        "                   hypothesis_template=\"This review is {}.\", multi_label=True)\n",
        "    zs = {lab: float(scr) for lab, scr in zip(zs_res[\"labels\"], zs_res[\"scores\"])}\n",
        "    ads  = zs.get(ZERO_SHOT_LABELS[0], 0.0)\n",
        "    irr  = zs.get(ZERO_SHOT_LABELS[1], 0.0)\n",
        "    rant = zs.get(ZERO_SHOT_LABELS[2], 0.0)\n",
        "    none = zs.get(ZERO_SHOT_LABELS[3], 0.0)\n",
        "\n",
        "    # toxicity top label\n",
        "    tox_label, tox_score = tox_top_label(toxic(text))\n",
        "\n",
        "\n",
        "    if tox_label and tox_score >= tox_tau:\n",
        "        if tox_label in TOX_TO_RANT:\n",
        "            return LABELS['REJECT'], POLICY_CATEGORIES['RANT_NO_VISIT']\n",
        "        if tox_label in TOX_TO_IRRELEVANT:\n",
        "            return LABELS['REJECT'], POLICY_CATEGORIES['IRRELEVANT']\n",
        "\n",
        "    if max(irr, rant) >= min(tau_irrelevant, tau_rant):\n",
        "        return LABELS['REJECT'], (POLICY_CATEGORIES['IRRELEVANT'] if irr >= rant else POLICY_CATEGORIES['RANT_NO_VISIT'])\n",
        "\n",
        "    has_ads, _ = ad_evidence(text)\n",
        "    if has_ads and (ads >= tau_ads) and (ads >= max(irr, rant) + ads_margin):\n",
        "        return LABELS['REJECT'], POLICY_CATEGORIES['NO_ADS']\n",
        "\n",
        "    return LABELS['APPROVE'], POLICY_CATEGORIES['NONE']\n",
        "\n",
        "# ========= metrics & training (binary, same as before) =========\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "def prepare_training_data(pseudo_df):\n",
        "    train_texts  = pseudo_df['text'].tolist()\n",
        "    train_labels = [1 if label == 'REJECT' else 0 for label in pseudo_df['pred_label']]\n",
        "    return train_texts, train_labels\n",
        "\n",
        "def train_custom_classification_model(train_texts, train_labels, model_name=\"review-policy-classifier\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=2)\n",
        "    enc = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "    train_ds = Dataset.from_dict({'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask'], 'labels': train_labels})\n",
        "\n",
        "    # ---- version-aware TrainingArguments (v4 vs v5)\n",
        "    base_kwargs = dict(\n",
        "        output_dir=f'./models/fine-tuned/{model_name}',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'./logs/{model_name}',\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Transformers v5+\n",
        "        args = TrainingArguments(**base_kwargs, eval_strategy=\"epoch\")\n",
        "    except TypeError:\n",
        "        # Transformers v4.x\n",
        "        args = TrainingArguments(**base_kwargs, evaluation_strategy=\"epoch\")\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args,\n",
        "        train_dataset=train_ds, eval_dataset=train_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    save_dir = f'./models/fine-tuned/{model_name}'\n",
        "    trainer.save_model(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"‚úÖ Model saved to: {save_dir}\")\n",
        "    return model, tokenizer, save_dir\n",
        "\n",
        "def load_aux_pipelines(device=None):\n",
        "    toxic = hf_pipeline(\"text-classification\", model=TOXIC_MODEL, top_k=None, device=device)\n",
        "    zshot = hf_pipeline(\"zero-shot-classification\", model=ZERO_SHOT_MODEL, device=device)\n",
        "    return toxic, zshot\n",
        "\n",
        "def predict_with_custom_model(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        conf = float(probs.max())\n",
        "        pred = int(probs.argmax())\n",
        "    return (\"REJECT\" if pred == 1 else \"APPROVE\"), conf\n",
        "\n",
        "def run_inference_pipeline(df, tau=CONFIDENCE_THRESHOLDS['DEFAULT']):\n",
        "    print(f\"\\nRunning inference‚Ä¶ ({training_mode})  samples={len(df)}\")\n",
        "    toxic_pipeline, zshot_pipeline = load_aux_pipelines(device)\n",
        "    results = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing reviews\"):\n",
        "        txt = str(row['text'])\n",
        "\n",
        "        # ---- robust access to optional custom model\n",
        "        cc = TRAINED_MODELS.get('custom_classifier')\n",
        "        if cc is not None:\n",
        "            pred_label, confidence = predict_with_custom_model(txt, cc['model'], cc['tokenizer'])\n",
        "            pred_category = \"Custom_Trained\"\n",
        "        else:\n",
        "            pred_label, pred_category = policy_zero_shot_fused(\n",
        "                zshot=zshot_pipeline, toxic=toxic_pipeline, text=txt,\n",
        "                tau_irrelevant=0.55, tau_rant=0.55, tau_ads=0.70, tox_tau=0.50, ads_margin=0.10\n",
        "            )\n",
        "            confidence = None  # rule-based fusion\n",
        "\n",
        "        # Optional: record toxicity top label/score for debugging\n",
        "        try:\n",
        "            tox_label, tox_score = tox_top_label(toxic_pipeline(txt))\n",
        "        except Exception:\n",
        "            tox_label, tox_score = \"NONE\", 0.0\n",
        "\n",
        "        results.append({\n",
        "            \"id\": row.get('id', len(results)+1),\n",
        "            \"text\": txt,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"pred_category\": pred_category,\n",
        "            \"confidence\": (round(float(confidence), 4) if confidence is not None else None),\n",
        "            \"toxicity_label\": tox_label,\n",
        "            \"toxicity_score\": round(float(tox_score), 4),\n",
        "            \"model_type\": training_mode\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    os.makedirs(\"results/predictions\", exist_ok=True)\n",
        "    out_path = f'results/predictions/predictions_{training_mode}.csv'\n",
        "    results_df.to_csv(out_path, index=False)\n",
        "    print(f\"‚úÖ Inference saved ‚Üí {out_path}  rows={len(results_df)}\")\n",
        "    return results_df\n",
        "\n",
        "# ========= Train (if pseudo-labels) or use pre-trained =========\n",
        "TRAINED_MODELS = {\"custom_classifier\": None}   # <-- init key to avoid KeyError\n",
        "if training_mode == \"fine-tuning\":\n",
        "    print(\"\\nüéØ FINE-TUNING MODE\")\n",
        "    tr_texts, tr_labels = prepare_training_data(pseudo_labels_df)\n",
        "    model, tok, model_path = train_custom_classification_model(tr_texts, tr_labels, \"review-policy-classifier\")\n",
        "    TRAINED_MODELS['custom_classifier'] = {'model_path': model_path, 'model': model, 'tokenizer': tok}\n",
        "else:\n",
        "    print(\"\\nüîÑ PRE-TRAINED MODE (no sentiment)\")\n",
        "    TRAINED_MODELS['custom_classifier'] = None\n",
        "\n",
        "# ========= Test on available data (same style) =========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(pseudo_labels_df) > 0:\n",
        "    test_data = pseudo_labels_df[['id','text']]\n",
        "    src = f\"Pseudo-labeled data (n={len(test_data)})\"\n",
        "else:\n",
        "    # fallback minimal sample\n",
        "    test_data = pd.DataFrame({'id':[1,2], 'text':[\n",
        "        \"Great food and service!\",\n",
        "        \"Use my promo code SAVE20 for discounts!\"\n",
        "    ]})\n",
        "    src = \"Minimal test data\"\n",
        "\n",
        "print(f\"Testing with: {src}\")\n",
        "hf_results = run_inference_pipeline(test_data, tau=CONFIDENCE_THRESHOLDS['DEFAULT'])\n",
        "\n",
        "# Pretty print head\n",
        "print(\"\\nüìä INFERENCE RESULTS\")\n",
        "print(\"=\"*50)\n",
        "disp_cols = ['id','text','pred_label','pred_category','confidence','model_type','toxicity_label','toxicity_score']\n",
        "dd = hf_results.copy()\n",
        "dd['text'] = dd['text'].apply(lambda x: x[:50]+\"...\" if isinstance(x,str) and len(x)>50 else x)\n",
        "print(dd[disp_cols].head(10).to_string(index=False))\n",
        "\n",
        "# Summary\n",
        "print(\"\\nüìà RESULTS SUMMARY\")\n",
        "print(hf_results['pred_label'].value_counts())\n",
        "print(f\"Average Confidence (fine-tuned only): {hf_results['confidence'].dropna().mean() if 'confidence' in hf_results else float('nan'):.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ MODEL TRAINING AND TESTING COMPLETE\")\n",
        "print(f\"Training Mode: {training_mode.upper()}\")\n",
        "print(\"Ready for Model Persistence: ‚úÖ Yes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Unified Spam Detection Model\n"
      ],
      "metadata": {
        "id": "lUbUl5xwsZTZ"
      },
      "id": "lUbUl5xwsZTZ"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Unified spam detection system combining machine learning with pattern analysis.\n",
        "Assumes data has already passed through toxicity/hate speech filtering pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "!pip install textstat\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class DetectionResult:\n",
        "    \"\"\"Container for detection results.\"\"\"\n",
        "    label: str  # 'APPROVE' or 'REJECT'\n",
        "    confidence: float  # 0.0 to 1.0\n",
        "    category: str  # violation category or 'None'\n",
        "    features: Dict  # method-specific features\n",
        "    confidence_interval: Tuple[float, float]\n",
        "\n",
        "class PatternFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract pattern-based features for ML model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Extract pattern features from texts.\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for text in X:\n",
        "            text_features = self._extract_pattern_features(text)\n",
        "\n",
        "            # Convert to feature vector\n",
        "            feature_vector = [\n",
        "                text_features['repetition_ratio'],\n",
        "                text_features['word_diversity_ratio'],\n",
        "                text_features['repeated_ngrams_ratio'],\n",
        "                text_features['caps_ratio'],\n",
        "                text_features['punct_ratio'],\n",
        "                text_features['readability_score'] / 100.0,  # Normalize\n",
        "                text_features['avg_sentence_length'] / 50.0,  # Normalize\n",
        "                text_features['word_count'] / 100.0,  # Normalize\n",
        "                text_features['template_score'],\n",
        "                text_features['phrase_repetition_score'],\n",
        "                text_features['local_repetition_score']\n",
        "            ]\n",
        "            features.append(feature_vector)\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def _extract_pattern_features(self, text: str) -> Dict:\n",
        "        \"\"\"Extract comprehensive pattern features from text.\"\"\"\n",
        "        words = text.split()\n",
        "        word_count = len(words)\n",
        "        char_count = len(text)\n",
        "\n",
        "        # Basic repetition analysis\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            word_lower = word.lower()\n",
        "            word_freq[word_lower] = word_freq.get(word_lower, 0) + 1\n",
        "\n",
        "        repetition_ratio = 0.0\n",
        "        if word_count > 0:\n",
        "            max_repetition = max(word_freq.values()) if word_freq else 1\n",
        "            repetition_ratio = max_repetition / word_count\n",
        "\n",
        "        # Word diversity (unique words / total words)\n",
        "        unique_words = len(set(word.lower() for word in words))\n",
        "        word_diversity_ratio = unique_words / max(word_count, 1)\n",
        "\n",
        "        # N-gram repetition detection (for \"food is good food is great food is nice\")\n",
        "        repeated_ngrams_ratio = self._detect_repeated_ngrams(words)\n",
        "\n",
        "        # Phrase repetition score (detects patterns like \"food is X\" repeating)\n",
        "        phrase_repetition_score = self._detect_phrase_patterns(words)\n",
        "\n",
        "        # Local repetition score (detects repetition in specific text segments)\n",
        "        local_repetition_score = self._detect_local_repetition(words)\n",
        "\n",
        "        # Template detection (common patterns)\n",
        "        template_indicators = [\n",
        "            r'\\b(excellent|amazing|great|good|bad|terrible)\\b.*\\b(food|service|place|restaurant)\\b',\n",
        "            r'\\b(recommend|suggest|try|visit)\\b.*\\b(place|restaurant|here)\\b',\n",
        "            r'\\b(will|would).*\\b(come|go|visit).*\\b(again|back)\\b'\n",
        "        ]\n",
        "\n",
        "        template_matches = sum(1 for pattern in template_indicators\n",
        "                             if re.search(pattern, text, re.IGNORECASE))\n",
        "        template_score = template_matches / len(template_indicators)\n",
        "\n",
        "        # Simple readability proxy (no textstat installed)\n",
        "        words = len(text.split())\n",
        "        sentences = len(re.split(r'[.!?]+', text))\n",
        "        if sentences > 0 and words > 0:\n",
        "            avg_words_per_sentence = words / sentences\n",
        "            readability_score = max(0, min(100, 120 - avg_words_per_sentence * 2))\n",
        "        else:\n",
        "            readability_score = 50.0\n",
        "\n",
        "\n",
        "        # Character-level features\n",
        "        caps_count = sum(1 for c in text if c.isupper())\n",
        "        punct_count = sum(1 for c in text if c in '!?.,;:')\n",
        "\n",
        "        caps_ratio = caps_count / max(char_count, 1)\n",
        "        punct_ratio = punct_count / max(char_count, 1)\n",
        "\n",
        "        # Sentence structure\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        sentence_count = len([s for s in sentences if s.strip()])\n",
        "        avg_sentence_length = word_count / max(sentence_count, 1)\n",
        "\n",
        "        return {\n",
        "            'word_count': word_count,\n",
        "            'char_count': char_count,\n",
        "            'repetition_ratio': repetition_ratio,\n",
        "            'word_diversity_ratio': word_diversity_ratio,\n",
        "            'repeated_ngrams_ratio': repeated_ngrams_ratio,\n",
        "            'phrase_repetition_score': phrase_repetition_score,\n",
        "            'local_repetition_score': local_repetition_score,\n",
        "            'template_score': template_score,\n",
        "            'readability_score': readability_score,\n",
        "            'caps_ratio': caps_ratio,\n",
        "            'punct_ratio': punct_ratio,\n",
        "            'sentence_count': sentence_count,\n",
        "            'avg_sentence_length': avg_sentence_length\n",
        "        }\n",
        "\n",
        "    def _detect_repeated_ngrams(self, words: List[str], n: int = 2) -> float:\n",
        "        \"\"\"Detect repeated n-grams that indicate spam patterns.\"\"\"\n",
        "        if len(words) < n * 2:  # Need at least 2 n-grams to compare\n",
        "            return 0.0\n",
        "\n",
        "        # Generate n-grams\n",
        "        ngrams = []\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngram = ' '.join(words[i:i+n]).lower()\n",
        "            ngrams.append(ngram)\n",
        "\n",
        "        if not ngrams:\n",
        "            return 0.0\n",
        "\n",
        "        # Count n-gram frequencies\n",
        "        ngram_freq = {}\n",
        "        for ngram in ngrams:\n",
        "            ngram_freq[ngram] = ngram_freq.get(ngram, 0) + 1\n",
        "\n",
        "        # Calculate repetition score\n",
        "        total_ngrams = len(ngrams)\n",
        "        repeated_ngrams = sum(1 for freq in ngram_freq.values() if freq > 1)\n",
        "\n",
        "        return repeated_ngrams / total_ngrams if total_ngrams > 0 else 0.0\n",
        "\n",
        "    def _detect_phrase_patterns(self, words: List[str]) -> float:\n",
        "        \"\"\"Detect repeating phrase patterns like 'food is X food is Y food is Z'.\"\"\"\n",
        "        if len(words) < 6:  # Need enough words for pattern detection\n",
        "            return 0.0\n",
        "\n",
        "        # Look for patterns where same 2-word prefix repeats\n",
        "        pattern_scores = []\n",
        "\n",
        "        # Check 2-word patterns\n",
        "        for i in range(len(words) - 3):\n",
        "            prefix = f\"{words[i]} {words[i+1]}\".lower()\n",
        "\n",
        "            # Count how many times this prefix appears\n",
        "            prefix_count = 0\n",
        "            for j in range(i, len(words) - 1):\n",
        "                if j + 1 < len(words):\n",
        "                    candidate = f\"{words[j]} {words[j+1]}\".lower()\n",
        "                    if candidate == prefix:\n",
        "                        prefix_count += 1\n",
        "\n",
        "            if prefix_count > 1:\n",
        "                # This prefix repeats - calculate pattern strength\n",
        "                pattern_strength = prefix_count / (len(words) / 3)  # Normalize by text length\n",
        "                pattern_scores.append(min(pattern_strength, 1.0))\n",
        "\n",
        "        # Also check for local repetition at the end of text (common spam pattern)\n",
        "        local_repetition_score = self._detect_local_repetition(words)\n",
        "\n",
        "        return max(max(pattern_scores) if pattern_scores else 0.0, local_repetition_score)\n",
        "\n",
        "    def _detect_local_repetition(self, words: List[str]) -> float:\n",
        "        \"\"\"Detect repetitive patterns in specific sections of text (e.g., end of text).\"\"\"\n",
        "        if len(words) < 6:\n",
        "            return 0.0\n",
        "\n",
        "        max_score = 0.0\n",
        "\n",
        "        # Check multiple segments: end of text and any suspicious consecutive patterns\n",
        "        segments_to_check = []\n",
        "\n",
        "        # 1. End segment (common spam pattern)\n",
        "        end_segment_size = min(10, max(6, len(words) // 3))\n",
        "        end_words = words[-end_segment_size:]\n",
        "        segments_to_check.append(('end', end_words))\n",
        "\n",
        "        # 2. Look for any consecutive repeated phrases throughout the text\n",
        "        for start_pos in range(len(words) - 6):  # Sliding window\n",
        "            window_size = min(8, len(words) - start_pos)\n",
        "            window_words = words[start_pos:start_pos + window_size]\n",
        "            segments_to_check.append(('window', window_words))\n",
        "\n",
        "        for segment_type, segment_words in segments_to_check:\n",
        "            # Look for exact phrase repetition in this segment\n",
        "            for phrase_len in [2, 3, 4]:  # Check 2-word, 3-word, 4-word phrases\n",
        "                if len(segment_words) < phrase_len * 2:  # Need at least 2 instances\n",
        "                    continue\n",
        "\n",
        "                for i in range(len(segment_words) - phrase_len + 1):\n",
        "                    phrase = ' '.join(segment_words[i:i+phrase_len]).lower()\n",
        "\n",
        "                    # Look for consecutive repetition (more suspicious than scattered)\n",
        "                    consecutive_count = 1\n",
        "                    next_pos = i + phrase_len\n",
        "\n",
        "                    while next_pos + phrase_len <= len(segment_words):\n",
        "                        next_phrase = ' '.join(segment_words[next_pos:next_pos+phrase_len]).lower()\n",
        "                        if next_phrase == phrase:\n",
        "                            consecutive_count += 1\n",
        "                            next_pos += phrase_len\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    if consecutive_count >= 2:  # Found consecutive repeated phrase\n",
        "                        # Higher score for consecutive repetition\n",
        "                        if segment_type == 'end':\n",
        "                            # End segment repetition is more suspicious\n",
        "                            local_score = (consecutive_count * phrase_len * 1.5) / len(segment_words)\n",
        "                        else:\n",
        "                            # General repetition\n",
        "                            local_score = (consecutive_count * phrase_len) / len(segment_words)\n",
        "\n",
        "                        max_score = max(max_score, min(local_score, 1.0))\n",
        "\n",
        "        return max_score\n",
        "\n",
        "\n",
        "class UnifiedSpamDetector:\n",
        "    \"\"\"Unified spam detector combining TF-IDF and pattern analysis in single ML model.\"\"\"\n",
        "\n",
        "    def __init__(self, max_features: int = 5000, ngram_range: Tuple[int, int] = (1, 3),\n",
        "                 spam_threshold: float = 0.3):\n",
        "        self.max_features = max_features\n",
        "        self.ngram_range = ngram_range\n",
        "        self.spam_threshold = spam_threshold\n",
        "        self.pipeline = None\n",
        "        self.calibrated_pipeline = None\n",
        "        self.training_size = 0\n",
        "\n",
        "    def fit(self, texts: List[str], labels: List[str], calibrate: bool = True):\n",
        "        \"\"\"\n",
        "        Fit the unified model combining TF-IDF and pattern features.\n",
        "\n",
        "        Args:\n",
        "            texts: List of review texts\n",
        "            labels: List of labels ('APPROVE' or 'REJECT')\n",
        "            calibrate: Whether to calibrate probabilities\n",
        "        \"\"\"\n",
        "        # Convert labels to binary\n",
        "        y = [1 if label == 'REJECT' else 0 for label in labels]\n",
        "        self.training_size = len(texts)\n",
        "\n",
        "        # For small datasets, disable ML-based classification and rely on patterns only\n",
        "        if len(texts) < 10:\n",
        "            # With very small training data, ML is unreliable - use pattern-only mode\n",
        "            self.effective_threshold = 0.9  # Very high threshold to disable ML classification\n",
        "            self.pattern_only_mode = True\n",
        "            print(f\"‚ö†Ô∏è  Very small training set ({len(texts)} samples). Using pattern-only detection mode.\")\n",
        "        elif len(texts) < 20:\n",
        "            # Small dataset - be more conservative with ML threshold\n",
        "            adjusted_threshold = min(0.6, self.spam_threshold + 0.2)\n",
        "            print(f\"‚ö†Ô∏è  Small training set ({len(texts)} samples). Adjusting threshold: {self.spam_threshold:.2f} ‚Üí {adjusted_threshold:.2f}\")\n",
        "            self.effective_threshold = adjusted_threshold\n",
        "            self.pattern_only_mode = False\n",
        "        else:\n",
        "            self.effective_threshold = self.spam_threshold\n",
        "            self.pattern_only_mode = False\n",
        "\n",
        "        # Create unified pipeline with both TF-IDF and pattern features\n",
        "        tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=self.max_features,\n",
        "            ngram_range=self.ngram_range,\n",
        "            stop_words='english',\n",
        "            lowercase=True,\n",
        "            min_df=1,\n",
        "            max_df=0.95\n",
        "        )\n",
        "\n",
        "        pattern_extractor = PatternFeatureExtractor()\n",
        "\n",
        "        # Combine features using FeatureUnion\n",
        "        feature_union = FeatureUnion([  # type: ignore\n",
        "            ('tfidf', tfidf_vectorizer),\n",
        "            ('patterns', pattern_extractor)\n",
        "        ])\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            ('features', feature_union),\n",
        "            ('classifier', LogisticRegression(\n",
        "                random_state=42,\n",
        "                max_iter=1000,\n",
        "                class_weight='balanced'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # Fit the pipeline\n",
        "        print(\"Training unified ML + Pattern model...\")\n",
        "        self.pipeline.fit(texts, y)\n",
        "\n",
        "        # Calibrate probabilities if requested\n",
        "        if calibrate and len(texts) >= 10:\n",
        "            min_class_size = min(sum(y), len(y) - sum(y))\n",
        "            cv_folds = min(3, max(2, min_class_size))\n",
        "\n",
        "            self.calibrated_pipeline = CalibratedClassifierCV(\n",
        "                self.pipeline,\n",
        "                method='isotonic',\n",
        "                cv=cv_folds\n",
        "            )\n",
        "            self.calibrated_pipeline.fit(texts, y)  # type: ignore\n",
        "        elif calibrate:\n",
        "            print(f\"Skipping calibration: need at least 10 samples, got {len(texts)}\")\n",
        "\n",
        "    def predict(self, texts: List[str], use_calibrated: bool = True) -> List[DetectionResult]:\n",
        "        \"\"\"Predict spam using unified model.\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
        "\n",
        "        pipeline = self.calibrated_pipeline if (use_calibrated and self.calibrated_pipeline) else self.pipeline\n",
        "\n",
        "        # Get predictions and probabilities\n",
        "        proba = pipeline.predict_proba(texts)  # type: ignore\n",
        "        spam_proba = proba[:, 1]  # Probability of spam (REJECT)\n",
        "\n",
        "        results = []\n",
        "        for i, prob in enumerate(spam_proba):\n",
        "            # Extract pattern features for analysis\n",
        "            pattern_extractor = PatternFeatureExtractor()\n",
        "            text_features = pattern_extractor._extract_pattern_features(texts[i])\n",
        "\n",
        "            # Check if we're in pattern-only mode (small training data)\n",
        "            pattern_only_mode = getattr(self, 'pattern_only_mode', False)\n",
        "            threshold = getattr(self, 'effective_threshold', self.spam_threshold)\n",
        "\n",
        "            if pattern_only_mode:\n",
        "                # Pattern-only classification for small datasets\n",
        "                word_count = text_features['word_count']\n",
        "\n",
        "                # Define clear spam patterns\n",
        "                is_obvious_spam = (\n",
        "                    (text_features['repetition_ratio'] > 0.8) or  # Very high word repetition\n",
        "                    (text_features['phrase_repetition_score'] > 0.7) or  # Very high phrase repetition\n",
        "                    (text_features['repeated_ngrams_ratio'] > 0.6) or  # Very high ngram repetition\n",
        "                    (text_features['local_repetition_score'] > 0.4) or  # Local repetition (like \"food is good food is good\")\n",
        "                    (word_count < 3 and text_features['repetition_ratio'] > 0.6)  # Short repetitive text\n",
        "                )\n",
        "\n",
        "                # Special cases for high-confidence spam detection\n",
        "                is_phrase_spam = text_features['phrase_repetition_score'] > 0.9\n",
        "                is_local_spam = text_features['local_repetition_score'] > 0.5\n",
        "\n",
        "                # Additional filters to avoid false positives (but not for phrase spam)\n",
        "                seems_legitimate = (\n",
        "                    text_features['readability_score'] > 30 and\n",
        "                    word_count > 8 and\n",
        "                    text_features['word_diversity_ratio'] > 0.6 and\n",
        "                    text_features['phrase_repetition_score'] < 0.8  # Allow phrase detection override\n",
        "                )\n",
        "\n",
        "                # Flag obvious spam patterns, high phrase repetition, or local repetition\n",
        "                if is_phrase_spam or is_local_spam or (is_obvious_spam and not seems_legitimate):\n",
        "                    label = 'REJECT'\n",
        "                    confidence = max(\n",
        "                        text_features['repetition_ratio'],\n",
        "                        text_features['phrase_repetition_score'],\n",
        "                        text_features['repeated_ngrams_ratio'],\n",
        "                        text_features['local_repetition_score']\n",
        "                    )\n",
        "                else:\n",
        "                    label = 'APPROVE'\n",
        "                    confidence = 1.0 - max(\n",
        "                        text_features['repetition_ratio'] * 0.5,\n",
        "                        text_features['phrase_repetition_score'] * 0.5,\n",
        "                        text_features['repeated_ngrams_ratio'] * 0.5,\n",
        "                        text_features['local_repetition_score'] * 0.5\n",
        "                    )\n",
        "\n",
        "                pattern_override = (label == 'REJECT')\n",
        "\n",
        "            else:\n",
        "                # Normal ML + pattern mode\n",
        "                label = 'REJECT' if prob > threshold else 'APPROVE'\n",
        "\n",
        "                # Pattern-based override for obvious spam\n",
        "                word_count = text_features['word_count']\n",
        "                is_very_repetitive = (\n",
        "                    (text_features['repetition_ratio'] > 0.8) or\n",
        "                    (text_features['phrase_repetition_score'] > 0.7) or\n",
        "                    (text_features['repeated_ngrams_ratio'] > 0.6)\n",
        "                )\n",
        "\n",
        "                has_reasonable_diversity = text_features['word_diversity_ratio'] > 0.3 and word_count > 6\n",
        "                is_likely_review = text_features['readability_score'] > 20 and word_count > 4\n",
        "\n",
        "                if is_very_repetitive and not (has_reasonable_diversity and is_likely_review):\n",
        "                    label = 'REJECT'\n",
        "                    pattern_override = True\n",
        "                    confidence = max(\n",
        "                        text_features['repetition_ratio'],\n",
        "                        text_features['phrase_repetition_score'],\n",
        "                        text_features['repeated_ngrams_ratio']\n",
        "                    )\n",
        "                else:\n",
        "                    pattern_override = False\n",
        "                    confidence = prob if label == 'REJECT' else (1 - prob)\n",
        "\n",
        "            # Determine category based on dominant pattern\n",
        "            category = 'None'\n",
        "            if label == 'REJECT':\n",
        "                if pattern_override:\n",
        "                    if text_features['repetition_ratio'] > 0.6:\n",
        "                        category = 'Repetitive_Spam'\n",
        "                    elif text_features['local_repetition_score'] > 0.4:\n",
        "                        category = 'Local_Repetition_Spam'\n",
        "                    elif text_features['phrase_repetition_score'] > 0.5:\n",
        "                        category = 'Phrase_Pattern_Spam'\n",
        "                    elif text_features['repeated_ngrams_ratio'] > 0.4:\n",
        "                        category = 'NGram_Pattern_Spam'\n",
        "                    else:\n",
        "                        category = 'Pattern_Spam'\n",
        "                else:\n",
        "                    # ML-detected spam\n",
        "                    if text_features['repetition_ratio'] > 0.4 or text_features['phrase_repetition_score'] > 0.4:\n",
        "                        category = 'Repetitive_Spam'\n",
        "                    elif text_features['template_score'] > 0.6:\n",
        "                        category = 'Template_Spam'\n",
        "                    else:\n",
        "                        category = 'ML_Detected_Spam'\n",
        "\n",
        "            # Simple confidence interval calculation\n",
        "            ci_margin = 0.1 * (1 - confidence)  # Smaller margin for higher confidence\n",
        "            ci_lower = max(0.0, confidence - ci_margin)\n",
        "            ci_upper = min(1.0, confidence + ci_margin)\n",
        "\n",
        "            results.append(DetectionResult(\n",
        "                label=label,\n",
        "                confidence=float(confidence),\n",
        "                category=category,\n",
        "                features={\n",
        "                    'spam_probability': float(prob),\n",
        "                    'pattern_features': text_features,\n",
        "                    'threshold_used': float(threshold)\n",
        "                },\n",
        "                confidence_interval=(float(ci_lower), float(ci_upper))\n",
        "            ))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_feature_importance(self, top_k: int = 20) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Get most important features from the unified model.\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not fitted.\")\n",
        "\n",
        "        try:\n",
        "            classifier = self.pipeline.named_steps['classifier']\n",
        "            feature_union = self.pipeline.named_steps['features']\n",
        "\n",
        "            # Get feature names\n",
        "            tfidf_vectorizer = feature_union.transformer_list[0][1]\n",
        "            tfidf_names = list(tfidf_vectorizer.get_feature_names_out())\n",
        "            pattern_names = [\n",
        "                'repetition_ratio', 'word_diversity_ratio', 'repeated_ngrams_ratio',\n",
        "                'caps_ratio', 'punct_ratio', 'readability_score',\n",
        "                'avg_sentence_length', 'word_count', 'template_score',\n",
        "                'phrase_repetition_score', 'local_repetition_score'\n",
        "            ]\n",
        "\n",
        "            all_feature_names = tfidf_names + pattern_names\n",
        "\n",
        "            # Get coefficients\n",
        "            coef = classifier.coef_[0]\n",
        "            top_indices = np.argsort(np.abs(coef))[-top_k:][::-1]\n",
        "\n",
        "            return [(all_feature_names[idx], float(coef[idx])) for idx in top_indices]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not extract feature importance: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "def load_training_data(file_path: str) -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"Load training data from CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Handle different column name variations\n",
        "    text_col = None\n",
        "    for col in ['text', 'review', 'content', 'text_clean']:\n",
        "        if col in df.columns:\n",
        "            text_col = col\n",
        "            break\n",
        "\n",
        "    if text_col is None:\n",
        "        raise ValueError(f\"No text column found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "\n",
        "    # Handle labels - check if column exists, if not create default\n",
        "    if 'gold_label' in df.columns:\n",
        "        labels = df['gold_label'].tolist()\n",
        "    elif 'label' in df.columns:\n",
        "        labels = df['label'].tolist()\n",
        "    else:\n",
        "        labels = ['APPROVE'] * len(texts)\n",
        "\n",
        "    # Handle categories - check if column exists, if not create default\n",
        "    if 'gold_category' in df.columns:\n",
        "        categories = df['gold_category'].tolist()\n",
        "    elif 'category' in df.columns:\n",
        "        categories = df['category'].tolist()\n",
        "    else:\n",
        "        categories = ['None'] * len(texts)\n",
        "\n",
        "    return texts, labels, categories\n",
        "\n",
        "\n",
        "def evaluate_detector(detector: UnifiedSpamDetector,\n",
        "                              texts: List[str],\n",
        "                              true_labels: List[str],\n",
        "                              true_categories: List[str]) -> Dict:\n",
        "    \"\"\"Evaluate the unified detector performance.\"\"\"\n",
        "    results = detector.predict(texts)\n",
        "\n",
        "    # Extract predictions\n",
        "    pred_labels = [r.label for r in results]\n",
        "    pred_categories = [r.category for r in results]\n",
        "\n",
        "    # Binary classification metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, pred_labels, average='binary', pos_label='REJECT'\n",
        "    )\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "    # Confidence analysis\n",
        "    confidences = [r.confidence for r in results]\n",
        "\n",
        "    evaluation_results = {\n",
        "        'binary_classification': {\n",
        "            'accuracy': float(accuracy),\n",
        "            'precision': float(precision),\n",
        "            'recall': float(recall),\n",
        "            'f1': float(f1)\n",
        "        },\n",
        "        'confidence_stats': {\n",
        "            'mean': float(np.mean(confidences)),\n",
        "            'std': float(np.std(confidences)),\n",
        "            'min': float(np.min(confidences)),\n",
        "            'max': float(np.max(confidences))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return evaluation_results"
      ],
      "metadata": {
        "id": "gLefa0zasVqK"
      },
      "id": "gLefa0zasVqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yXwykZFTvj3v"
      },
      "id": "yXwykZFTvj3v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure a dataframe `df` with columns: text, gold_label, gold_category\n",
        "import pandas as pd, os\n",
        "\n",
        "PSEUDO_DIR  = \"data/pseudo-label\"\n",
        "PSEUDO_PATH = os.path.join(PSEUDO_DIR, \"gemini_pseudo_labels.csv\")\n",
        "\n",
        "def choose_training_df():\n",
        "    # 1) Prefer cached pseudo-labels if present\n",
        "    if os.path.exists(PSEUDO_PATH):\n",
        "        print(f\"üì¶ Using cached pseudo-labels ‚Üí {PSEUDO_PATH}\")\n",
        "        tdf = pd.read_csv(PSEUDO_PATH)\n",
        "        # normalize column names from pseudo-labels\n",
        "        rename_map = {\n",
        "            \"pred_label\": \"gold_label\",\n",
        "            \"pred_category\": \"gold_category\"\n",
        "        }\n",
        "        for k, v in rename_map.items():\n",
        "            if k in tdf.columns and v not in tdf.columns:\n",
        "                tdf = tdf.rename(columns={k: v})\n",
        "        # sanity check\n",
        "        need = {\"text\", \"gold_label\", \"gold_category\"}\n",
        "        missing = need - set(tdf.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Pseudo-label file missing columns: {missing}\")\n",
        "        # ensure types\n",
        "        tdf[\"text\"] = tdf[\"text\"].astype(str)\n",
        "        tdf[\"gold_label\"] = tdf[\"gold_label\"].astype(str)\n",
        "        tdf[\"gold_category\"] = tdf[\"gold_category\"].astype(str)\n",
        "        print(f\"‚úÖ Pseudo-label rows: {len(tdf)}\")\n",
        "        return tdf[[\"text\", \"gold_label\", \"gold_category\"]].copy()\n",
        "\n",
        "    # 2) Optional CSV fallbacks if you have a manual train file\n",
        "    for path in [\"data/train.csv\", \"train.csv\", \"data/df.csv\"]:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"üìÑ Loading training data from {path}\")\n",
        "            tdf = pd.read_csv(path)\n",
        "            # normalize possible column variants\n",
        "            if \"label\" in tdf.columns and \"gold_label\" not in tdf.columns:\n",
        "                tdf = tdf.rename(columns={\"label\": \"gold_label\"})\n",
        "            if \"category\" in tdf.columns and \"gold_category\" not in tdf.columns:\n",
        "                tdf = tdf.rename(columns={\"category\": \"gold_category\"})\n",
        "            need = {\"text\", \"gold_label\", \"gold_category\"}\n",
        "            missing = need - set(tdf.columns)\n",
        "            if missing:\n",
        "                raise ValueError(f\"Training CSV missing columns: {missing}\")\n",
        "            tdf[\"text\"] = tdf[\"text\"].astype(str)\n",
        "            return tdf[[\"text\", \"gold_label\", \"gold_category\"]].copy()\n",
        "\n",
        "    # 3) Last resort: tiny demo set\n",
        "    print(\"‚ÑπÔ∏è No pseudo-labels or CSV found; using small demo dataset (5 rows).\")\n",
        "    return pd.DataFrame({\n",
        "        \"text\": [\n",
        "            \"Food is good food is great food is nice food is good\",\n",
        "            \"Service was quick and friendly. Would return.\",\n",
        "            \"Amazing place! Highly recommend this restaurant.\",\n",
        "            \"Visit my site http://spam.biz ‚Äì use promo code NOW!\",\n",
        "            \"Terrible service. Will not come back.\"\n",
        "        ],\n",
        "        \"gold_label\": [\"REJECT\", \"APPROVE\", \"APPROVE\", \"REJECT\", \"APPROVE\"],\n",
        "        \"gold_category\": [\"Repetitive_Spam\", \"None\", \"Template_Spam\", \"ML_Detected_Spam\", \"None\"]\n",
        "    })\n",
        "\n",
        "# Important: don't keep an old global df around\n",
        "if 'df' in globals():\n",
        "    del df\n",
        "\n",
        "train_df = choose_training_df()\n",
        "print(train_df.head(5))\n",
        "# -----------------------"
      ],
      "metadata": {
        "id": "hga5kenfvkNV"
      },
      "id": "hga5kenfvkNV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts      = train_df['text'].astype(str).tolist()\n",
        "labels     = train_df['gold_label'].tolist()\n",
        "categories = train_df['gold_category'].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test, cat_train, cat_test = train_test_split(\n",
        "    texts, labels, categories, test_size=0.3, random_state=42, stratify=labels if len(set(labels)) > 1 else None\n",
        ")\n",
        "detector = UnifiedSpamDetector(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 3),\n",
        "    spam_threshold=0.3\n",
        ")\n",
        "\n",
        "detector.fit(X_train, y_train, calibrate=True)\n",
        "\n",
        "eval_results = evaluate_detector(detector, X_test, y_test, cat_test)\n",
        "print(\"Unified Spam Detector Evaluation:\")\n",
        "eval_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T41uylNzvpS_"
      },
      "id": "T41uylNzvpS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4c020a20",
      "metadata": {
        "id": "4c020a20"
      },
      "source": [
        "## 9. Model Persistence and Export (After Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54193be4",
      "metadata": {
        "id": "54193be4"
      },
      "outputs": [],
      "source": [
        "# Model Persistence and Training Data Export\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"MODEL PERSISTENCE AND DATA EXPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('models/saved_models', exist_ok=True)\n",
        "os.makedirs('data/training', exist_ok=True)\n",
        "os.makedirs('data/predictions', exist_ok=True)\n",
        "os.makedirs('results/model_info', exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# === Save UnifiedSpamDetector (scikit-learn pipeline) ===\n",
        "model_filename = f\"models/saved_models/unified_spam_detector_{timestamp}.joblib\"\n",
        "joblib.dump(detector, model_filename)\n",
        "\n",
        "print(f\"‚úÖ UnifiedSpamDetector saved at {model_filename}\")\n",
        "\n",
        "# Track it in model_info\n",
        "model_info[\"sklearn_model_path\"] = model_filename\n",
        "\n",
        "\n",
        "# Generate timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Model information\n",
        "model_info = {\n",
        "    \"training_timestamp\": timestamp,\n",
        "    \"training_mode\": training_mode,\n",
        "    \"base_model\": BASE_MODEL if training_mode == \"fine-tuning\" else \"pre-trained-only\",\n",
        "    \"auxiliary_models\": {\n",
        "      \"toxicity_model\": TOXIC_MODEL,\n",
        "      \"zero_shot_model\": ZERO_SHOT_MODEL\n",
        "  },\n",
        "    \"training_data_size\": len(pseudo_labels_df) if has_training_data else 0,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
        "    \"performance_metrics\": {}\n",
        "}\n",
        "\n",
        "print(f\"Training Mode: {training_mode}\")\n",
        "print(f\"Timestamp: {timestamp}\")\n",
        "\n",
        "# Save trained models\n",
        "if training_mode == \"fine-tuning\" and TRAINED_MODELS['custom_classifier'] is not None:\n",
        "    print(f\"\\nüíæ SAVING FINE-TUNED MODELS\")\n",
        "\n",
        "    # Custom model is already saved during training\n",
        "    custom_model_path = TRAINED_MODELS['custom_classifier']['model_path']\n",
        "    final_model_path = f'models/saved_models/review_classifier_{timestamp}'\n",
        "\n",
        "    # Copy to final location with timestamp\n",
        "    if os.path.exists(custom_model_path):\n",
        "        shutil.copytree(custom_model_path, final_model_path, dirs_exist_ok=True)\n",
        "        print(f\"‚úÖ Fine-tuned model copied to: {final_model_path}\")\n",
        "\n",
        "        model_info[\"custom_model_path\"] = final_model_path\n",
        "        model_info[\"model_files\"] = {\n",
        "            \"config\": f\"{final_model_path}/config.json\",\n",
        "            \"model\": f\"{final_model_path}/pytorch_model.bin\",\n",
        "            \"tokenizer\": f\"{final_model_path}/tokenizer.json\"\n",
        "        }\n",
        "\n",
        "        # Test model loading\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "            test_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
        "            test_model = AutoModelForSequenceClassification.from_pretrained(final_model_path)\n",
        "            print(\"‚úÖ Model loading test successful\")\n",
        "            model_info[\"model_loadable\"] = True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model loading test failed: {e}\")\n",
        "            model_info[\"model_loadable\"] = False\n",
        "    else:\n",
        "        print(f\"‚ùå Custom model path not found: {custom_model_path}\")\n",
        "        model_info[\"custom_model_path\"] = None\n",
        "\n",
        "else:\n",
        "    print(f\"\\nüì¶ SAVING PRE-TRAINED MODEL REFERENCES\")\n",
        "    model_info[\"custom_model_path\"] = None\n",
        "    model_info[\"model_files\"] = None\n",
        "\n",
        "# Save auxiliary model pipeline state (for consistency in inference)\n",
        "auxiliary_info = {\n",
        "    \"toxicity_model\": TOXIC_MODEL,\n",
        "    \"zero_shot_model\": ZERO_SHOT_MODEL,\n",
        "    \"device_used\": 0 if torch.cuda.is_available() else -1,\n",
        "    \"models_loaded\": False  # we store names; loader will instantiate\n",
        "}\n",
        "\n",
        "\n",
        "# Save training data if available\n",
        "if has_training_data:\n",
        "    print(f\"\\nüíæ SAVING TRAINING DATA\")\n",
        "\n",
        "    # Save pseudo-labeled training data\n",
        "    training_data_path = f'data/training/pseudo_labels_{timestamp}.csv'\n",
        "    pseudo_labels_df.to_csv(training_data_path, index=False)\n",
        "    print(f\"‚úÖ Training data saved: {training_data_path}\")\n",
        "\n",
        "    model_info[\"training_data_path\"] = training_data_path\n",
        "    model_info[\"training_data_size\"] = len(pseudo_labels_df)\n",
        "\n",
        "    # Save label distribution\n",
        "    label_dist = pseudo_labels_df['pred_label'].value_counts().to_dict()\n",
        "    model_info[\"training_label_distribution\"] = label_dist\n",
        "    print(f\"   Training Labels: {label_dist}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è No training data to save\")\n",
        "    model_info[\"training_data_path\"] = None\n",
        "\n",
        "# Save prediction results\n",
        "if 'hf_results' in locals():\n",
        "    print(f\"\\nüíæ SAVING PREDICTION RESULTS\")\n",
        "\n",
        "    predictions_path = f'data/predictions/predictions_{timestamp}.csv'\n",
        "    hf_results.to_csv(predictions_path, index=False)\n",
        "    print(f\"‚úÖ Predictions saved: {predictions_path}\")\n",
        "\n",
        "    model_info[\"predictions_path\"] = predictions_path\n",
        "    model_info[\"predictions_count\"] = len(hf_results)\n",
        "\n",
        "    # Add performance metrics\n",
        "    pred_dist = hf_results['pred_label'].value_counts().to_dict()\n",
        "    avg_confidence = hf_results['confidence'].mean()\n",
        "\n",
        "    model_info[\"performance_metrics\"] = {\n",
        "        \"prediction_distribution\": pred_dist,\n",
        "        \"average_confidence\": round(float(avg_confidence), 4),\n",
        "        \"total_processed\": len(hf_results)\n",
        "    }\n",
        "    print(f\"   Prediction Labels: {pred_dist}\")\n",
        "    print(f\"   Average Confidence: {avg_confidence:.4f}\")\n",
        "\n",
        "# Save comprehensive model information\n",
        "model_info_path = f'results/model_info/model_info_{timestamp}.json'\n",
        "with open(model_info_path, 'w') as f:\n",
        "    json.dump(model_info, f, indent=2, default=str)\n",
        "\n",
        "print(f\"‚úÖ Model info saved: {model_info_path}\")\n",
        "\n",
        "# Create deployment-ready structure for inference pipeline\n",
        "print(f\"\\nüöÄ CREATING DEPLOYMENT STRUCTURE\")\n",
        "\n",
        "# Create data/actual with sample data for inference pipeline\n",
        "os.makedirs('data/actual', exist_ok=True)\n",
        "\n",
        "# If we have results, save a sample to data/actual for the inference pipeline\n",
        "if 'hf_results' in locals() and len(hf_results) > 0:\n",
        "    # Create sample data for inference testing\n",
        "    sample_actual = hf_results[['id', 'text']].head(3).copy()\n",
        "    sample_actual.to_csv('data/actual/sample_reviews.csv', index=False)\n",
        "    print(\"‚úÖ Sample data for inference: data/actual/sample_reviews.csv\")\n",
        "\n",
        "# Save latest model paths for inference pipeline\n",
        "latest_model_config = {\n",
        "    \"latest_model_info\": model_info_path,\n",
        "    \"training_mode\": training_mode,\n",
        "    \"custom_model_path\": model_info.get(\"custom_model_path\"),\n",
        "    \"auxiliary_models\": auxiliary_info,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
        "    \"timestamp\": timestamp\n",
        "}\n",
        "\n",
        "with open('models/saved_models/latest_config.json', 'w') as f:\n",
        "    json.dump(latest_model_config, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Latest model config: models/saved_models/latest_config.json\")\n",
        "\n",
        "# Create model loading utilities for inference\n",
        "model_loader_code = '''\"\"\"\n",
        "Model Loading Utilities for Review Classification Pipeline\n",
        "Generated automatically during training\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "def load_latest_models():\n",
        "    \"\"\"Load the most recently trained models\"\"\"\n",
        "\n",
        "    config_path = 'models/saved_models/latest_config.json'\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(\"No trained models found. Run training pipeline first.\")\n",
        "\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # Load custom model if available\n",
        "    if config['custom_model_path'] and os.path.exists(config['custom_model_path']):\n",
        "        models['custom'] = {\n",
        "            'tokenizer': AutoTokenizer.from_pretrained(config['custom_model_path']),\n",
        "            'model': AutoModelForSequenceClassification.from_pretrained(config['custom_model_path'])\n",
        "        }\n",
        "\n",
        "    # Load auxiliary models\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    aux_config = config['auxiliary_models']\n",
        "\n",
        "    models['auxiliary'] = {\n",
        "    'toxicity': pipeline(\"text-classification\",\n",
        "                         model=aux_config['toxicity_model'], device=device),\n",
        "    'zero_shot': pipeline(\"zero-shot-classification\",\n",
        "                          model=aux_config['zero_shot_model'], device=device),\n",
        "}\n",
        "\n",
        "\n",
        "    return models, config\n",
        "\n",
        "def predict_review(text, models, config):\n",
        "    \"\"\"Make prediction using loaded models\"\"\"\n",
        "\n",
        "    if 'custom' in models:\n",
        "        # Use fine-tuned model\n",
        "        inputs = models['custom']['tokenizer'](\n",
        "            text, truncation=True, padding=True,\n",
        "            max_length=256, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = models['custom']['model'](**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            confidence = float(torch.max(predictions))\n",
        "            predicted_class = int(torch.argmax(predictions))\n",
        "\n",
        "        label = \"REJECT\" if predicted_class == 1 else \"APPROVE\"\n",
        "\n",
        "    else:\n",
        "        # Use zero-shot classification\n",
        "        # Implementation would mirror the training notebook logic\n",
        "        label = \"APPROVE\"  # Placeholder\n",
        "        confidence = 0.5\n",
        "\n",
        "    return {\n",
        "        'label': label,\n",
        "        'confidence': confidence,\n",
        "        'model_type': config['training_mode']\n",
        "    }\n",
        "'''\n",
        "\n",
        "Path(\"src/utils\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"src/__init__.py\").touch(exist_ok=True)\n",
        "Path(\"src/utils/__init__.py\").touch(exist_ok=True)\n",
        "\n",
        "with open('src/utils/model_loader.py', 'w') as f:\n",
        "    f.write(model_loader_code)\n",
        "\n",
        "print(\"‚úÖ Model loader utilities: src/utils/model_loader.py\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"PERSISTENCE COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"‚úÖ Training Mode: {training_mode}\")\n",
        "print(f\"‚úÖ Timestamp: {timestamp}\")\n",
        "\n",
        "if model_info.get(\"custom_model_path\"):\n",
        "    print(f\"‚úÖ Custom Model: {model_info['custom_model_path']}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Custom Model: None (using pre-trained)\")\n",
        "\n",
        "print(f\"‚úÖ Model Info: {model_info_path}\")\n",
        "print(f\"‚úÖ Latest Config: models/saved_models/latest_config.json\")\n",
        "\n",
        "if has_training_data:\n",
        "    print(f\"‚úÖ Training Data: {model_info['training_data_path']}\")\n",
        "    print(f\"   Size: {model_info['training_data_size']} examples\")\n",
        "\n",
        "if 'hf_results' in locals():\n",
        "    print(f\"‚úÖ Predictions: {model_info['predictions_path']}\")\n",
        "    print(f\"   Processed: {model_info['predictions_count']} reviews\")\n",
        "\n",
        "print(f\"\\nüéØ READY FOR INFERENCE PIPELINE\")\n",
        "print(f\"   Use: models/saved_models/latest_config.json\")\n",
        "print(f\"   Data: data/actual/sample_reviews.csv\")\n",
        "print(f\"   Load: src/utils/model_loader.py\")\n",
        "\n",
        "print(f\"\\n‚úÖ MODEL TRAINING AND PERSISTENCE COMPLETE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "981d961c",
      "metadata": {
        "id": "981d961c"
      },
      "source": [
        "## 10. Model Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e21722",
      "metadata": {
        "id": "e7e21722"
      },
      "outputs": [],
      "source": [
        "# --- Model Performance Evaluation with Ground Truth (robust) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: if you don't want seaborn, comment out the next 2 lines\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# 1) Choose the ground-truth dataframe you actually used to create predictions\n",
        "#    Prefer train_df (pseudo-labels) if available; otherwise fall back to df.\n",
        "GT_DF = None\n",
        "if 'train_df' in globals():\n",
        "    GT_DF = train_df\n",
        "elif 'df' in globals():\n",
        "    GT_DF = df\n",
        "\n",
        "def _to_str_label(x):\n",
        "    \"\"\"Map numeric/boolean to APPROVE/REJECT and normalize strings.\"\"\"\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    if isinstance(x, (int, np.integer, float, np.floating)):\n",
        "        # interpret 1 = REJECT, 0 = APPROVE\n",
        "        return \"REJECT\" if int(x) == 1 else \"APPROVE\"\n",
        "    s = str(x).strip().upper()\n",
        "    if s in {\"APPROVE\", \"REJECT\"}:\n",
        "        return s\n",
        "    # try a few loose mappings\n",
        "    if s in {\"YES\", \"SPAM\", \"BLOCK\"}:\n",
        "        return \"REJECT\"\n",
        "    if s in {\"NO\", \"HAM\", \"ALLOW\"}:\n",
        "        return \"APPROVE\"\n",
        "    return np.nan  # unknown -> will be filtered out\n",
        "\n",
        "def evaluate_model_with_ground_truth(pred_df: pd.DataFrame, gt_df: pd.DataFrame | None):\n",
        "    print(\"\\nModel Performance Evaluation\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    df_eval = pred_df.copy()\n",
        "\n",
        "    # 2) Attach ground truth if needed\n",
        "    if gt_df is not None and 'gold_label' not in df_eval.columns:\n",
        "        if 'id' in df_eval.columns and 'id' in gt_df.columns:\n",
        "            df_eval = df_eval.merge(gt_df[['id','gold_label','gold_category']], on='id', how='left')\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Could not merge ground truth by 'id' (missing column).\")\n",
        "    elif gt_df is None and 'gold_label' not in df_eval.columns:\n",
        "        print(\"No ground truth available - cannot evaluate accuracy.\")\n",
        "        return None\n",
        "\n",
        "    if 'gold_label' not in df_eval.columns:\n",
        "        print(\"No ground truth column 'gold_label' found after merge.\")\n",
        "        return None\n",
        "\n",
        "    # 3) Normalize labels to strings and drop NaNs\n",
        "    df_eval['gold_label_norm'] = df_eval['gold_label'].apply(_to_str_label)\n",
        "    df_eval['pred_label_norm'] = df_eval['pred_label'].apply(_to_str_label)\n",
        "\n",
        "    before = len(df_eval)\n",
        "    df_eval = df_eval.dropna(subset=['gold_label_norm', 'pred_label_norm'])\n",
        "    after = len(df_eval)\n",
        "    if after < before:\n",
        "        print(f\"‚ÑπÔ∏è Dropped {before-after} rows with missing/unknown labels after normalization.\")\n",
        "\n",
        "    # 4) Guard: ensure only APPROVE/REJECT remain\n",
        "    allowed = {\"APPROVE\",\"REJECT\"}\n",
        "    bad_gt = set(df_eval['gold_label_norm']) - allowed\n",
        "    bad_pr = set(df_eval['pred_label_norm']) - allowed\n",
        "    if bad_gt or bad_pr:\n",
        "        print(f\"‚ö†Ô∏è Unexpected labels present. gold={bad_gt}, pred={bad_pr}\")\n",
        "\n",
        "    # 5) Compute metrics\n",
        "    y_true = df_eval['gold_label_norm']\n",
        "    y_pred = df_eval['pred_label_norm']\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred, labels=['APPROVE','REJECT'])\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=['APPROVE','REJECT'])\n",
        "\n",
        "    print(f\"Overall Accuracy: {acc:.3f}\\n\")\n",
        "    print(\"Detailed Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # 6) Confusion matrix plot\n",
        "    try:\n",
        "        plt.figure(figsize=(6,5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['APPROVE','REJECT'],\n",
        "                    yticklabels=['APPROVE','REJECT'])\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.show()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 7) Simple error listing\n",
        "    errs = df_eval[y_true != y_pred]\n",
        "    if len(errs):\n",
        "        print(f\"\\nError Analysis ({len(errs)} mistakes / {len(df_eval)} rows)\")\n",
        "        print(\"=\"*50)\n",
        "        for _, r in errs.iterrows():\n",
        "            txt = r['text'][:120] + (\"‚Ä¶\" if isinstance(r['text'], str) and len(r['text'])>120 else \"\")\n",
        "            print(f\"ID {r.get('id','?')}: true={r['gold_label_norm']} pred={r['pred_label_norm']} | {txt}\")\n",
        "    else:\n",
        "        print(\"\\nNo classification errors found.\")\n",
        "\n",
        "    return {'accuracy': acc, 'n': len(df_eval), 'errors': int(len(errs))}\n",
        "\n",
        "# ---- Run evaluation on your predictions ----\n",
        "if 'hf_results' in globals():\n",
        "    eval_out = evaluate_model_with_ground_truth(hf_results, GT_DF)\n",
        "else:\n",
        "    print(\"No prediction results found - run inference first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598ac73c",
      "metadata": {
        "id": "598ac73c"
      },
      "source": [
        "## 11. Pipeline Summary and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5638aff5",
      "metadata": {
        "id": "5638aff5"
      },
      "outputs": [],
      "source": [
        "# Complete Pipeline Summary and Architecture Validation\n",
        "print(\"REVIEW-RATER PIPELINE ARCHITECTURE 2.0\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we're in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Environment Summary\n",
        "print(f\"\\n1. ENVIRONMENT SETUP\")\n",
        "print(f\"   Platform: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"   GPU Available: {'‚úÖ Yes' if torch.cuda.is_available() else '‚ùå No'}\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Directory Structure Validation\n",
        "print(f\"\\n2. DIRECTORY STRUCTURE\")\n",
        "expected_dirs = [\n",
        "    'data/raw', 'data/clean', 'data/pseudo-label', 'data/training',\n",
        "    'data/testing', 'data/actual', 'data/sample',\n",
        "    'models/saved_models', 'models/cache',\n",
        "    'results/predictions', 'results/inference'\n",
        "]\n",
        "\n",
        "for directory in expected_dirs:\n",
        "    status = \"‚úÖ\" if os.path.exists(directory) else \"‚ùå\"\n",
        "    print(f\"   {status} {directory}\")\n",
        "\n",
        "# Pipeline Architecture Summary\n",
        "print(f\"\\n3. PIPELINE ARCHITECTURE\")\n",
        "print(f\"   Training Flow (00_ipynb):\")\n",
        "print(f\"      data/raw ‚Üí (external) ‚Üí data/clean\")\n",
        "print(f\"      data/clean ‚Üí (gemini) ‚Üí data/pseudo-label\")\n",
        "print(f\"      data/pseudo-label ‚Üí data/testing + data/training\")\n",
        "print(f\"      data/clean ‚Üí data/training (combined)\")\n",
        "print(f\"      HuggingFace training on data/training with feedback loop\")\n",
        "print(f\"      Trained models ‚Üí models/saved_models\")\n",
        "print(f\"\")\n",
        "print(f\"   Inference Flow (01_ipynb):\")\n",
        "print(f\"      data/actual ‚Üí models/saved_models ‚Üí inference ‚Üí results/inference\")\n",
        "\n",
        "# Component Status\n",
        "print(f\"\\n4. COMPONENT STATUS\")\n",
        "components = {\n",
        "    'Constants loaded': 'DEFAULT_MODELS' in globals(),\n",
        "    'Sample data ready': 'df' in locals() or 'sample_df' in locals(),\n",
        "    'HuggingFace ready': True,  # Installed in environment setup\n",
        "    'Gemini available': 'gemini_available' in locals() and locals().get('gemini_available', False),\n",
        "    'Directory structure': all(os.path.exists(d) for d in ['data/clean', 'data/pseudo-label', 'data/actual']),\n",
        "}\n",
        "\n",
        "for component, status in components.items():\n",
        "    print(f\"   {'‚úÖ' if status else '‚ùå'} {component}\")\n",
        "\n",
        "# Model Performance Summary\n",
        "print(f\"\\n5. MODEL PERFORMANCE\")\n",
        "prediction_data = None\n",
        "for var_name in ['hf_results', 'all_predictions_df', 'predictions_df', 'results_df']:\n",
        "    if var_name in globals():\n",
        "        var_value = globals()[var_name]\n",
        "        if hasattr(var_value, 'shape') and len(var_value) > 0:\n",
        "            prediction_data = var_value\n",
        "            break\n",
        "\n",
        "if prediction_data is not None:\n",
        "    print(f\"   ‚úÖ Predictions available: {len(prediction_data)} reviews\")\n",
        "    if 'confidence' in prediction_data.columns:\n",
        "        avg_conf = prediction_data['confidence'].mean()\n",
        "        print(f\"   ‚úÖ Average confidence: {avg_conf:.3f}\")\n",
        "    if 'pred_label' in prediction_data.columns:\n",
        "        label_dist = prediction_data['pred_label'].value_counts()\n",
        "        print(f\"   ‚úÖ Label distribution: {dict(label_dist)}\")\n",
        "else:\n",
        "    print(f\"   ‚ùå No prediction data available\")\n",
        "\n",
        "# Integration Readiness\n",
        "print(f\"\\n6. INTEGRATION READINESS\")\n",
        "integration_checks = {\n",
        "    'Structured output': prediction_data is not None,\n",
        "    'Spam detection ready': True,  # Architecture supports it\n",
        "    'Production deployment': os.path.exists('data/actual'),\n",
        "    'Model persistence': 'save_trained_pipeline' in globals(),\n",
        "    'Inference pipeline': os.path.exists('notebooks/01_inference_pipeline.ipynb'),\n",
        "}\n",
        "\n",
        "for check, status in integration_checks.items():\n",
        "    print(f\"   {'‚úÖ' if status else '‚ùå'} {check}\")\n",
        "\n",
        "# Next Steps\n",
        "print(f\"\\n7. NEXT STEPS\")\n",
        "print(f\"   Training Phase (This Notebook):\")\n",
        "print(f\"   1. ‚úÖ Environment setup complete\")\n",
        "print(f\"   2. ‚úÖ Directory structure created\")\n",
        "print(f\"   3. ‚úÖ Pipeline architecture established\")\n",
        "print(f\"   4. üîÑ Run HuggingFace pipeline (cell 8)\")\n",
        "print(f\"   5. üîÑ Export trained models (cell 9)\")\n",
        "print(f\"\")\n",
        "print(f\"   Production Phase:\")\n",
        "print(f\"   1. üìã Place actual review data in data/actual/\")\n",
        "print(f\"   2. üìã Run 01_inference_pipeline.ipynb\")\n",
        "print(f\"   3. üìã Check results in results/inference/\")\n",
        "\n",
        "# Final Status\n",
        "print(f\"\\n8. OVERALL STATUS\")\n",
        "overall_ready = all([\n",
        "    os.path.exists('data/clean'),\n",
        "    os.path.exists('data/actual'),\n",
        "    'DEFAULT_MODELS' in globals(),\n",
        "    'save_trained_pipeline' in globals()\n",
        "])\n",
        "\n",
        "if overall_ready:\n",
        "    print(f\"   üöÄ PIPELINE READY FOR PRODUCTION\")\n",
        "    print(f\"   ‚úÖ Training architecture: Complete\")\n",
        "    print(f\"   ‚úÖ Inference architecture: Complete\")\n",
        "    print(f\"   ‚úÖ Data flow: Established\")\n",
        "    print(f\"   ‚úÖ Integration points: Ready\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  PIPELINE SETUP IN PROGRESS\")\n",
        "    print(f\"   Run all cells to complete setup\")\n",
        "\n",
        "print(f\"\\nPIPELINE ARCHITECTURE 2.0 SUMMARY\")\n",
        "print(f\"=\" * 70)\n",
        "print(f\"‚úÖ data/raw ‚Üí data/clean ‚Üí data/pseudo-label ‚Üí data/training/testing\")\n",
        "print(f\"‚úÖ HuggingFace training with Gemini feedback loop\")\n",
        "print(f\"‚úÖ models/saved_models for production deployment\")\n",
        "print(f\"‚úÖ data/actual ‚Üí 01_ipynb ‚Üí results/inference\")\n",
        "print(f\"‚úÖ Spam detection integration ready\")\n",
        "print(f\"‚úÖ Complete separation of training and inference phases\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}