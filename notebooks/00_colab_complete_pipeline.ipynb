{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tathan08/review-rater/blob/main/notebooks/00_colab_complete_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6da5986",
      "metadata": {
        "id": "c6da5986"
      },
      "source": [
        "# Pipeline Architecture 2.0 - Training Phase\n",
        "\n",
        "## Data Flow Overview\n",
        "\n",
        "```\n",
        "data/raw → (external processing) → data/clean\n",
        "data/clean → (00_ipynb + gemini) → data/pseudo-label  \n",
        "data/pseudo-label → data/testing + data/training\n",
        "data/clean → data/training (combined)\n",
        "HuggingFace model trained on data/training with feedback loop against gemini\n",
        "Trained models → models/saved_models\n",
        "```\n",
        "\n",
        "## Directory Structure\n",
        "\n",
        "### Data Directories\n",
        "- **`data/raw`**: Raw input data (processed externally)\n",
        "- **`data/clean`**: Cleaned/processed data from data/raw\n",
        "- **`data/pseudo-label`**: Pseudo-labeled data generated by Gemini from data/clean\n",
        "- **`data/training`**: Training data (combination of data/clean + data/pseudo-label)\n",
        "- **`data/testing`**: Testing data split from data/pseudo-label\n",
        "- **`data/actual`**: Production data for inference (used by 01_inference_pipeline.ipynb)\n",
        "\n",
        "### Model Directories\n",
        "- **`models/saved_models`**: Trained models ready for production\n",
        "- **`models/cache`**: Model cache files\n",
        "\n",
        "### Results Directories\n",
        "- **`results/predictions`**: Training predictions and evaluations\n",
        "- **`results/inference`**: Production inference results\n",
        "\n",
        "## Pipeline Components\n",
        "\n",
        "### Training Phase (This Notebook - 00_ipynb)\n",
        "1. **Environment Setup**: Install packages, configure GPU\n",
        "2. **Data Processing**: Create directory structure, load sample data\n",
        "3. **Gemini Pseudo-Labeling**: Generate high-quality labels for training\n",
        "4. **HuggingFace Training**: Train models with feedback loop against Gemini\n",
        "5. **Model Export**: Save trained models to models/saved_models\n",
        "\n",
        "### Inference Phase (01_ipynb)\n",
        "1. **Load Trained Models**: From models/saved_models\n",
        "2. **Process Production Data**: From data/actual\n",
        "3. **Generate Predictions**: Using trained pipeline\n",
        "4. **Save Results**: To results/inference\n",
        "\n",
        "## Integration Points\n",
        "- **Spam Detection**: Pipeline ready for spam detection model integration\n",
        "- **Feedback Loop**: HuggingFace model iteratively improved against Gemini predictions\n",
        "- **Production Ready**: Complete separation of training and inference phases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b975af5e",
      "metadata": {
        "id": "b975af5e"
      },
      "source": [
        "# Review Classification Pipeline - Complete Google Colab Implementation\n",
        "\n",
        "This notebook implements the complete review classification pipeline for detecting Google review policy violations, fully configured for Google Colab.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "### Phase 1: Environment Setup and Data Structure\n",
        "- Install all required packages (transformers, torch, google-generativeai, etc.)\n",
        "- Create proper directory structure (data/clean, data/pseudo-label, etc.)\n",
        "- Load sample data for demonstration\n",
        "\n",
        "### Phase 2: Core Pipeline Components\n",
        "- **Ollama Pipeline**: Local LLM classification (for reference, not runnable in Colab)\n",
        "- **HuggingFace Pipeline**: Zero-shot classification using pre-trained models\n",
        "- **Gemini Pseudo-Labeling**: High-quality label generation for training data\n",
        "- **Ensemble Method**: Combines multiple approaches for best results\n",
        "\n",
        "### Phase 3: Future Spam Detection Integration\n",
        "- Pipeline output will be piped into a spam detection model\n",
        "- Structured JSON output format for downstream processing\n",
        "- Confidence scoring for reliable filtering\n",
        "\n",
        "### Phase 4: Evaluation and Analysis\n",
        "- Comprehensive performance metrics\n",
        "- Policy category accuracy assessment\n",
        "- Model comparison and improvement recommendations\n",
        "\n",
        "**Key Features:**\n",
        "- **Policy Categories**: No_Ads, Irrelevant, Rant_No_Visit detection\n",
        "- **Zero Setup**: Everything configured for Google Colab\n",
        "- **Extensible**: Ready for spam detection integration\n",
        "- **Production Ready**: Structured output and comprehensive evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5462d932",
      "metadata": {
        "id": "5462d932"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "96f35169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96f35169",
        "outputId": "e8f3e9d5-91b2-4a2b-dbaa-b4136bcbb1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Core packages installed successfully!\n",
            "Using device: cpu\n",
            "Using CPU - models will run slower but still functional\n",
            "Environment configured for optimal performance\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for the complete pipeline\n",
        "!pip install -q transformers==4.43.3 torch pandas scikit-learn\n",
        "!pip install -q google-generativeai tqdm datasets accelerate\n",
        "!pip install -q ipywidgets matplotlib seaborn wordcloud\n",
        "\n",
        "print(\"✅ Core packages installed successfully!\")\n",
        "\n",
        "# Check GPU availability and setup device\n",
        "import torch\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"Using CPU - models will run slower but still functional\")\n",
        "\n",
        "# Set environment for optimal performance\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid warnings\n",
        "print(\"Environment configured for optimal performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d42837",
      "metadata": {
        "id": "77d42837"
      },
      "source": [
        "## 2. Project Structure Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "c0558d7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0558d7e",
        "outputId": "a15fc316-dce4-47ad-b837-ec210b3c9551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Complete directory structure created!\n",
            "Created 20 directories\n",
            "✅ data/clean\n",
            "✅ data/pseudo-label\n",
            "✅ data/sample\n",
            "✅ results/predictions\n",
            "\n",
            "Directory structure matches production pipeline!\n"
          ]
        }
      ],
      "source": [
        "# Create complete directory structure matching the actual pipeline\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create all necessary directories (matching actual pipeline structure)\n",
        "directories = [\n",
        "    # Source code structure\n",
        "    'src/config', 'src/core', 'src/pseudo_labelling', 'src/pipeline', 'src/integration',\n",
        "\n",
        "    # Data directories (matching actual structure)\n",
        "    'data/raw',           # For raw input data\n",
        "    'data/clean',         # For cleaned/processed data (from data/raw)\n",
        "    'data/pseudo-label',  # For pseudo-labeled data from Gemini (from data/clean)\n",
        "    'data/training',      # For training data split (from data/clean + data/pseudo-label)\n",
        "    'data/testing',       # For testing data split (from data/pseudo-label)\n",
        "    'data/actual',        # For actual production data to be processed by 01_inference_pipeline.ipynb\n",
        "    'data/sample',        # For sample data\n",
        "\n",
        "    # Results directories\n",
        "    'results/predictions',   # All predictions\n",
        "    'results/evaluations',   # For evaluation results\n",
        "    'results/reports',       # For generated reports\n",
        "\n",
        "    # Other directories\n",
        "    'models/saved_models',   # For trained models\n",
        "    'models/cache',          # For model cache\n",
        "    'logs/pipeline_logs',    # For pipeline logs\n",
        "    'prompts',               # Prompt engineering\n",
        "    'docs'                   # Documentation\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    # Create __init__.py files for Python packages\n",
        "    if directory.startswith('src/'):\n",
        "        with open(f'{directory}/__init__.py', 'w') as f:\n",
        "            f.write('# Review Classification Pipeline Package\\n')\n",
        "\n",
        "print(\"✅ Complete directory structure created!\")\n",
        "print(f\"Created {len(directories)} directories\")\n",
        "\n",
        "# Verify critical directories exist\n",
        "critical_dirs = ['data/clean', 'data/pseudo-label', 'data/sample', 'results/predictions']\n",
        "for dir_name in critical_dirs:\n",
        "    if os.path.exists(dir_name):\n",
        "        print(f\"✅ {dir_name}\")\n",
        "    else:\n",
        "        print(f\"❌ {dir_name} - MISSING!\")\n",
        "\n",
        "print(\"\\nDirectory structure matches production pipeline!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc18cdbb",
      "metadata": {
        "id": "bc18cdbb"
      },
      "source": [
        "## 3. Sample Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "8856980d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8856980d",
        "outputId": "77c8b454-c47c-4da3-f5aa-cce3a8046a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Production sample data loaded!\n",
            "\n",
            "Sample Data Overview:\n",
            " id                                                             text gold_label gold_category\n",
            "  1          Use my promo code EAT10 for 10% off! DM me on WhatsApp.     REJECT        No_Ads\n",
            "  2     Great laksa; broth was rich and staff friendly. Will return.    APPROVE          None\n",
            "  3 Crypto is the future. Buy BTC now! Nothing to do with this cafe.     REJECT    Irrelevant\n",
            "  4                          Overpriced scammers. Society is doomed.     REJECT Rant_No_Visit\n",
            "  5 Visited on 18 Aug, ordered set A; cashier fixed a double-charge.    APPROVE          None\n",
            "\n",
            "Label Distribution:\n",
            "APPROVE: 2 reviews\n",
            "REJECT:  3 reviews\n",
            "\n",
            "Category Distribution:\n",
            "None: 2 reviews\n",
            "No_Ads: 1 reviews\n",
            "Irrelevant: 1 reviews\n",
            "Rant_No_Visit: 1 reviews\n",
            "\n",
            "This data demonstrates all policy violation types:\n",
            "• No_Ads: Promotional codes and contact solicitation\n",
            "• Irrelevant: Off-topic content unrelated to business\n",
            "• Rant_No_Visit: Generic negative comments without visit evidence\n",
            "• None: Legitimate reviews that should be approved\n"
          ]
        }
      ],
      "source": [
        "# Load actual sample data from the production pipeline\n",
        "sample_data = {\n",
        "    'id': [1, 2, 3, 4, 5],\n",
        "    'text': [\n",
        "        \"Use my promo code EAT10 for 10% off! DM me on WhatsApp.\",\n",
        "        \"Great laksa; broth was rich and staff friendly. Will return.\",\n",
        "        \"Crypto is the future. Buy BTC now! Nothing to do with this cafe.\",\n",
        "        \"Overpriced scammers. Society is doomed.\",\n",
        "        \"Visited on 18 Aug, ordered set A; cashier fixed a double-charge.\"\n",
        "    ],\n",
        "    'gold_label': ['REJECT', 'APPROVE', 'REJECT', 'REJECT', 'APPROVE'],\n",
        "    'gold_category': ['No_Ads', 'None', 'Irrelevant', 'Rant_No_Visit', 'None']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "df.to_csv('data/sample/sample_reviews.csv', index=False)\n",
        "\n",
        "print(\"✅ Production sample data loaded!\")\n",
        "print(\"\\nSample Data Overview:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "print(f\"APPROVE: {len(df[df['gold_label'] == 'APPROVE'])} reviews\")\n",
        "print(f\"REJECT:  {len(df[df['gold_label'] == 'REJECT'])} reviews\")\n",
        "\n",
        "print(f\"\\nCategory Distribution:\")\n",
        "for category in df['gold_category'].value_counts().index:\n",
        "    count = df['gold_category'].value_counts()[category]\n",
        "    print(f\"{category}: {count} reviews\")\n",
        "\n",
        "print(f\"\\nThis data demonstrates all policy violation types:\")\n",
        "print(\"• No_Ads: Promotional codes and contact solicitation\")\n",
        "print(\"• Irrelevant: Off-topic content unrelated to business\")\n",
        "print(\"• Rant_No_Visit: Generic negative comments without visit evidence\")\n",
        "print(\"• None: Legitimate reviews that should be approved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1014facd",
      "metadata": {
        "id": "1014facd"
      },
      "source": [
        "## 4. Configuration Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "a97baccb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a97baccb",
        "outputId": "d9a0d86c-5473-4523-ed15-95a49c71f995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Configuration created matching production pipeline!\n",
            "Data directory: data/sample\n",
            "HF Zero-shot model: facebook/bart-large-mnli\n",
            "Ensemble tau: 0.55\n",
            "Predictions output: results/predictions/predictions_hf.csv\n"
          ]
        }
      ],
      "source": [
        "# Create configuration classes matching the actual pipeline\n",
        "config_code = '''\n",
        "\"\"\"\n",
        "Pipeline Configuration Classes - Matching Production Structure\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for model settings\"\"\"\n",
        "    # HuggingFace models (matching actual pipeline)\n",
        "    hf_sentiment_model: str = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "    hf_toxicity_model: str = \"unitary/toxic-bert\"\n",
        "    hf_zero_shot_model: str = \"facebook/bart-large-mnli\"\n",
        "\n",
        "    # Gemini configuration\n",
        "    gemini_model: str = \"gemini-2.5-flash-lite\"\n",
        "\n",
        "    # Confidence thresholds (matching actual pipeline)\n",
        "    sentiment_threshold: float = 0.7\n",
        "    toxicity_threshold: float = 0.5\n",
        "    zero_shot_threshold: float = 0.7\n",
        "    ensemble_tau: float = 0.55\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    \"\"\"Configuration for data paths and settings\"\"\"\n",
        "    data_dir: str = \"data\"\n",
        "    raw_data_dir: str = \"data/raw\"\n",
        "    processed_data_dir: str = \"data/clean\"  # Matches actual structure\n",
        "    sample_data_dir: str = \"data/sample\"\n",
        "    pseudo_label_dir: str = \"data/pseudo-label\"  # Matches actual structure\n",
        "    training_dir: str = \"data/training\"\n",
        "    testing_dir: str = \"data/testing\"\n",
        "\n",
        "    # Default input file\n",
        "    sample_reviews_file: str = \"data/sample/sample_reviews.csv\"\n",
        "\n",
        "@dataclass\n",
        "class OutputConfig:\n",
        "    \"\"\"Configuration for output paths\"\"\"\n",
        "    results_dir: str = \"results\"\n",
        "    predictions_dir: str = \"results/predictions\"\n",
        "    evaluations_dir: str = \"results/evaluations\"\n",
        "    reports_dir: str = \"results/reports\"\n",
        "\n",
        "    # Default output files (matching actual pipeline)\n",
        "    hf_predictions: str = \"results/predictions/predictions_hf.csv\"\n",
        "    ensemble_predictions: str = \"results/predictions/predictions_ens.csv\"\n",
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    \"\"\"Main pipeline configuration combining all components\"\"\"\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    output: OutputConfig = field(default_factory=OutputConfig)\n",
        "\n",
        "    # Gemini configuration\n",
        "    gemini_api_key: str = \"\"\n",
        "\n",
        "    # Pipeline settings\n",
        "    batch_size: int = 32\n",
        "    max_workers: int = 4\n",
        "    cache_predictions: bool = True\n",
        "    verbose_logging: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create directories if they don't exist\"\"\"\n",
        "        directories = [\n",
        "            self.data.raw_data_dir,\n",
        "            self.data.processed_data_dir,\n",
        "            self.data.sample_data_dir,\n",
        "            self.data.pseudo_label_dir,\n",
        "            self.data.training_dir,\n",
        "            self.data.testing_dir,\n",
        "            self.output.predictions_dir,\n",
        "            self.output.evaluations_dir,\n",
        "            self.output.reports_dir\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Global configuration instance\n",
        "config = PipelineConfig()\n",
        "'''\n",
        "\n",
        "with open('src/config/pipeline_config.py', 'w') as f:\n",
        "    f.write(config_code)\n",
        "\n",
        "print(\"✅ Configuration created matching production pipeline!\")\n",
        "\n",
        "# Test configuration\n",
        "exec(config_code)\n",
        "test_config = PipelineConfig()\n",
        "print(f\"Data directory: {test_config.data.sample_data_dir}\")\n",
        "print(f\"HF Zero-shot model: {test_config.model.hf_zero_shot_model}\")\n",
        "print(f\"Ensemble tau: {test_config.model.ensemble_tau}\")\n",
        "print(f\"Predictions output: {test_config.output.hf_predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "becf61f4",
      "metadata": {
        "id": "becf61f4"
      },
      "source": [
        "## 5. Constants and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "f5b648a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5b648a5",
        "outputId": "b3f588b2-b342-4f6e-e362-ed5828ecb9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Constants and prompts created matching production pipeline!\n",
            "Policy categories: ['No_Ads', 'Irrelevant', 'Rant_No_Visit', 'None']\n",
            "Zero-shot model: facebook/bart-large-mnli\n",
            "Default confidence threshold: 0.55\n",
            "Zero-shot labels configured: 4 categories\n"
          ]
        }
      ],
      "source": [
        "# Create constants and prompts matching the actual pipeline\n",
        "constants_code = '''\n",
        "\"\"\"\n",
        "Core Constants - Matching Production Pipeline\n",
        "\"\"\"\n",
        "\n",
        "# Policy Categories (matching actual pipeline)\n",
        "POLICY_CATEGORIES = {\n",
        "    'NO_ADS': 'No_Ads',\n",
        "    'IRRELEVANT': 'Irrelevant',\n",
        "    'RANT_NO_VISIT': 'Rant_No_Visit',\n",
        "    'NONE': 'None'\n",
        "}\n",
        "\n",
        "# Label Types (matching actual pipeline)\n",
        "LABELS = {\n",
        "    'APPROVE': 'APPROVE',\n",
        "    'REJECT': 'REJECT'\n",
        "}\n",
        "\n",
        "# Default Models (matching actual pipeline)\n",
        "DEFAULT_MODELS = {\n",
        "    'SENTIMENT': \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    'TOXIC': \"unitary/toxic-bert\",\n",
        "    'ZERO_SHOT': \"facebook/bart-large-mnli\",\n",
        "    'GEMINI_DEFAULT': \"gemini-2.5-flash-lite\"\n",
        "}\n",
        "\n",
        "# Zero-shot Classification Labels (matching actual pipeline)\n",
        "ZERO_SHOT_LABELS = [\n",
        "    \"an advertisement or promotional solicitation for this business (promo code, referral, links, contact to buy)\",\n",
        "    \"off-topic or unrelated to this business (e.g., politics, crypto, chain messages, personal stories not about this place)\",\n",
        "    \"a generic negative rant about this business without evidence of a visit (short insults, 'scam', 'overpriced', 'worst ever')\",\n",
        "    \"a relevant on-topic description of a visit or experience at this business\"\n",
        "]\n",
        "\n",
        "# Mapping zero-shot labels to policy categories\n",
        "ZERO_SHOT_TO_POLICY = {\n",
        "    ZERO_SHOT_LABELS[0]: POLICY_CATEGORIES['NO_ADS'],\n",
        "    ZERO_SHOT_LABELS[1]: POLICY_CATEGORIES['IRRELEVANT'],\n",
        "    ZERO_SHOT_LABELS[2]: POLICY_CATEGORIES['RANT_NO_VISIT'],\n",
        "    ZERO_SHOT_LABELS[3]: POLICY_CATEGORIES['NONE']\n",
        "}\n",
        "\n",
        "# Confidence Thresholds\n",
        "CONFIDENCE_THRESHOLDS = {\n",
        "    'HIGH': 0.8,\n",
        "    'MEDIUM': 0.6,\n",
        "    'LOW': 0.4,\n",
        "    'DEFAULT': 0.55\n",
        "}\n",
        "'''\n",
        "\n",
        "with open('src/core/constants.py', 'w') as f:\n",
        "    f.write(constants_code)\n",
        "\n",
        "# Create prompt templates (matching actual pipeline)\n",
        "prompts_code = '''\n",
        "\"\"\"\n",
        "Policy Prompts - Matching Production Pipeline\n",
        "\"\"\"\n",
        "\n",
        "# JSON schema all prompts must return\n",
        "TEMPLATE_JSON = \"\"\"Return ONLY JSON with no extra text:\n",
        "{\"label\":\"<APPROVE|REJECT>\",\"category\":\"<No_Ads|Irrelevant|Rant_No_Visit|None>\",\n",
        " \"rationale\":\"<short>\",\"confidence\":<0.0-1.0>,\n",
        " \"flags\":{\"links\":false,\"coupon\":false,\"visit_claimed\":false}}\n",
        "\"\"\"\n",
        "\n",
        "# ===== 1) NO ADS / PROMOTIONAL =====\n",
        "NO_ADS_SYSTEM = \"\"\"You are a content policy checker for location reviews.\n",
        "If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0. Do not reject for other policies.\n",
        "Reject ONLY if the review contains clear advertising or promotional solicitation:\n",
        "- referral/promo/coupon codes, price lists, booking/ordering links, contact-for-order (DM me / WhatsApp / Telegram / email / call), affiliate pitches.\n",
        "Do NOT mark generic off-topic content (e.g., crypto/politics) as Ads unless it includes explicit solicitation to buy or contact.\n",
        "Approve normal experiences even if positive or mentioning 'cheap' or 'good deal'.\n",
        "Output the required JSON only.\n",
        "\"\"\"\n",
        "\n",
        "# ===== 2) IRRELEVANT CONTENT =====\n",
        "IRRELEVANT_SYSTEM = \"\"\"You are checking ONLY for the 'Irrelevant' policy.\n",
        "\n",
        "Decision rule (mutually exclusive):\n",
        "- If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0.\n",
        "- Do not reject for other policies (e.g., Ads or Rant_No_Visit).\n",
        "\n",
        "Reject as Irrelevant when the text is off-topic and unrelated to THIS venue/service:\n",
        "- unrelated politics/news/crypto hype/chain messages/personal stories\n",
        "- generic advice not tied to this place (e.g., 'buy BTC now', 'vote X'), etc.\n",
        "- content about another business or location without discussing this one\n",
        "\n",
        "Return ONLY JSON with fields: label, category, rationale, confidence (0.0–1.0), flags.\n",
        "\"\"\"\n",
        "\n",
        "# ===== 3) RANTS WITHOUT VISIT =====\n",
        "RANT_NO_VISIT_SYSTEM = \"\"\"Reject generic rants or accusations clearly targeting THIS place but with no evidence of a visit.\n",
        "These rants are often:\n",
        "- Short and emotional (e.g., 'Terrible place', 'Worst ever', 'Overpriced scammers')\n",
        "- Broad accusations ('scam', 'rip-off', 'fraud')\n",
        "- Negative judgments about pricing, quality, or character of the venue\n",
        "Reject them even if the reviewer does not explicitly say 'this place/restaurant' — assume negativity is directed at the business being reviewed.\n",
        "Approve only if the reviewer provides concrete evidence of a visit (date, food ordered, staff interaction).\n",
        "Output JSON only.\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(system_text: str, review_text: str, fewshots):\n",
        "    demo = \"\\\\n\\\\n\".join(\n",
        "        [f\"Review:\\\\n{r}\\\\nExpected JSON:\\\\n{j}\" for r,j in fewshots]\n",
        "    )\n",
        "    return f\"\"\"{system_text}\n",
        "\n",
        "{TEMPLATE_JSON}\n",
        "\n",
        "{demo}\n",
        "\n",
        "Now classify this review. Return ONLY JSON.\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\"\n",
        "'''\n",
        "\n",
        "with open('prompts/policy_prompts.py', 'w') as f:\n",
        "    f.write(prompts_code)\n",
        "\n",
        "print(\"✅ Constants and prompts created matching production pipeline!\")\n",
        "\n",
        "# Test constants\n",
        "exec(constants_code)\n",
        "print(f\"Policy categories: {list(POLICY_CATEGORIES.values())}\")\n",
        "print(f\"Zero-shot model: {DEFAULT_MODELS['ZERO_SHOT']}\")\n",
        "print(f\"Default confidence threshold: {CONFIDENCE_THRESHOLDS['DEFAULT']}\")\n",
        "print(f\"Zero-shot labels configured: {len(ZERO_SHOT_LABELS)} categories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6015eaed",
      "metadata": {
        "id": "6015eaed"
      },
      "source": [
        "## 6. Gemini API Configuration and Pseudo-Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "33aa5b43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "33aa5b43",
        "outputId": "144026e0-7740-48fc-8ae0-d2e3b771f836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab secrets not found: Secret GEMINI_API_KEY does not exist.\n",
            "Please enter your Gemini API key manually:\n",
            "\n",
            "Instructions:\n",
            "1. Go to: https://aistudio.google.com/app/apikey\n",
            "2. Click 'Create API Key'\n",
            "3. Copy the key\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3878733885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mGEMINI_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your Gemini API key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mapi_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"manual\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Gemini API Key Setup and Pseudo-Labeling Implementation\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Option A: Try Colab secrets first (recommended)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"✅ Gemini API key loaded from Colab secrets\")\n",
        "    api_source = \"secrets\"\n",
        "except Exception as e:\n",
        "    print(f\"Colab secrets not found: {e}\")\n",
        "\n",
        "    # Option B: Manual input fallback\n",
        "    print(\"Please enter your Gemini API key manually:\")\n",
        "    print(\"\")\n",
        "    print(\"Instructions:\")\n",
        "    print(\"1. Go to: https://aistudio.google.com/app/apikey\")\n",
        "    print(\"2. Click 'Create API Key'\")\n",
        "    print(\"3. Copy the key\")\n",
        "    print(\"\")\n",
        "\n",
        "    GEMINI_API_KEY = input(\"Enter your Gemini API key: \").strip()\n",
        "    api_source = \"manual\"\n",
        "\n",
        "# Configure Gemini\n",
        "if GEMINI_API_KEY:\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "    # Test the API\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "        test_response = model.generate_content(\"Test: Say 'API working'\")\n",
        "        print(f\"✅ Gemini API test successful!\")\n",
        "        print(f\"   Source: {api_source}\")\n",
        "        print(f\"   Model: gemini-2.5-flash-lite\")\n",
        "        gemini_available = True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gemini test failed: {e}\")\n",
        "        print(\"Check your API key and quota limits\")\n",
        "        gemini_available = False\n",
        "else:\n",
        "    print(\"No API key provided\")\n",
        "    print(\"Pipeline will run without Gemini pseudo-labeling\")\n",
        "    print(\"HuggingFace components will still work perfectly!\")\n",
        "    gemini_available = False\n",
        "\n",
        "print(f\"\\nConfiguration Summary:\")\n",
        "print(f\"   Gemini Available: {'✅ Yes' if gemini_available else '❌ No'}\")\n",
        "print(f\"   HuggingFace: ✅ Ready\")\n",
        "print(f\"   Pipeline Mode: {'Full' if gemini_available else 'HuggingFace Only'}\")\n",
        "\n",
        "# Gemini Pseudo-Labeling Implementation (for Training Data Generation)\n",
        "def classify_with_gemini(text: str, model_name=\"gemini-2.0-flash-exp\"):\n",
        "    \"\"\"Generate pseudo-labels using Gemini for training data\"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel(model_name)\n",
        "\n",
        "        # Enhanced prompt for better pseudo-labeling\n",
        "        prompt = f\"\"\"\n",
        "You are a Google review policy expert. Classify this review and provide a JSON response.\n",
        "\n",
        "Review: \"{text}\"\n",
        "\n",
        "Policy Categories:\n",
        "- No_Ads: Contains advertisements, promotional content, promo codes, contact information\n",
        "- Irrelevant: Off-topic content (politics, crypto, unrelated businesses)\n",
        "- Rant_No_Visit: Generic negative rants without evidence of visiting\n",
        "- None: Legitimate review about an actual visit/experience\n",
        "\n",
        "Respond with JSON only:\n",
        "{{\"label\": \"APPROVE\" or \"REJECT\", \"category\": \"policy_name\", \"confidence\": 0.0-1.0, \"rationale\": \"brief_explanation\"}}\n",
        "\"\"\"\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        response_text = response.text.strip()\n",
        "\n",
        "        # Try to parse JSON response\n",
        "        if response_text.startswith('```json'):\n",
        "            response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
        "        elif response_text.startswith('```'):\n",
        "            response_text = response_text.replace('```', '').strip()\n",
        "\n",
        "        try:\n",
        "            result = json.loads(response_text)\n",
        "\n",
        "            # Validate required fields\n",
        "            if all(key in result for key in ['label', 'category', 'confidence']):\n",
        "                return {\n",
        "                    \"label\": result['label'],\n",
        "                    \"category\": result['category'],\n",
        "                    \"confidence\": float(result['confidence']),\n",
        "                    \"rationale\": result.get('rationale', 'Gemini classification')\n",
        "                }\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        # Fallback parsing if JSON fails\n",
        "        if 'REJECT' in response_text.upper():\n",
        "            # Try to extract category\n",
        "            if 'No_Ads' in response_text or 'advertisement' in response_text.lower():\n",
        "                category = 'No_Ads'\n",
        "            elif 'Irrelevant' in response_text or 'off-topic' in response_text.lower():\n",
        "                category = 'Irrelevant'\n",
        "            elif 'Rant_No_Visit' in response_text or 'rant' in response_text.lower():\n",
        "                category = 'Rant_No_Visit'\n",
        "            else:\n",
        "                category = 'No_Ads'  # Default\n",
        "\n",
        "            return {\"label\": \"REJECT\", \"category\": category, \"confidence\": 0.7, \"rationale\": \"Parsed from text\"}\n",
        "        else:\n",
        "            return {\"label\": \"APPROVE\", \"category\": \"None\", \"confidence\": 0.7, \"rationale\": \"Parsed from text\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gemini API error: {e}\")\n",
        "        return {\"label\": \"APPROVE\", \"category\": \"None\", \"confidence\": 0.0, \"rationale\": f\"API error: {e}\"}\n",
        "\n",
        "def generate_pseudo_labels_with_gemini(unlabeled_df, confidence_threshold=0.8, max_labels=50):\n",
        "    \"\"\"Generate high-quality pseudo-labels for training data\"\"\"\n",
        "    if not gemini_available:\n",
        "        print(\"❌ Gemini not available for pseudo-labeling\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Generating pseudo-labels for training data...\")\n",
        "    print(f\"Confidence threshold: {confidence_threshold}\")\n",
        "    print(f\"Max labels to generate: {max_labels}\")\n",
        "\n",
        "    pseudo_labels = []\n",
        "    processed_count = 0\n",
        "\n",
        "    print(f\"Processing {min(len(unlabeled_df), max_labels)} reviews...\")\n",
        "\n",
        "    for _, row in tqdm(unlabeled_df.iterrows(), total=min(len(unlabeled_df), max_labels), desc=\"Generating pseudo-labels\"):\n",
        "        if processed_count >= max_labels:\n",
        "            break\n",
        "\n",
        "        text = str(row['text'])\n",
        "        result = classify_with_gemini(text)\n",
        "\n",
        "        # Only include high-confidence predictions for training\n",
        "        if result['confidence'] >= confidence_threshold:\n",
        "            pseudo_labels.append({\n",
        "                'id': row.get('id', processed_count + 100),\n",
        "                'text': text,\n",
        "                'pred_label': result['label'],\n",
        "                'pred_category': result['category'],\n",
        "                'confidence': result['confidence'],\n",
        "                'rationale': result['rationale'],\n",
        "                'source': 'gemini_pseudo'\n",
        "            })\n",
        "\n",
        "        processed_count += 1\n",
        "        time.sleep(0.1)  # Rate limiting for API\n",
        "\n",
        "        # Progress update\n",
        "        if processed_count % 5 == 0:\n",
        "            print(f\"   Processed {processed_count} reviews, generated {len(pseudo_labels)} high-confidence labels\")\n",
        "\n",
        "    pseudo_df = pd.DataFrame(pseudo_labels)\n",
        "\n",
        "    if len(pseudo_df) > 0:\n",
        "        # Save to proper directory for training\n",
        "        output_path = 'data/pseudo-label/gemini_pseudo_labels.csv'\n",
        "        pseudo_df.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"✅ Generated {len(pseudo_df)} high-confidence pseudo-labels for training\")\n",
        "        print(f\"Saved to: {output_path}\")\n",
        "        print(f\"Label distribution: {pseudo_df['pred_label'].value_counts().to_dict()}\")\n",
        "        print(f\"Category distribution: {pseudo_df['pred_category'].value_counts().to_dict()}\")\n",
        "\n",
        "        # Quality metrics\n",
        "        avg_confidence = pseudo_df['confidence'].mean()\n",
        "        print(f\"Average confidence: {avg_confidence:.3f}\")\n",
        "\n",
        "        return pseudo_df\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No high-confidence pseudo-labels generated\")\n",
        "        print(\"Try lowering confidence threshold or checking API responses\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Generate Training Data with Pseudo-Labels\n",
        "if gemini_available:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GENERATING TRAINING DATA WITH PSEUDO-LABELS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create diverse unlabeled data for pseudo-labeling\n",
        "    unlabeled_data = {\n",
        "        'id': list(range(101, 121)),\n",
        "        'text': [\n",
        "            \"Amazing food and service, definitely coming back!\",\n",
        "            \"Visit our website for exclusive deals and discounts - use code SAVE20\",\n",
        "            \"The worst experience ever, everything was terrible, total scam\",\n",
        "            \"Staff was friendly, food was fresh and tasty, good value for money\",\n",
        "            \"This place is overpriced, never going back, waste of money\",\n",
        "            \"Great atmosphere, perfect for family dinner, ordered the set meal and dessert\",\n",
        "            \"Follow my Instagram @foodie123 for more reviews and promos\",\n",
        "            \"Bitcoin is going to the moon! Buy now before it's too late!\",\n",
        "            \"Had the chicken rice here yesterday, portion was generous and taste was authentic\",\n",
        "            \"DM me for discount codes! Also selling crypto courses online\",\n",
        "            \"Terrible service, rude staff, food was cold when it arrived\",\n",
        "            \"The laksa here reminds me of my grandmother's cooking, very nostalgic\",\n",
        "            \"Check out my YouTube channel for food reviews and crypto tips\",\n",
        "            \"Went here for lunch with colleagues, everyone enjoyed their meals\",\n",
        "            \"Overpriced tourist trap, locals know better places nearby\",\n",
        "            \"Made reservation for 6pm, got seated immediately, excellent service throughout\",\n",
        "            \"Politics in this country is corrupt, restaurants like this are part of the problem\",\n",
        "            \"Their signature dish was perfectly seasoned, will definitely recommend to friends\",\n",
        "            \"Worst restaurant in Singapore, total ripoff, avoid at all costs\",\n",
        "            \"Celebrated my birthday here last week, staff even brought out a cake\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    unlabeled_df = pd.DataFrame(unlabeled_data)\n",
        "    print(f\"Created {len(unlabeled_df)} diverse reviews for pseudo-labeling\")\n",
        "\n",
        "    # Generate pseudo-labels for training\n",
        "    print(f\"\\nStarting Gemini pseudo-labeling for training data...\")\n",
        "    pseudo_labels_df = generate_pseudo_labels_with_gemini(\n",
        "        unlabeled_df,\n",
        "        confidence_threshold=0.7,  # Lower threshold for more training data\n",
        "        max_labels=20\n",
        "    )\n",
        "\n",
        "    if len(pseudo_labels_df) > 0:\n",
        "        print(f\"\\n✅ PSEUDO-LABELING COMPLETE\")\n",
        "        print(f\"Generated {len(pseudo_labels_df)} high-quality labels for training\")\n",
        "        print(f\"Ready for HuggingFace model fine-tuning!\")\n",
        "\n",
        "        # Preview the training data\n",
        "        print(f\"\\nTraining Data Preview:\")\n",
        "        display_cols = ['text', 'pred_label', 'pred_category', 'confidence']\n",
        "        for idx, row in pseudo_labels_df.head(3).iterrows():\n",
        "            text_preview = row['text'][:60] + \"...\" if len(row['text']) > 60 else row['text']\n",
        "            print(f\"   {row['pred_label']} | {row['pred_category']} | {row['confidence']:.2f} | {text_preview}\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No pseudo-labels generated - will use pre-trained models only\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SKIPPING PSEUDO-LABELING (Gemini not available)\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Will proceed with pre-trained HuggingFace models only\")\n",
        "    pseudo_labels_df = pd.DataFrame()\n",
        "\n",
        "print(f\"\\nTraining data preparation: {'✅ Complete' if len(pseudo_labels_df) > 0 else '❌ Skipped'}\")\n",
        "print(f\"Ready for HuggingFace model training: {'✅ Yes' if len(pseudo_labels_df) > 0 else '⚠️ Pre-trained only'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f255cad8",
      "metadata": {
        "id": "f255cad8"
      },
      "source": [
        "## 7. HuggingFace Model Training with Pseudo-Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2235ac4d",
      "metadata": {
        "id": "2235ac4d"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers -q\n",
        "!pip install -qU \"transformers>=4.45.0\" \"accelerate>=0.34.0\" \"huggingface_hub>=0.23.0\"\n",
        "\n",
        "# 7. HuggingFace Model Training with Pseudo-Labels\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding, pipeline as hf_pipeline,\n",
        "    __version__ as HF_VER,   # <-- for version-aware args\n",
        ")\n",
        "from packaging.version import parse                # <-- for version-aware args\n",
        "from datasets import Dataset\n",
        "import torch, numpy as np, pandas as pd, re, os\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"HUGGINGFACE MODEL TRAINING WITH PSEUDO-LABELS (no sentiment, fused policy logic)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ---- import your constants (as provided)\n",
        "from src.core.constants import (\n",
        "    DEFAULT_MODELS, ZERO_SHOT_LABELS, ZERO_SHOT_TO_POLICY,\n",
        "    POLICY_CATEGORIES, LABELS, CONFIDENCE_THRESHOLDS\n",
        ")\n",
        "\n",
        "# ---- device & training mode (same behavior)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "has_training_data = ('pseudo_labels_df' in locals()) and (len(pseudo_labels_df) > 0)\n",
        "training_mode = \"fine-tuning\" if has_training_data else \"pre-trained\"\n",
        "print(\"Mode:\", training_mode)\n",
        "\n",
        "# ---- models (NO sentiment)\n",
        "BASE_MODEL      = \"distilbert-base-uncased\"\n",
        "TOXIC_MODEL     = DEFAULT_MODELS['TOXIC']\n",
        "ZERO_SHOT_MODEL = DEFAULT_MODELS['ZERO_SHOT']\n",
        "\n",
        "# ========= new policy helpers (ad evidence + toxicity gate) =========\n",
        "AD_PATTERNS = [\n",
        "    r\"https?://\", r\"\\bwww\\.\", r\"\\.[a-z]{2,6}\\b\",\n",
        "    r\"\\b(?:\\+?\\d[\\s\\-()]*){7,}\\b\",\n",
        "    r\"\\bpromo(?:\\s*code)?\\b\", r\"\\bdiscount\\b\", r\"\\bcoupon\\b\",\n",
        "    r\"\\breferral\\b\", r\"\\buse\\s*code\\b\", r\"\\benter\\s*code\\b\",\n",
        "    r\"\\bwhatsapp\\b\",\n",
        "    r\"\\bdm\\s+(?:me|us)\\b\",\n",
        "    r\"\\bcontact\\s+(?:us|me)\\b\", r\"\\bcall\\s+(?:us|me)\\b\",\n",
        "]\n",
        "AD_REGEX = re.compile(\"|\".join(AD_PATTERNS), flags=re.IGNORECASE)\n",
        "TOX_TO_RANT = {\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\"}\n",
        "TOX_TO_IRRELEVANT = {\"identity_hate\"}\n",
        "\n",
        "def ad_evidence(text: str):\n",
        "    t = text or \"\"\n",
        "    m = AD_REGEX.search(t)\n",
        "    return bool(m), (m.group(0) if m else \"\")\n",
        "\n",
        "def tox_top_label(tox_output):\n",
        "    \"\"\"\n",
        "    Normalize HF toxicity pipeline outputs into (label, score).\n",
        "    Handles dict, list[dict], list[list[dict]].\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(tox_output, dict):\n",
        "            candidates = [tox_output]\n",
        "        elif isinstance(tox_output, list):\n",
        "            if len(tox_output) and isinstance(tox_output[0], dict):\n",
        "                candidates = tox_output\n",
        "            elif len(tox_output) and isinstance(tox_output[0], list):\n",
        "                candidates = tox_output[0]\n",
        "            else:\n",
        "                candidates = []\n",
        "        else:\n",
        "            candidates = []\n",
        "        if not candidates:\n",
        "            return \"NONE\", 0.0\n",
        "        best = max(candidates, key=lambda d: float(d.get(\"score\", 0.0)))\n",
        "        return best.get(\"label\", \"NONE\"), float(best.get(\"score\", 0.0))\n",
        "    except Exception:\n",
        "        return \"NONE\", 0.0\n",
        "\n",
        "\n",
        "def policy_zero_shot_fused(zshot, toxic, text: str,\n",
        "                           tau_irrelevant=0.55, tau_rant=0.55,\n",
        "                           tau_ads=0.70, tox_tau=0.50, ads_margin=0.10):\n",
        "    \"\"\"\n",
        "    1) Toxicity gate:\n",
        "         - identity_hate -> Irrelevant\n",
        "         - toxic/insult/obscene/threat -> Rant_No_Visit\n",
        "    2) Else if Irrelevant/Rant over thresholds -> that category\n",
        "    3) Else if Ads high + ad evidence + margin -> No_Ads\n",
        "    4) Else -> None\n",
        "    Returns: (pred_label 'APPROVE'/'REJECT', pred_category)\n",
        "    \"\"\"\n",
        "    # zero-shot on your label set\n",
        "    zs_res = zshot(text, candidate_labels=ZERO_SHOT_LABELS,\n",
        "                   hypothesis_template=\"This review is {}.\", multi_label=True)\n",
        "    zs = {lab: float(scr) for lab, scr in zip(zs_res[\"labels\"], zs_res[\"scores\"])}\n",
        "    ads  = zs.get(ZERO_SHOT_LABELS[0], 0.0)\n",
        "    irr  = zs.get(ZERO_SHOT_LABELS[1], 0.0)\n",
        "    rant = zs.get(ZERO_SHOT_LABELS[2], 0.0)\n",
        "    none = zs.get(ZERO_SHOT_LABELS[3], 0.0)\n",
        "\n",
        "    # toxicity top label\n",
        "    tox_label, tox_score = tox_top_label(toxic(text))\n",
        "\n",
        "\n",
        "    if tox_label and tox_score >= tox_tau:\n",
        "        if tox_label in TOX_TO_RANT:\n",
        "            return LABELS['REJECT'], POLICY_CATEGORIES['RANT_NO_VISIT']\n",
        "        if tox_label in TOX_TO_IRRELEVANT:\n",
        "            return LABELS['REJECT'], POLICY_CATEGORIES['IRRELEVANT']\n",
        "\n",
        "    if max(irr, rant) >= min(tau_irrelevant, tau_rant):\n",
        "        return LABELS['REJECT'], (POLICY_CATEGORIES['IRRELEVANT'] if irr >= rant else POLICY_CATEGORIES['RANT_NO_VISIT'])\n",
        "\n",
        "    has_ads, _ = ad_evidence(text)\n",
        "    if has_ads and (ads >= tau_ads) and (ads >= max(irr, rant) + ads_margin):\n",
        "        return LABELS['REJECT'], POLICY_CATEGORIES['NO_ADS']\n",
        "\n",
        "    return LABELS['APPROVE'], POLICY_CATEGORIES['NONE']\n",
        "\n",
        "# ========= metrics & training (binary, same as before) =========\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "def prepare_training_data(pseudo_df):\n",
        "    train_texts  = pseudo_df['text'].tolist()\n",
        "    train_labels = [1 if label == 'REJECT' else 0 for label in pseudo_df['pred_label']]\n",
        "    return train_texts, train_labels\n",
        "\n",
        "def train_custom_classification_model(train_texts, train_labels, model_name=\"review-policy-classifier\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=2)\n",
        "    enc = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "    train_ds = Dataset.from_dict({'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask'], 'labels': train_labels})\n",
        "\n",
        "    # ---- version-aware TrainingArguments (v4 vs v5)\n",
        "    base_kwargs = dict(\n",
        "        output_dir=f'./models/fine-tuned/{model_name}',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'./logs/{model_name}',\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Transformers v5+\n",
        "        args = TrainingArguments(**base_kwargs, eval_strategy=\"epoch\")\n",
        "    except TypeError:\n",
        "        # Transformers v4.x\n",
        "        args = TrainingArguments(**base_kwargs, evaluation_strategy=\"epoch\")\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args,\n",
        "        train_dataset=train_ds, eval_dataset=train_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    save_dir = f'./models/fine-tuned/{model_name}'\n",
        "    trainer.save_model(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"✅ Model saved to: {save_dir}\")\n",
        "    return model, tokenizer, save_dir\n",
        "\n",
        "def load_aux_pipelines(device=None):\n",
        "    toxic = hf_pipeline(\"text-classification\", model=TOXIC_MODEL, top_k=None, device=device)\n",
        "    zshot = hf_pipeline(\"zero-shot-classification\", model=ZERO_SHOT_MODEL, device=device)\n",
        "    return toxic, zshot\n",
        "\n",
        "def predict_with_custom_model(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        conf = float(probs.max())\n",
        "        pred = int(probs.argmax())\n",
        "    return (\"REJECT\" if pred == 1 else \"APPROVE\"), conf\n",
        "\n",
        "def run_inference_pipeline(df, tau=CONFIDENCE_THRESHOLDS['DEFAULT']):\n",
        "    print(f\"\\nRunning inference… ({training_mode})  samples={len(df)}\")\n",
        "    toxic_pipeline, zshot_pipeline = load_aux_pipelines(device)\n",
        "    results = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing reviews\"):\n",
        "        txt = str(row['text'])\n",
        "\n",
        "        # ---- robust access to optional custom model\n",
        "        cc = TRAINED_MODELS.get('custom_classifier')\n",
        "        if cc is not None:\n",
        "            pred_label, confidence = predict_with_custom_model(txt, cc['model'], cc['tokenizer'])\n",
        "            pred_category = \"Custom_Trained\"\n",
        "        else:\n",
        "            pred_label, pred_category = policy_zero_shot_fused(\n",
        "                zshot=zshot_pipeline, toxic=toxic_pipeline, text=txt,\n",
        "                tau_irrelevant=0.55, tau_rant=0.55, tau_ads=0.70, tox_tau=0.50, ads_margin=0.10\n",
        "            )\n",
        "            confidence = None  # rule-based fusion\n",
        "\n",
        "        # Optional: record toxicity top label/score for debugging\n",
        "        try:\n",
        "            tox_label, tox_score = tox_top_label(toxic_pipeline(txt))\n",
        "        except Exception:\n",
        "            tox_label, tox_score = \"NONE\", 0.0\n",
        "\n",
        "        results.append({\n",
        "            \"id\": row.get('id', len(results)+1),\n",
        "            \"text\": txt,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"pred_category\": pred_category,\n",
        "            \"confidence\": (round(float(confidence), 4) if confidence is not None else None),\n",
        "            \"toxicity_label\": tox_label,\n",
        "            \"toxicity_score\": round(float(tox_score), 4),\n",
        "            \"model_type\": training_mode\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    os.makedirs(\"results/predictions\", exist_ok=True)\n",
        "    out_path = f'results/predictions/predictions_{training_mode}.csv'\n",
        "    results_df.to_csv(out_path, index=False)\n",
        "    print(f\"✅ Inference saved → {out_path}  rows={len(results_df)}\")\n",
        "    return results_df\n",
        "\n",
        "# ========= Train (if pseudo-labels) or use pre-trained =========\n",
        "TRAINED_MODELS = {\"custom_classifier\": None}   # <-- init key to avoid KeyError\n",
        "if training_mode == \"fine-tuning\":\n",
        "    print(\"\\n🎯 FINE-TUNING MODE\")\n",
        "    tr_texts, tr_labels = prepare_training_data(pseudo_labels_df)\n",
        "    model, tok, model_path = train_custom_classification_model(tr_texts, tr_labels, \"review-policy-classifier\")\n",
        "    TRAINED_MODELS['custom_classifier'] = {'model_path': model_path, 'model': model, 'tokenizer': tok}\n",
        "else:\n",
        "    print(\"\\n🔄 PRE-TRAINED MODE (no sentiment)\")\n",
        "    TRAINED_MODELS['custom_classifier'] = None\n",
        "\n",
        "# ========= Test on available data (same style) =========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if 'df' in locals() and len(df) > 0:\n",
        "    test_data = df\n",
        "    src = \"Sample data\"\n",
        "elif 'pseudo_labels_df' in locals() and len(pseudo_labels_df) > 0:\n",
        "    test_data = pseudo_labels_df[['id','text']].head(5)\n",
        "    src = \"Pseudo-labeled data\"\n",
        "else:\n",
        "    test_data = pd.DataFrame({'id':[1,2], 'text':[\n",
        "        \"Great food and service!\",\n",
        "        \"Use my promo code SAVE20 for discounts!\"\n",
        "    ]})\n",
        "    src = \"Minimal test data\"\n",
        "\n",
        "print(f\"Testing with: {src} | n={len(test_data)}\")\n",
        "hf_results = run_inference_pipeline(test_data, tau=CONFIDENCE_THRESHOLDS['DEFAULT'])\n",
        "\n",
        "# Pretty print head\n",
        "print(\"\\n📊 INFERENCE RESULTS\")\n",
        "print(\"=\"*50)\n",
        "disp_cols = ['id','text','pred_label','pred_category','confidence','model_type','toxicity_label','toxicity_score']\n",
        "dd = hf_results.copy()\n",
        "dd['text'] = dd['text'].apply(lambda x: x[:50]+\"...\" if isinstance(x,str) and len(x)>50 else x)\n",
        "print(dd[disp_cols].head(10).to_string(index=False))\n",
        "\n",
        "# Summary\n",
        "print(\"\\n📈 RESULTS SUMMARY\")\n",
        "print(hf_results['pred_label'].value_counts())\n",
        "print(f\"Average Confidence (fine-tuned only): {hf_results['confidence'].dropna().mean() if 'confidence' in hf_results else float('nan'):.3f}\")\n",
        "\n",
        "print(\"\\n✅ MODEL TRAINING AND TESTING COMPLETE\")\n",
        "print(f\"Training Mode: {training_mode.upper()}\")\n",
        "print(\"Ready for Model Persistence: ✅ Yes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Unified Spam Detection Model\n"
      ],
      "metadata": {
        "id": "lUbUl5xwsZTZ"
      },
      "id": "lUbUl5xwsZTZ"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Unified spam detection system combining machine learning with pattern analysis.\n",
        "Assumes data has already passed through toxicity/hate speech filtering pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "!pip install textstat\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class DetectionResult:\n",
        "    \"\"\"Container for detection results.\"\"\"\n",
        "    label: str  # 'APPROVE' or 'REJECT'\n",
        "    confidence: float  # 0.0 to 1.0\n",
        "    category: str  # violation category or 'None'\n",
        "    features: Dict  # method-specific features\n",
        "    confidence_interval: Tuple[float, float]\n",
        "\n",
        "class PatternFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract pattern-based features for ML model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Extract pattern features from texts.\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for text in X:\n",
        "            text_features = self._extract_pattern_features(text)\n",
        "\n",
        "            # Convert to feature vector\n",
        "            feature_vector = [\n",
        "                text_features['repetition_ratio'],\n",
        "                text_features['word_diversity_ratio'],\n",
        "                text_features['repeated_ngrams_ratio'],\n",
        "                text_features['caps_ratio'],\n",
        "                text_features['punct_ratio'],\n",
        "                text_features['readability_score'] / 100.0,  # Normalize\n",
        "                text_features['avg_sentence_length'] / 50.0,  # Normalize\n",
        "                text_features['word_count'] / 100.0,  # Normalize\n",
        "                text_features['template_score'],\n",
        "                text_features['phrase_repetition_score'],\n",
        "                text_features['local_repetition_score']\n",
        "            ]\n",
        "            features.append(feature_vector)\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def _extract_pattern_features(self, text: str) -> Dict:\n",
        "        \"\"\"Extract comprehensive pattern features from text.\"\"\"\n",
        "        words = text.split()\n",
        "        word_count = len(words)\n",
        "        char_count = len(text)\n",
        "\n",
        "        # Basic repetition analysis\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            word_lower = word.lower()\n",
        "            word_freq[word_lower] = word_freq.get(word_lower, 0) + 1\n",
        "\n",
        "        repetition_ratio = 0.0\n",
        "        if word_count > 0:\n",
        "            max_repetition = max(word_freq.values()) if word_freq else 1\n",
        "            repetition_ratio = max_repetition / word_count\n",
        "\n",
        "        # Word diversity (unique words / total words)\n",
        "        unique_words = len(set(word.lower() for word in words))\n",
        "        word_diversity_ratio = unique_words / max(word_count, 1)\n",
        "\n",
        "        # N-gram repetition detection (for \"food is good food is great food is nice\")\n",
        "        repeated_ngrams_ratio = self._detect_repeated_ngrams(words)\n",
        "\n",
        "        # Phrase repetition score (detects patterns like \"food is X\" repeating)\n",
        "        phrase_repetition_score = self._detect_phrase_patterns(words)\n",
        "\n",
        "        # Local repetition score (detects repetition in specific text segments)\n",
        "        local_repetition_score = self._detect_local_repetition(words)\n",
        "\n",
        "        # Template detection (common patterns)\n",
        "        template_indicators = [\n",
        "            r'\\b(excellent|amazing|great|good|bad|terrible)\\b.*\\b(food|service|place|restaurant)\\b',\n",
        "            r'\\b(recommend|suggest|try|visit)\\b.*\\b(place|restaurant|here)\\b',\n",
        "            r'\\b(will|would).*\\b(come|go|visit).*\\b(again|back)\\b'\n",
        "        ]\n",
        "\n",
        "        template_matches = sum(1 for pattern in template_indicators\n",
        "                             if re.search(pattern, text, re.IGNORECASE))\n",
        "        template_score = template_matches / len(template_indicators)\n",
        "\n",
        "        # Simple readability proxy (no textstat installed)\n",
        "        words = len(text.split())\n",
        "        sentences = len(re.split(r'[.!?]+', text))\n",
        "        if sentences > 0 and words > 0:\n",
        "            avg_words_per_sentence = words / sentences\n",
        "            readability_score = max(0, min(100, 120 - avg_words_per_sentence * 2))\n",
        "        else:\n",
        "            readability_score = 50.0\n",
        "\n",
        "\n",
        "        # Character-level features\n",
        "        caps_count = sum(1 for c in text if c.isupper())\n",
        "        punct_count = sum(1 for c in text if c in '!?.,;:')\n",
        "\n",
        "        caps_ratio = caps_count / max(char_count, 1)\n",
        "        punct_ratio = punct_count / max(char_count, 1)\n",
        "\n",
        "        # Sentence structure\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        sentence_count = len([s for s in sentences if s.strip()])\n",
        "        avg_sentence_length = word_count / max(sentence_count, 1)\n",
        "\n",
        "        return {\n",
        "            'word_count': word_count,\n",
        "            'char_count': char_count,\n",
        "            'repetition_ratio': repetition_ratio,\n",
        "            'word_diversity_ratio': word_diversity_ratio,\n",
        "            'repeated_ngrams_ratio': repeated_ngrams_ratio,\n",
        "            'phrase_repetition_score': phrase_repetition_score,\n",
        "            'local_repetition_score': local_repetition_score,\n",
        "            'template_score': template_score,\n",
        "            'readability_score': readability_score,\n",
        "            'caps_ratio': caps_ratio,\n",
        "            'punct_ratio': punct_ratio,\n",
        "            'sentence_count': sentence_count,\n",
        "            'avg_sentence_length': avg_sentence_length\n",
        "        }\n",
        "\n",
        "    def _detect_repeated_ngrams(self, words: List[str], n: int = 2) -> float:\n",
        "        \"\"\"Detect repeated n-grams that indicate spam patterns.\"\"\"\n",
        "        if len(words) < n * 2:  # Need at least 2 n-grams to compare\n",
        "            return 0.0\n",
        "\n",
        "        # Generate n-grams\n",
        "        ngrams = []\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngram = ' '.join(words[i:i+n]).lower()\n",
        "            ngrams.append(ngram)\n",
        "\n",
        "        if not ngrams:\n",
        "            return 0.0\n",
        "\n",
        "        # Count n-gram frequencies\n",
        "        ngram_freq = {}\n",
        "        for ngram in ngrams:\n",
        "            ngram_freq[ngram] = ngram_freq.get(ngram, 0) + 1\n",
        "\n",
        "        # Calculate repetition score\n",
        "        total_ngrams = len(ngrams)\n",
        "        repeated_ngrams = sum(1 for freq in ngram_freq.values() if freq > 1)\n",
        "\n",
        "        return repeated_ngrams / total_ngrams if total_ngrams > 0 else 0.0\n",
        "\n",
        "    def _detect_phrase_patterns(self, words: List[str]) -> float:\n",
        "        \"\"\"Detect repeating phrase patterns like 'food is X food is Y food is Z'.\"\"\"\n",
        "        if len(words) < 6:  # Need enough words for pattern detection\n",
        "            return 0.0\n",
        "\n",
        "        # Look for patterns where same 2-word prefix repeats\n",
        "        pattern_scores = []\n",
        "\n",
        "        # Check 2-word patterns\n",
        "        for i in range(len(words) - 3):\n",
        "            prefix = f\"{words[i]} {words[i+1]}\".lower()\n",
        "\n",
        "            # Count how many times this prefix appears\n",
        "            prefix_count = 0\n",
        "            for j in range(i, len(words) - 1):\n",
        "                if j + 1 < len(words):\n",
        "                    candidate = f\"{words[j]} {words[j+1]}\".lower()\n",
        "                    if candidate == prefix:\n",
        "                        prefix_count += 1\n",
        "\n",
        "            if prefix_count > 1:\n",
        "                # This prefix repeats - calculate pattern strength\n",
        "                pattern_strength = prefix_count / (len(words) / 3)  # Normalize by text length\n",
        "                pattern_scores.append(min(pattern_strength, 1.0))\n",
        "\n",
        "        # Also check for local repetition at the end of text (common spam pattern)\n",
        "        local_repetition_score = self._detect_local_repetition(words)\n",
        "\n",
        "        return max(max(pattern_scores) if pattern_scores else 0.0, local_repetition_score)\n",
        "\n",
        "    def _detect_local_repetition(self, words: List[str]) -> float:\n",
        "        \"\"\"Detect repetitive patterns in specific sections of text (e.g., end of text).\"\"\"\n",
        "        if len(words) < 6:\n",
        "            return 0.0\n",
        "\n",
        "        max_score = 0.0\n",
        "\n",
        "        # Check multiple segments: end of text and any suspicious consecutive patterns\n",
        "        segments_to_check = []\n",
        "\n",
        "        # 1. End segment (common spam pattern)\n",
        "        end_segment_size = min(10, max(6, len(words) // 3))\n",
        "        end_words = words[-end_segment_size:]\n",
        "        segments_to_check.append(('end', end_words))\n",
        "\n",
        "        # 2. Look for any consecutive repeated phrases throughout the text\n",
        "        for start_pos in range(len(words) - 6):  # Sliding window\n",
        "            window_size = min(8, len(words) - start_pos)\n",
        "            window_words = words[start_pos:start_pos + window_size]\n",
        "            segments_to_check.append(('window', window_words))\n",
        "\n",
        "        for segment_type, segment_words in segments_to_check:\n",
        "            # Look for exact phrase repetition in this segment\n",
        "            for phrase_len in [2, 3, 4]:  # Check 2-word, 3-word, 4-word phrases\n",
        "                if len(segment_words) < phrase_len * 2:  # Need at least 2 instances\n",
        "                    continue\n",
        "\n",
        "                for i in range(len(segment_words) - phrase_len + 1):\n",
        "                    phrase = ' '.join(segment_words[i:i+phrase_len]).lower()\n",
        "\n",
        "                    # Look for consecutive repetition (more suspicious than scattered)\n",
        "                    consecutive_count = 1\n",
        "                    next_pos = i + phrase_len\n",
        "\n",
        "                    while next_pos + phrase_len <= len(segment_words):\n",
        "                        next_phrase = ' '.join(segment_words[next_pos:next_pos+phrase_len]).lower()\n",
        "                        if next_phrase == phrase:\n",
        "                            consecutive_count += 1\n",
        "                            next_pos += phrase_len\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    if consecutive_count >= 2:  # Found consecutive repeated phrase\n",
        "                        # Higher score for consecutive repetition\n",
        "                        if segment_type == 'end':\n",
        "                            # End segment repetition is more suspicious\n",
        "                            local_score = (consecutive_count * phrase_len * 1.5) / len(segment_words)\n",
        "                        else:\n",
        "                            # General repetition\n",
        "                            local_score = (consecutive_count * phrase_len) / len(segment_words)\n",
        "\n",
        "                        max_score = max(max_score, min(local_score, 1.0))\n",
        "\n",
        "        return max_score\n",
        "\n",
        "\n",
        "class UnifiedSpamDetector:\n",
        "    \"\"\"Unified spam detector combining TF-IDF and pattern analysis in single ML model.\"\"\"\n",
        "\n",
        "    def __init__(self, max_features: int = 5000, ngram_range: Tuple[int, int] = (1, 3),\n",
        "                 spam_threshold: float = 0.3):\n",
        "        self.max_features = max_features\n",
        "        self.ngram_range = ngram_range\n",
        "        self.spam_threshold = spam_threshold\n",
        "        self.pipeline = None\n",
        "        self.calibrated_pipeline = None\n",
        "        self.training_size = 0\n",
        "\n",
        "    def fit(self, texts: List[str], labels: List[str], calibrate: bool = True):\n",
        "        \"\"\"\n",
        "        Fit the unified model combining TF-IDF and pattern features.\n",
        "\n",
        "        Args:\n",
        "            texts: List of review texts\n",
        "            labels: List of labels ('APPROVE' or 'REJECT')\n",
        "            calibrate: Whether to calibrate probabilities\n",
        "        \"\"\"\n",
        "        # Convert labels to binary\n",
        "        y = [1 if label == 'REJECT' else 0 for label in labels]\n",
        "        self.training_size = len(texts)\n",
        "\n",
        "        # For small datasets, disable ML-based classification and rely on patterns only\n",
        "        if len(texts) < 10:\n",
        "            # With very small training data, ML is unreliable - use pattern-only mode\n",
        "            self.effective_threshold = 0.9  # Very high threshold to disable ML classification\n",
        "            self.pattern_only_mode = True\n",
        "            print(f\"⚠️  Very small training set ({len(texts)} samples). Using pattern-only detection mode.\")\n",
        "        elif len(texts) < 20:\n",
        "            # Small dataset - be more conservative with ML threshold\n",
        "            adjusted_threshold = min(0.6, self.spam_threshold + 0.2)\n",
        "            print(f\"⚠️  Small training set ({len(texts)} samples). Adjusting threshold: {self.spam_threshold:.2f} → {adjusted_threshold:.2f}\")\n",
        "            self.effective_threshold = adjusted_threshold\n",
        "            self.pattern_only_mode = False\n",
        "        else:\n",
        "            self.effective_threshold = self.spam_threshold\n",
        "            self.pattern_only_mode = False\n",
        "\n",
        "        # Create unified pipeline with both TF-IDF and pattern features\n",
        "        tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=self.max_features,\n",
        "            ngram_range=self.ngram_range,\n",
        "            stop_words='english',\n",
        "            lowercase=True,\n",
        "            min_df=1,\n",
        "            max_df=0.95\n",
        "        )\n",
        "\n",
        "        pattern_extractor = PatternFeatureExtractor()\n",
        "\n",
        "        # Combine features using FeatureUnion\n",
        "        feature_union = FeatureUnion([  # type: ignore\n",
        "            ('tfidf', tfidf_vectorizer),\n",
        "            ('patterns', pattern_extractor)\n",
        "        ])\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            ('features', feature_union),\n",
        "            ('classifier', LogisticRegression(\n",
        "                random_state=42,\n",
        "                max_iter=1000,\n",
        "                class_weight='balanced'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # Fit the pipeline\n",
        "        print(\"Training unified ML + Pattern model...\")\n",
        "        self.pipeline.fit(texts, y)\n",
        "\n",
        "        # Calibrate probabilities if requested\n",
        "        if calibrate and len(texts) >= 10:\n",
        "            min_class_size = min(sum(y), len(y) - sum(y))\n",
        "            cv_folds = min(3, max(2, min_class_size))\n",
        "\n",
        "            self.calibrated_pipeline = CalibratedClassifierCV(\n",
        "                self.pipeline,\n",
        "                method='isotonic',\n",
        "                cv=cv_folds\n",
        "            )\n",
        "            self.calibrated_pipeline.fit(texts, y)  # type: ignore\n",
        "        elif calibrate:\n",
        "            print(f\"Skipping calibration: need at least 10 samples, got {len(texts)}\")\n",
        "\n",
        "    def predict(self, texts: List[str], use_calibrated: bool = True) -> List[DetectionResult]:\n",
        "        \"\"\"Predict spam using unified model.\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
        "\n",
        "        pipeline = self.calibrated_pipeline if (use_calibrated and self.calibrated_pipeline) else self.pipeline\n",
        "\n",
        "        # Get predictions and probabilities\n",
        "        proba = pipeline.predict_proba(texts)  # type: ignore\n",
        "        spam_proba = proba[:, 1]  # Probability of spam (REJECT)\n",
        "\n",
        "        results = []\n",
        "        for i, prob in enumerate(spam_proba):\n",
        "            # Extract pattern features for analysis\n",
        "            pattern_extractor = PatternFeatureExtractor()\n",
        "            text_features = pattern_extractor._extract_pattern_features(texts[i])\n",
        "\n",
        "            # Check if we're in pattern-only mode (small training data)\n",
        "            pattern_only_mode = getattr(self, 'pattern_only_mode', False)\n",
        "            threshold = getattr(self, 'effective_threshold', self.spam_threshold)\n",
        "\n",
        "            if pattern_only_mode:\n",
        "                # Pattern-only classification for small datasets\n",
        "                word_count = text_features['word_count']\n",
        "\n",
        "                # Define clear spam patterns\n",
        "                is_obvious_spam = (\n",
        "                    (text_features['repetition_ratio'] > 0.8) or  # Very high word repetition\n",
        "                    (text_features['phrase_repetition_score'] > 0.7) or  # Very high phrase repetition\n",
        "                    (text_features['repeated_ngrams_ratio'] > 0.6) or  # Very high ngram repetition\n",
        "                    (text_features['local_repetition_score'] > 0.4) or  # Local repetition (like \"food is good food is good\")\n",
        "                    (word_count < 3 and text_features['repetition_ratio'] > 0.6)  # Short repetitive text\n",
        "                )\n",
        "\n",
        "                # Special cases for high-confidence spam detection\n",
        "                is_phrase_spam = text_features['phrase_repetition_score'] > 0.9\n",
        "                is_local_spam = text_features['local_repetition_score'] > 0.5\n",
        "\n",
        "                # Additional filters to avoid false positives (but not for phrase spam)\n",
        "                seems_legitimate = (\n",
        "                    text_features['readability_score'] > 30 and\n",
        "                    word_count > 8 and\n",
        "                    text_features['word_diversity_ratio'] > 0.6 and\n",
        "                    text_features['phrase_repetition_score'] < 0.8  # Allow phrase detection override\n",
        "                )\n",
        "\n",
        "                # Flag obvious spam patterns, high phrase repetition, or local repetition\n",
        "                if is_phrase_spam or is_local_spam or (is_obvious_spam and not seems_legitimate):\n",
        "                    label = 'REJECT'\n",
        "                    confidence = max(\n",
        "                        text_features['repetition_ratio'],\n",
        "                        text_features['phrase_repetition_score'],\n",
        "                        text_features['repeated_ngrams_ratio'],\n",
        "                        text_features['local_repetition_score']\n",
        "                    )\n",
        "                else:\n",
        "                    label = 'APPROVE'\n",
        "                    confidence = 1.0 - max(\n",
        "                        text_features['repetition_ratio'] * 0.5,\n",
        "                        text_features['phrase_repetition_score'] * 0.5,\n",
        "                        text_features['repeated_ngrams_ratio'] * 0.5,\n",
        "                        text_features['local_repetition_score'] * 0.5\n",
        "                    )\n",
        "\n",
        "                pattern_override = (label == 'REJECT')\n",
        "\n",
        "            else:\n",
        "                # Normal ML + pattern mode\n",
        "                label = 'REJECT' if prob > threshold else 'APPROVE'\n",
        "\n",
        "                # Pattern-based override for obvious spam\n",
        "                word_count = text_features['word_count']\n",
        "                is_very_repetitive = (\n",
        "                    (text_features['repetition_ratio'] > 0.8) or\n",
        "                    (text_features['phrase_repetition_score'] > 0.7) or\n",
        "                    (text_features['repeated_ngrams_ratio'] > 0.6)\n",
        "                )\n",
        "\n",
        "                has_reasonable_diversity = text_features['word_diversity_ratio'] > 0.3 and word_count > 6\n",
        "                is_likely_review = text_features['readability_score'] > 20 and word_count > 4\n",
        "\n",
        "                if is_very_repetitive and not (has_reasonable_diversity and is_likely_review):\n",
        "                    label = 'REJECT'\n",
        "                    pattern_override = True\n",
        "                    confidence = max(\n",
        "                        text_features['repetition_ratio'],\n",
        "                        text_features['phrase_repetition_score'],\n",
        "                        text_features['repeated_ngrams_ratio']\n",
        "                    )\n",
        "                else:\n",
        "                    pattern_override = False\n",
        "                    confidence = prob if label == 'REJECT' else (1 - prob)\n",
        "\n",
        "            # Determine category based on dominant pattern\n",
        "            category = 'None'\n",
        "            if label == 'REJECT':\n",
        "                if pattern_override:\n",
        "                    if text_features['repetition_ratio'] > 0.6:\n",
        "                        category = 'Repetitive_Spam'\n",
        "                    elif text_features['local_repetition_score'] > 0.4:\n",
        "                        category = 'Local_Repetition_Spam'\n",
        "                    elif text_features['phrase_repetition_score'] > 0.5:\n",
        "                        category = 'Phrase_Pattern_Spam'\n",
        "                    elif text_features['repeated_ngrams_ratio'] > 0.4:\n",
        "                        category = 'NGram_Pattern_Spam'\n",
        "                    else:\n",
        "                        category = 'Pattern_Spam'\n",
        "                else:\n",
        "                    # ML-detected spam\n",
        "                    if text_features['repetition_ratio'] > 0.4 or text_features['phrase_repetition_score'] > 0.4:\n",
        "                        category = 'Repetitive_Spam'\n",
        "                    elif text_features['template_score'] > 0.6:\n",
        "                        category = 'Template_Spam'\n",
        "                    else:\n",
        "                        category = 'ML_Detected_Spam'\n",
        "\n",
        "            # Simple confidence interval calculation\n",
        "            ci_margin = 0.1 * (1 - confidence)  # Smaller margin for higher confidence\n",
        "            ci_lower = max(0.0, confidence - ci_margin)\n",
        "            ci_upper = min(1.0, confidence + ci_margin)\n",
        "\n",
        "            results.append(DetectionResult(\n",
        "                label=label,\n",
        "                confidence=float(confidence),\n",
        "                category=category,\n",
        "                features={\n",
        "                    'spam_probability': float(prob),\n",
        "                    'pattern_features': text_features,\n",
        "                    'threshold_used': float(threshold)\n",
        "                },\n",
        "                confidence_interval=(float(ci_lower), float(ci_upper))\n",
        "            ))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_feature_importance(self, top_k: int = 20) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Get most important features from the unified model.\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not fitted.\")\n",
        "\n",
        "        try:\n",
        "            classifier = self.pipeline.named_steps['classifier']\n",
        "            feature_union = self.pipeline.named_steps['features']\n",
        "\n",
        "            # Get feature names\n",
        "            tfidf_vectorizer = feature_union.transformer_list[0][1]\n",
        "            tfidf_names = list(tfidf_vectorizer.get_feature_names_out())\n",
        "            pattern_names = [\n",
        "                'repetition_ratio', 'word_diversity_ratio', 'repeated_ngrams_ratio',\n",
        "                'caps_ratio', 'punct_ratio', 'readability_score',\n",
        "                'avg_sentence_length', 'word_count', 'template_score',\n",
        "                'phrase_repetition_score', 'local_repetition_score'\n",
        "            ]\n",
        "\n",
        "            all_feature_names = tfidf_names + pattern_names\n",
        "\n",
        "            # Get coefficients\n",
        "            coef = classifier.coef_[0]\n",
        "            top_indices = np.argsort(np.abs(coef))[-top_k:][::-1]\n",
        "\n",
        "            return [(all_feature_names[idx], float(coef[idx])) for idx in top_indices]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not extract feature importance: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "def load_training_data(file_path: str) -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"Load training data from CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Handle different column name variations\n",
        "    text_col = None\n",
        "    for col in ['text', 'review', 'content', 'text_clean']:\n",
        "        if col in df.columns:\n",
        "            text_col = col\n",
        "            break\n",
        "\n",
        "    if text_col is None:\n",
        "        raise ValueError(f\"No text column found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "\n",
        "    # Handle labels - check if column exists, if not create default\n",
        "    if 'gold_label' in df.columns:\n",
        "        labels = df['gold_label'].tolist()\n",
        "    elif 'label' in df.columns:\n",
        "        labels = df['label'].tolist()\n",
        "    else:\n",
        "        labels = ['APPROVE'] * len(texts)\n",
        "\n",
        "    # Handle categories - check if column exists, if not create default\n",
        "    if 'gold_category' in df.columns:\n",
        "        categories = df['gold_category'].tolist()\n",
        "    elif 'category' in df.columns:\n",
        "        categories = df['category'].tolist()\n",
        "    else:\n",
        "        categories = ['None'] * len(texts)\n",
        "\n",
        "    return texts, labels, categories\n",
        "\n",
        "\n",
        "def evaluate_detector(detector: UnifiedSpamDetector,\n",
        "                              texts: List[str],\n",
        "                              true_labels: List[str],\n",
        "                              true_categories: List[str]) -> Dict:\n",
        "    \"\"\"Evaluate the unified detector performance.\"\"\"\n",
        "    results = detector.predict(texts)\n",
        "\n",
        "    # Extract predictions\n",
        "    pred_labels = [r.label for r in results]\n",
        "    pred_categories = [r.category for r in results]\n",
        "\n",
        "    # Binary classification metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, pred_labels, average='binary', pos_label='REJECT'\n",
        "    )\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "    # Confidence analysis\n",
        "    confidences = [r.confidence for r in results]\n",
        "\n",
        "    evaluation_results = {\n",
        "        'binary_classification': {\n",
        "            'accuracy': float(accuracy),\n",
        "            'precision': float(precision),\n",
        "            'recall': float(recall),\n",
        "            'f1': float(f1)\n",
        "        },\n",
        "        'confidence_stats': {\n",
        "            'mean': float(np.mean(confidences)),\n",
        "            'std': float(np.std(confidences)),\n",
        "            'min': float(np.min(confidences)),\n",
        "            'max': float(np.max(confidences))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return evaluation_results"
      ],
      "metadata": {
        "id": "gLefa0zasVqK"
      },
      "id": "gLefa0zasVqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yXwykZFTvj3v"
      },
      "id": "yXwykZFTvj3v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure a dataframe `df` with columns: text, gold_label, gold_category\n",
        "import pandas as pd, os\n",
        "\n",
        "def ensure_df():\n",
        "    if 'df' in globals():\n",
        "        return df\n",
        "\n",
        "    for name in ['reviews_df', 'dataset', 'data']:\n",
        "        if name in globals():\n",
        "            print(f\"Using existing dataframe `{name}` as df\")\n",
        "            return globals()[name]\n",
        "\n",
        "    for path in [\"data/train.csv\", \"train.csv\", \"data/df.csv\"]:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Loading dataframe from {path}\")\n",
        "            return pd.read_csv(path)\n",
        "\n",
        "    print(\"No dataframe found; creating demo dataset.\")\n",
        "    return pd.DataFrame({\n",
        "        \"text\": [\n",
        "            \"Food is good food is great food is nice food is good\",\n",
        "            \"Service was quick and friendly. Would return.\",\n",
        "            \"Amazing place! Highly recommend this restaurant.\",\n",
        "            \"Visit my site http://spam.biz – use promo code NOW!\",\n",
        "            \"Terrible service. Will not come back.\"\n",
        "        ],\n",
        "        \"gold_label\": [\"REJECT\", \"APPROVE\", \"APPROVE\", \"REJECT\", \"APPROVE\"],\n",
        "        \"gold_category\": [\"Repetitive_Spam\", \"None\", \"Template_Spam\", \"ML_Detected_Spam\", \"None\"]\n",
        "    })\n",
        "\n",
        "df = ensure_df()\n",
        "\n",
        "# Normalize column names if your CSV uses different ones\n",
        "if 'gold_label' not in df.columns and 'label' in df.columns:\n",
        "    df = df.rename(columns={'label': 'gold_label'})\n",
        "if 'gold_category' not in df.columns and 'category' in df.columns:\n",
        "    df = df.rename(columns={'category': 'gold_category'})\n",
        "\n",
        "required = {'text', 'gold_label', 'gold_category'}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}. Present: {list(df.columns)}\")\n",
        "\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "id": "hga5kenfvkNV"
      },
      "id": "hga5kenfvkNV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = df['text'].astype(str).tolist()\n",
        "labels = df['gold_label'].tolist()\n",
        "categories = df['gold_category'].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test, cat_train, cat_test = train_test_split(\n",
        "    texts, labels, categories, test_size=0.3, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "detector = UnifiedSpamDetector(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 3),\n",
        "    spam_threshold=0.3\n",
        ")\n",
        "\n",
        "detector.fit(X_train, y_train, calibrate=True)\n",
        "\n",
        "eval_results = evaluate_detector(detector, X_test, y_test, cat_test)\n",
        "print(\"Unified Spam Detector Evaluation:\")\n",
        "eval_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T41uylNzvpS_"
      },
      "id": "T41uylNzvpS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4c020a20",
      "metadata": {
        "id": "4c020a20"
      },
      "source": [
        "## 9. Model Persistence and Export (After Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54193be4",
      "metadata": {
        "id": "54193be4"
      },
      "outputs": [],
      "source": [
        "# Model Persistence and Training Data Export\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"MODEL PERSISTENCE AND DATA EXPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('models/saved_models', exist_ok=True)\n",
        "os.makedirs('data/training', exist_ok=True)\n",
        "os.makedirs('data/predictions', exist_ok=True)\n",
        "os.makedirs('results/model_info', exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# === Save UnifiedSpamDetector (scikit-learn pipeline) ===\n",
        "model_filename = f\"models/saved_models/unified_spam_detector_{timestamp}.joblib\"\n",
        "joblib.dump(detector, model_filename)\n",
        "\n",
        "print(f\"✅ UnifiedSpamDetector saved at {model_filename}\")\n",
        "\n",
        "# Track it in model_info\n",
        "model_info[\"sklearn_model_path\"] = model_filename\n",
        "\n",
        "\n",
        "# Generate timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Model information\n",
        "model_info = {\n",
        "    \"training_timestamp\": timestamp,\n",
        "    \"training_mode\": training_mode,\n",
        "    \"base_model\": BASE_MODEL if training_mode == \"fine-tuning\" else \"pre-trained-only\",\n",
        "    \"auxiliary_models\": {\n",
        "      \"toxicity_model\": TOXIC_MODEL,\n",
        "      \"zero_shot_model\": ZERO_SHOT_MODEL\n",
        "  },\n",
        "    \"training_data_size\": len(pseudo_labels_df) if has_training_data else 0,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
        "    \"performance_metrics\": {}\n",
        "}\n",
        "\n",
        "print(f\"Training Mode: {training_mode}\")\n",
        "print(f\"Timestamp: {timestamp}\")\n",
        "\n",
        "# Save trained models\n",
        "if training_mode == \"fine-tuning\" and TRAINED_MODELS['custom_classifier'] is not None:\n",
        "    print(f\"\\n💾 SAVING FINE-TUNED MODELS\")\n",
        "\n",
        "    # Custom model is already saved during training\n",
        "    custom_model_path = TRAINED_MODELS['custom_classifier']['model_path']\n",
        "    final_model_path = f'models/saved_models/review_classifier_{timestamp}'\n",
        "\n",
        "    # Copy to final location with timestamp\n",
        "    if os.path.exists(custom_model_path):\n",
        "        shutil.copytree(custom_model_path, final_model_path, dirs_exist_ok=True)\n",
        "        print(f\"✅ Fine-tuned model copied to: {final_model_path}\")\n",
        "\n",
        "        model_info[\"custom_model_path\"] = final_model_path\n",
        "        model_info[\"model_files\"] = {\n",
        "            \"config\": f\"{final_model_path}/config.json\",\n",
        "            \"model\": f\"{final_model_path}/pytorch_model.bin\",\n",
        "            \"tokenizer\": f\"{final_model_path}/tokenizer.json\"\n",
        "        }\n",
        "\n",
        "        # Test model loading\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "            test_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
        "            test_model = AutoModelForSequenceClassification.from_pretrained(final_model_path)\n",
        "            print(\"✅ Model loading test successful\")\n",
        "            model_info[\"model_loadable\"] = True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Model loading test failed: {e}\")\n",
        "            model_info[\"model_loadable\"] = False\n",
        "    else:\n",
        "        print(f\"❌ Custom model path not found: {custom_model_path}\")\n",
        "        model_info[\"custom_model_path\"] = None\n",
        "\n",
        "else:\n",
        "    print(f\"\\n📦 SAVING PRE-TRAINED MODEL REFERENCES\")\n",
        "    model_info[\"custom_model_path\"] = None\n",
        "    model_info[\"model_files\"] = None\n",
        "\n",
        "# Save auxiliary model pipeline state (for consistency in inference)\n",
        "auxiliary_info = {\n",
        "    \"toxicity_model\": TOXIC_MODEL,\n",
        "    \"zero_shot_model\": ZERO_SHOT_MODEL,\n",
        "    \"device_used\": 0 if torch.cuda.is_available() else -1,\n",
        "    \"models_loaded\": False  # we store names; loader will instantiate\n",
        "}\n",
        "\n",
        "\n",
        "# Save training data if available\n",
        "if has_training_data:\n",
        "    print(f\"\\n💾 SAVING TRAINING DATA\")\n",
        "\n",
        "    # Save pseudo-labeled training data\n",
        "    training_data_path = f'data/training/pseudo_labels_{timestamp}.csv'\n",
        "    pseudo_labels_df.to_csv(training_data_path, index=False)\n",
        "    print(f\"✅ Training data saved: {training_data_path}\")\n",
        "\n",
        "    model_info[\"training_data_path\"] = training_data_path\n",
        "    model_info[\"training_data_size\"] = len(pseudo_labels_df)\n",
        "\n",
        "    # Save label distribution\n",
        "    label_dist = pseudo_labels_df['pred_label'].value_counts().to_dict()\n",
        "    model_info[\"training_label_distribution\"] = label_dist\n",
        "    print(f\"   Training Labels: {label_dist}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n⚠️ No training data to save\")\n",
        "    model_info[\"training_data_path\"] = None\n",
        "\n",
        "# Save prediction results\n",
        "if 'hf_results' in locals():\n",
        "    print(f\"\\n💾 SAVING PREDICTION RESULTS\")\n",
        "\n",
        "    predictions_path = f'data/predictions/predictions_{timestamp}.csv'\n",
        "    hf_results.to_csv(predictions_path, index=False)\n",
        "    print(f\"✅ Predictions saved: {predictions_path}\")\n",
        "\n",
        "    model_info[\"predictions_path\"] = predictions_path\n",
        "    model_info[\"predictions_count\"] = len(hf_results)\n",
        "\n",
        "    # Add performance metrics\n",
        "    pred_dist = hf_results['pred_label'].value_counts().to_dict()\n",
        "    avg_confidence = hf_results['confidence'].mean()\n",
        "\n",
        "    model_info[\"performance_metrics\"] = {\n",
        "        \"prediction_distribution\": pred_dist,\n",
        "        \"average_confidence\": round(float(avg_confidence), 4),\n",
        "        \"total_processed\": len(hf_results)\n",
        "    }\n",
        "    print(f\"   Prediction Labels: {pred_dist}\")\n",
        "    print(f\"   Average Confidence: {avg_confidence:.4f}\")\n",
        "\n",
        "# Save comprehensive model information\n",
        "model_info_path = f'results/model_info/model_info_{timestamp}.json'\n",
        "with open(model_info_path, 'w') as f:\n",
        "    json.dump(model_info, f, indent=2, default=str)\n",
        "\n",
        "print(f\"✅ Model info saved: {model_info_path}\")\n",
        "\n",
        "# Create deployment-ready structure for inference pipeline\n",
        "print(f\"\\n🚀 CREATING DEPLOYMENT STRUCTURE\")\n",
        "\n",
        "# Create data/actual with sample data for inference pipeline\n",
        "os.makedirs('data/actual', exist_ok=True)\n",
        "\n",
        "# If we have results, save a sample to data/actual for the inference pipeline\n",
        "if 'hf_results' in locals() and len(hf_results) > 0:\n",
        "    # Create sample data for inference testing\n",
        "    sample_actual = hf_results[['id', 'text']].head(3).copy()\n",
        "    sample_actual.to_csv('data/actual/sample_reviews.csv', index=False)\n",
        "    print(\"✅ Sample data for inference: data/actual/sample_reviews.csv\")\n",
        "\n",
        "# Save latest model paths for inference pipeline\n",
        "latest_model_config = {\n",
        "    \"latest_model_info\": model_info_path,\n",
        "    \"training_mode\": training_mode,\n",
        "    \"custom_model_path\": model_info.get(\"custom_model_path\"),\n",
        "    \"auxiliary_models\": auxiliary_info,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
        "    \"timestamp\": timestamp\n",
        "}\n",
        "\n",
        "with open('models/saved_models/latest_config.json', 'w') as f:\n",
        "    json.dump(latest_model_config, f, indent=2)\n",
        "\n",
        "print(\"✅ Latest model config: models/saved_models/latest_config.json\")\n",
        "\n",
        "# Create model loading utilities for inference\n",
        "model_loader_code = '''\"\"\"\n",
        "Model Loading Utilities for Review Classification Pipeline\n",
        "Generated automatically during training\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "def load_latest_models():\n",
        "    \"\"\"Load the most recently trained models\"\"\"\n",
        "\n",
        "    config_path = 'models/saved_models/latest_config.json'\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(\"No trained models found. Run training pipeline first.\")\n",
        "\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # Load custom model if available\n",
        "    if config['custom_model_path'] and os.path.exists(config['custom_model_path']):\n",
        "        models['custom'] = {\n",
        "            'tokenizer': AutoTokenizer.from_pretrained(config['custom_model_path']),\n",
        "            'model': AutoModelForSequenceClassification.from_pretrained(config['custom_model_path'])\n",
        "        }\n",
        "\n",
        "    # Load auxiliary models\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    aux_config = config['auxiliary_models']\n",
        "\n",
        "    models['auxiliary'] = {\n",
        "    'toxicity': pipeline(\"text-classification\",\n",
        "                         model=aux_config['toxicity_model'], device=device),\n",
        "    'zero_shot': pipeline(\"zero-shot-classification\",\n",
        "                          model=aux_config['zero_shot_model'], device=device),\n",
        "}\n",
        "\n",
        "\n",
        "    return models, config\n",
        "\n",
        "def predict_review(text, models, config):\n",
        "    \"\"\"Make prediction using loaded models\"\"\"\n",
        "\n",
        "    if 'custom' in models:\n",
        "        # Use fine-tuned model\n",
        "        inputs = models['custom']['tokenizer'](\n",
        "            text, truncation=True, padding=True,\n",
        "            max_length=256, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = models['custom']['model'](**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            confidence = float(torch.max(predictions))\n",
        "            predicted_class = int(torch.argmax(predictions))\n",
        "\n",
        "        label = \"REJECT\" if predicted_class == 1 else \"APPROVE\"\n",
        "\n",
        "    else:\n",
        "        # Use zero-shot classification\n",
        "        # Implementation would mirror the training notebook logic\n",
        "        label = \"APPROVE\"  # Placeholder\n",
        "        confidence = 0.5\n",
        "\n",
        "    return {\n",
        "        'label': label,\n",
        "        'confidence': confidence,\n",
        "        'model_type': config['training_mode']\n",
        "    }\n",
        "'''\n",
        "\n",
        "Path(\"src/utils\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"src/__init__.py\").touch(exist_ok=True)\n",
        "Path(\"src/utils/__init__.py\").touch(exist_ok=True)\n",
        "\n",
        "with open('src/utils/model_loader.py', 'w') as f:\n",
        "    f.write(model_loader_code)\n",
        "\n",
        "print(\"✅ Model loader utilities: src/utils/model_loader.py\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"PERSISTENCE COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"✅ Training Mode: {training_mode}\")\n",
        "print(f\"✅ Timestamp: {timestamp}\")\n",
        "\n",
        "if model_info.get(\"custom_model_path\"):\n",
        "    print(f\"✅ Custom Model: {model_info['custom_model_path']}\")\n",
        "else:\n",
        "    print(f\"⚠️ Custom Model: None (using pre-trained)\")\n",
        "\n",
        "print(f\"✅ Model Info: {model_info_path}\")\n",
        "print(f\"✅ Latest Config: models/saved_models/latest_config.json\")\n",
        "\n",
        "if has_training_data:\n",
        "    print(f\"✅ Training Data: {model_info['training_data_path']}\")\n",
        "    print(f\"   Size: {model_info['training_data_size']} examples\")\n",
        "\n",
        "if 'hf_results' in locals():\n",
        "    print(f\"✅ Predictions: {model_info['predictions_path']}\")\n",
        "    print(f\"   Processed: {model_info['predictions_count']} reviews\")\n",
        "\n",
        "print(f\"\\n🎯 READY FOR INFERENCE PIPELINE\")\n",
        "print(f\"   Use: models/saved_models/latest_config.json\")\n",
        "print(f\"   Data: data/actual/sample_reviews.csv\")\n",
        "print(f\"   Load: src/utils/model_loader.py\")\n",
        "\n",
        "print(f\"\\n✅ MODEL TRAINING AND PERSISTENCE COMPLETE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "981d961c",
      "metadata": {
        "id": "981d961c"
      },
      "source": [
        "## 10. Model Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e21722",
      "metadata": {
        "id": "e7e21722"
      },
      "outputs": [],
      "source": [
        "# Model Performance Evaluation with Ground Truth\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_model_with_ground_truth(predictions_df):\n",
        "    \"\"\"Evaluate model using actual ground truth labels\"\"\"\n",
        "\n",
        "    print(\"Model Performance Evaluation\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Check if we have ground truth\n",
        "    if 'gold_label' not in predictions_df.columns:\n",
        "        print(\"No ground truth available - cannot evaluate accuracy\")\n",
        "        return None\n",
        "\n",
        "    # Calculate accuracy metrics\n",
        "    accuracy = accuracy_score(predictions_df['gold_label'], predictions_df['pred_label'])\n",
        "    report = classification_report(predictions_df['gold_label'], predictions_df['pred_label'])\n",
        "    cm = confusion_matrix(predictions_df['gold_label'], predictions_df['pred_label'])\n",
        "\n",
        "    print(f\"Overall Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Show confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['APPROVE', 'REJECT'],\n",
        "                yticklabels=['APPROVE', 'REJECT'])\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # Detailed error analysis\n",
        "    errors = predictions_df[predictions_df['gold_label'] != predictions_df['pred_label']]\n",
        "    if len(errors) > 0:\n",
        "        print(f\"\\nError Analysis ({len(errors)} mistakes out of {len(predictions_df)}):\")\n",
        "        print(\"=\" * 50)\n",
        "        for _, row in errors.iterrows():\n",
        "            text_preview = row['text'][:60] + \"...\" if len(row['text']) > 60 else row['text']\n",
        "            print(f\"ID {row['id']}: Expected {row['gold_label']}, Got {row['pred_label']}\")\n",
        "            if 'gold_category' in row:\n",
        "                print(f\"  Category: Expected {row['gold_category']}, Got {row['pred_category']}\")\n",
        "            print(f\"  Text: {text_preview}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"\\nNo classification errors found\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'total_predictions': len(predictions_df),\n",
        "        'errors': len(errors),\n",
        "        'error_rate': len(errors) / len(predictions_df)\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "if 'hf_results' in locals() and 'df' in locals():\n",
        "    # Add ground truth to results\n",
        "    if 'gold_label' not in hf_results.columns:\n",
        "        hf_results = hf_results.merge(df[['id', 'gold_label', 'gold_category']], on='id', how='left')\n",
        "\n",
        "    evaluation = evaluate_model_with_ground_truth(hf_results)\n",
        "elif 'hf_results' in locals():\n",
        "    print(\"Predictions found but no ground truth data available\")\n",
        "else:\n",
        "    print(\"No prediction results found - run the HuggingFace pipeline first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598ac73c",
      "metadata": {
        "id": "598ac73c"
      },
      "source": [
        "## 11. Pipeline Summary and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5638aff5",
      "metadata": {
        "id": "5638aff5"
      },
      "outputs": [],
      "source": [
        "# Complete Pipeline Summary and Architecture Validation\n",
        "print(\"REVIEW-RATER PIPELINE ARCHITECTURE 2.0\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we're in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Environment Summary\n",
        "print(f\"\\n1. ENVIRONMENT SETUP\")\n",
        "print(f\"   Platform: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"   GPU Available: {'✅ Yes' if torch.cuda.is_available() else '❌ No'}\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Directory Structure Validation\n",
        "print(f\"\\n2. DIRECTORY STRUCTURE\")\n",
        "expected_dirs = [\n",
        "    'data/raw', 'data/clean', 'data/pseudo-label', 'data/training',\n",
        "    'data/testing', 'data/actual', 'data/sample',\n",
        "    'models/saved_models', 'models/cache',\n",
        "    'results/predictions', 'results/inference'\n",
        "]\n",
        "\n",
        "for directory in expected_dirs:\n",
        "    status = \"✅\" if os.path.exists(directory) else \"❌\"\n",
        "    print(f\"   {status} {directory}\")\n",
        "\n",
        "# Pipeline Architecture Summary\n",
        "print(f\"\\n3. PIPELINE ARCHITECTURE\")\n",
        "print(f\"   Training Flow (00_ipynb):\")\n",
        "print(f\"      data/raw → (external) → data/clean\")\n",
        "print(f\"      data/clean → (gemini) → data/pseudo-label\")\n",
        "print(f\"      data/pseudo-label → data/testing + data/training\")\n",
        "print(f\"      data/clean → data/training (combined)\")\n",
        "print(f\"      HuggingFace training on data/training with feedback loop\")\n",
        "print(f\"      Trained models → models/saved_models\")\n",
        "print(f\"\")\n",
        "print(f\"   Inference Flow (01_ipynb):\")\n",
        "print(f\"      data/actual → models/saved_models → inference → results/inference\")\n",
        "\n",
        "# Component Status\n",
        "print(f\"\\n4. COMPONENT STATUS\")\n",
        "components = {\n",
        "    'Constants loaded': 'DEFAULT_MODELS' in globals(),\n",
        "    'Sample data ready': 'df' in locals() or 'sample_df' in locals(),\n",
        "    'HuggingFace ready': True,  # Installed in environment setup\n",
        "    'Gemini available': 'gemini_available' in locals() and locals().get('gemini_available', False),\n",
        "    'Directory structure': all(os.path.exists(d) for d in ['data/clean', 'data/pseudo-label', 'data/actual']),\n",
        "}\n",
        "\n",
        "for component, status in components.items():\n",
        "    print(f\"   {'✅' if status else '❌'} {component}\")\n",
        "\n",
        "# Model Performance Summary\n",
        "print(f\"\\n5. MODEL PERFORMANCE\")\n",
        "prediction_data = None\n",
        "for var_name in ['hf_results', 'all_predictions_df', 'predictions_df', 'results_df']:\n",
        "    if var_name in globals():\n",
        "        var_value = globals()[var_name]\n",
        "        if hasattr(var_value, 'shape') and len(var_value) > 0:\n",
        "            prediction_data = var_value\n",
        "            break\n",
        "\n",
        "if prediction_data is not None:\n",
        "    print(f\"   ✅ Predictions available: {len(prediction_data)} reviews\")\n",
        "    if 'confidence' in prediction_data.columns:\n",
        "        avg_conf = prediction_data['confidence'].mean()\n",
        "        print(f\"   ✅ Average confidence: {avg_conf:.3f}\")\n",
        "    if 'pred_label' in prediction_data.columns:\n",
        "        label_dist = prediction_data['pred_label'].value_counts()\n",
        "        print(f\"   ✅ Label distribution: {dict(label_dist)}\")\n",
        "else:\n",
        "    print(f\"   ❌ No prediction data available\")\n",
        "\n",
        "# Integration Readiness\n",
        "print(f\"\\n6. INTEGRATION READINESS\")\n",
        "integration_checks = {\n",
        "    'Structured output': prediction_data is not None,\n",
        "    'Spam detection ready': True,  # Architecture supports it\n",
        "    'Production deployment': os.path.exists('data/actual'),\n",
        "    'Model persistence': 'save_trained_pipeline' in globals(),\n",
        "    'Inference pipeline': os.path.exists('notebooks/01_inference_pipeline.ipynb'),\n",
        "}\n",
        "\n",
        "for check, status in integration_checks.items():\n",
        "    print(f\"   {'✅' if status else '❌'} {check}\")\n",
        "\n",
        "# Next Steps\n",
        "print(f\"\\n7. NEXT STEPS\")\n",
        "print(f\"   Training Phase (This Notebook):\")\n",
        "print(f\"   1. ✅ Environment setup complete\")\n",
        "print(f\"   2. ✅ Directory structure created\")\n",
        "print(f\"   3. ✅ Pipeline architecture established\")\n",
        "print(f\"   4. 🔄 Run HuggingFace pipeline (cell 8)\")\n",
        "print(f\"   5. 🔄 Export trained models (cell 9)\")\n",
        "print(f\"\")\n",
        "print(f\"   Production Phase:\")\n",
        "print(f\"   1. 📋 Place actual review data in data/actual/\")\n",
        "print(f\"   2. 📋 Run 01_inference_pipeline.ipynb\")\n",
        "print(f\"   3. 📋 Check results in results/inference/\")\n",
        "\n",
        "# Final Status\n",
        "print(f\"\\n8. OVERALL STATUS\")\n",
        "overall_ready = all([\n",
        "    os.path.exists('data/clean'),\n",
        "    os.path.exists('data/actual'),\n",
        "    'DEFAULT_MODELS' in globals(),\n",
        "    'save_trained_pipeline' in globals()\n",
        "])\n",
        "\n",
        "if overall_ready:\n",
        "    print(f\"   🚀 PIPELINE READY FOR PRODUCTION\")\n",
        "    print(f\"   ✅ Training architecture: Complete\")\n",
        "    print(f\"   ✅ Inference architecture: Complete\")\n",
        "    print(f\"   ✅ Data flow: Established\")\n",
        "    print(f\"   ✅ Integration points: Ready\")\n",
        "else:\n",
        "    print(f\"   ⚠️  PIPELINE SETUP IN PROGRESS\")\n",
        "    print(f\"   Run all cells to complete setup\")\n",
        "\n",
        "print(f\"\\nPIPELINE ARCHITECTURE 2.0 SUMMARY\")\n",
        "print(f\"=\" * 70)\n",
        "print(f\"✅ data/raw → data/clean → data/pseudo-label → data/training/testing\")\n",
        "print(f\"✅ HuggingFace training with Gemini feedback loop\")\n",
        "print(f\"✅ models/saved_models for production deployment\")\n",
        "print(f\"✅ data/actual → 01_ipynb → results/inference\")\n",
        "print(f\"✅ Spam detection integration ready\")\n",
        "print(f\"✅ Complete separation of training and inference phases\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}