{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6da5986",
   "metadata": {},
   "source": [
    "# Pipeline Architecture 2.0 - Training Phase\n",
    "\n",
    "## Data Flow Overview\n",
    "\n",
    "```\n",
    "data/raw ‚Üí (external processing) ‚Üí data/clean\n",
    "data/clean ‚Üí (00_ipynb + gemini) ‚Üí data/pseudo-label  \n",
    "data/pseudo-label ‚Üí data/testing + data/training\n",
    "data/clean ‚Üí data/training (combined)\n",
    "HuggingFace model trained on data/training with feedback loop against gemini\n",
    "Trained models ‚Üí models/saved_models\n",
    "```\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "### Data Directories\n",
    "- **`data/raw`**: Raw input data (processed externally)\n",
    "- **`data/clean`**: Cleaned/processed data from data/raw\n",
    "- **`data/pseudo-label`**: Pseudo-labeled data generated by Gemini from data/clean\n",
    "- **`data/training`**: Training data (combination of data/clean + data/pseudo-label)\n",
    "- **`data/testing`**: Testing data split from data/pseudo-label\n",
    "- **`data/actual`**: Production data for inference (used by 01_inference_pipeline.ipynb)\n",
    "\n",
    "### Model Directories\n",
    "- **`models/saved_models`**: Trained models ready for production\n",
    "- **`models/cache`**: Model cache files\n",
    "\n",
    "### Results Directories\n",
    "- **`results/predictions`**: Training predictions and evaluations\n",
    "- **`results/inference`**: Production inference results\n",
    "\n",
    "## Pipeline Components\n",
    "\n",
    "### Training Phase (This Notebook - 00_ipynb)\n",
    "1. **Environment Setup**: Install packages, configure GPU\n",
    "2. **Data Processing**: Create directory structure, load sample data\n",
    "3. **Gemini Pseudo-Labeling**: Generate high-quality labels for training\n",
    "4. **HuggingFace Training**: Train models with feedback loop against Gemini\n",
    "5. **Model Export**: Save trained models to models/saved_models\n",
    "\n",
    "### Inference Phase (01_ipynb)\n",
    "1. **Load Trained Models**: From models/saved_models\n",
    "2. **Process Production Data**: From data/actual\n",
    "3. **Generate Predictions**: Using trained pipeline\n",
    "4. **Save Results**: To results/inference\n",
    "\n",
    "## Integration Points\n",
    "- **Spam Detection**: Pipeline ready for spam detection model integration\n",
    "- **Feedback Loop**: HuggingFace model iteratively improved against Gemini predictions\n",
    "- **Production Ready**: Complete separation of training and inference phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975af5e",
   "metadata": {},
   "source": [
    "# Review Classification Pipeline - Complete Google Colab Implementation\n",
    "\n",
    "This notebook implements the complete review classification pipeline for detecting Google review policy violations, fully configured for Google Colab.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "### Phase 1: Environment Setup and Data Structure\n",
    "- Install all required packages (transformers, torch, google-generativeai, etc.)\n",
    "- Create proper directory structure (data/clean, data/pseudo-label, etc.)\n",
    "- Load sample data for demonstration\n",
    "\n",
    "### Phase 2: Core Pipeline Components\n",
    "- **Ollama Pipeline**: Local LLM classification (for reference, not runnable in Colab)\n",
    "- **HuggingFace Pipeline**: Zero-shot classification using pre-trained models\n",
    "- **Gemini Pseudo-Labeling**: High-quality label generation for training data\n",
    "- **Ensemble Method**: Combines multiple approaches for best results\n",
    "\n",
    "### Phase 3: Future Spam Detection Integration\n",
    "- Pipeline output will be piped into a spam detection model\n",
    "- Structured JSON output format for downstream processing\n",
    "- Confidence scoring for reliable filtering\n",
    "\n",
    "### Phase 4: Evaluation and Analysis\n",
    "- Comprehensive performance metrics\n",
    "- Policy category accuracy assessment\n",
    "- Model comparison and improvement recommendations\n",
    "\n",
    "**Key Features:**\n",
    "- **Policy Categories**: No_Ads, Irrelevant, Rant_No_Visit detection\n",
    "- **Zero Setup**: Everything configured for Google Colab\n",
    "- **Extensible**: Ready for spam detection integration\n",
    "- **Production Ready**: Structured output and comprehensive evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462d932",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f35169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core packages installed successfully!\n",
      "Using device: cpu\n",
      "Using CPU - models will run slower but still functional\n",
      "Environment configured for optimal performance\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for the complete pipeline\n",
    "!pip install -q transformers==4.43.3 torch pandas scikit-learn\n",
    "!pip install -q google-generativeai tqdm datasets accelerate\n",
    "!pip install -q ipywidgets matplotlib seaborn wordcloud\n",
    "\n",
    "print(\"‚úÖ Core packages installed successfully!\")\n",
    "\n",
    "# Check GPU availability and setup device\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"Using CPU - models will run slower but still functional\")\n",
    "\n",
    "# Set environment for optimal performance\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid warnings\n",
    "print(\"Environment configured for optimal performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d42837",
   "metadata": {},
   "source": [
    "## 2. Project Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0558d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created!\n",
      "Created directories: ['src/config', 'src/core', 'src/pseudo_labelling', 'src/utils', 'data/sample', 'data/raw', 'data/processed', 'results/predictions', 'results/evaluations', 'results/reports', 'logs/pipeline_logs', 'models/cache', 'prompts']\n"
     ]
    }
   ],
   "source": [
    "# Create complete directory structure matching the actual pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create all necessary directories (matching actual pipeline structure)\n",
    "directories = [\n",
    "    # Source code structure\n",
    "    'src/config', 'src/core', 'src/pseudo_labelling', 'src/pipeline', 'src/integration',\n",
    "    \n",
    "    # Data directories (matching actual structure)\n",
    "    'data/raw',           # For raw input data  \n",
    "    'data/clean',         # For cleaned/processed data (from data/raw)\n",
    "    'data/pseudo-label',  # For pseudo-labeled data from Gemini (from data/clean)\n",
    "    'data/training',      # For training data split (from data/clean + data/pseudo-label)\n",
    "    'data/testing',       # For testing data split (from data/pseudo-label)\n",
    "    'data/actual',        # For actual production data to be processed by 01_inference_pipeline.ipynb\n",
    "    'data/sample',        # For sample data\n",
    "    \n",
    "    # Results directories\n",
    "    'results/predictions',   # All predictions\n",
    "    'results/evaluations',   # For evaluation results\n",
    "    'results/reports',       # For generated reports\n",
    "    \n",
    "    # Other directories\n",
    "    'models/saved_models',   # For trained models\n",
    "    'models/cache',          # For model cache\n",
    "    'logs/pipeline_logs',    # For pipeline logs\n",
    "    'prompts',               # Prompt engineering\n",
    "    'docs'                   # Documentation\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    # Create __init__.py files for Python packages\n",
    "    if directory.startswith('src/'):\n",
    "        with open(f'{directory}/__init__.py', 'w') as f:\n",
    "            f.write('# Review Classification Pipeline Package\\n')\n",
    "\n",
    "print(\"‚úÖ Complete directory structure created!\")\n",
    "print(f\"Created {len(directories)} directories\")\n",
    "\n",
    "# Verify critical directories exist\n",
    "critical_dirs = ['data/clean', 'data/pseudo-label', 'data/sample', 'results/predictions']\n",
    "for dir_name in critical_dirs:\n",
    "    if os.path.exists(dir_name):\n",
    "        print(f\"‚úÖ {dir_name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_name} - MISSING!\")\n",
    "\n",
    "print(\"\\nDirectory structure matches production pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18cdbb",
   "metadata": {},
   "source": [
    "## 3. Sample Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created for pseudo-labeling demonstration!\n",
      "   id                                        text gold_label  gold_category\n",
      "0   1            Great product, highly recommend!    APPROVE           None\n",
      "1   2            Terrible service, waste of money     REJECT     Irrelevant\n",
      "2   3  Check out this amazing deal at example.com     REJECT         No_Ads\n",
      "3   4       The staff was rude and unprofessional     REJECT     Irrelevant\n",
      "4   5     Overpriced scammers. Society is doomed.     REJECT  Rant_No_Visit\n",
      "\n",
      "This data will be used as:\n",
      "- Gold standard for evaluation\n",
      "- Seed data for training\n",
      "- Reference for pseudo-labeling quality assessment\n"
     ]
    }
   ],
   "source": [
    "# Load actual sample data from the production pipeline\n",
    "sample_data = {\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'text': [\n",
    "        \"Use my promo code EAT10 for 10% off! DM me on WhatsApp.\",\n",
    "        \"Great laksa; broth was rich and staff friendly. Will return.\",\n",
    "        \"Crypto is the future. Buy BTC now! Nothing to do with this cafe.\",\n",
    "        \"Overpriced scammers. Society is doomed.\",\n",
    "        \"Visited on 18 Aug, ordered set A; cashier fixed a double-charge.\"\n",
    "    ],\n",
    "    'gold_label': ['REJECT', 'APPROVE', 'REJECT', 'REJECT', 'APPROVE'],\n",
    "    'gold_category': ['No_Ads', 'None', 'Irrelevant', 'Rant_No_Visit', 'None']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "df.to_csv('data/sample/sample_reviews.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Production sample data loaded!\")\n",
    "print(\"\\nSample Data Overview:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(f\"APPROVE: {len(df[df['gold_label'] == 'APPROVE'])} reviews\")\n",
    "print(f\"REJECT:  {len(df[df['gold_label'] == 'REJECT'])} reviews\")\n",
    "\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "for category in df['gold_category'].value_counts().index:\n",
    "    count = df['gold_category'].value_counts()[category]\n",
    "    print(f\"{category}: {count} reviews\")\n",
    "\n",
    "print(f\"\\nThis data demonstrates all policy violation types:\")\n",
    "print(\"‚Ä¢ No_Ads: Promotional codes and contact solicitation\")\n",
    "print(\"‚Ä¢ Irrelevant: Off-topic content unrelated to business\") \n",
    "print(\"‚Ä¢ Rant_No_Visit: Generic negative comments without visit evidence\")\n",
    "print(\"‚Ä¢ None: Legitimate reviews that should be approved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014facd",
   "metadata": {},
   "source": [
    "## 4. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97baccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration created with pseudo-labeling support!\n"
     ]
    }
   ],
   "source": [
    "# Create configuration classes matching the actual pipeline\n",
    "config_code = '''\n",
    "\"\"\"\n",
    "Pipeline Configuration Classes - Matching Production Structure\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model settings\"\"\"\n",
    "    # HuggingFace models (matching actual pipeline)\n",
    "    hf_sentiment_model: str = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    hf_toxicity_model: str = \"unitary/toxic-bert\"\n",
    "    hf_zero_shot_model: str = \"facebook/bart-large-mnli\"\n",
    "    \n",
    "    # Gemini configuration\n",
    "    gemini_model: str = \"gemini-2.5-flash-lite\"\n",
    "    \n",
    "    # Confidence thresholds (matching actual pipeline)\n",
    "    sentiment_threshold: float = 0.7\n",
    "    toxicity_threshold: float = 0.5\n",
    "    zero_shot_threshold: float = 0.7\n",
    "    ensemble_tau: float = 0.55\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data paths and settings\"\"\"\n",
    "    data_dir: str = \"data\"\n",
    "    raw_data_dir: str = \"data/raw\"\n",
    "    processed_data_dir: str = \"data/clean\"  # Matches actual structure\n",
    "    sample_data_dir: str = \"data/sample\"\n",
    "    pseudo_label_dir: str = \"data/pseudo-label\"  # Matches actual structure\n",
    "    training_dir: str = \"data/training\"\n",
    "    testing_dir: str = \"data/testing\"\n",
    "    \n",
    "    # Default input file\n",
    "    sample_reviews_file: str = \"data/sample/sample_reviews.csv\"\n",
    "\n",
    "@dataclass\n",
    "class OutputConfig:\n",
    "    \"\"\"Configuration for output paths\"\"\"\n",
    "    results_dir: str = \"results\"\n",
    "    predictions_dir: str = \"results/predictions\"\n",
    "    evaluations_dir: str = \"results/evaluations\"\n",
    "    reports_dir: str = \"results/reports\"\n",
    "    \n",
    "    # Default output files (matching actual pipeline)\n",
    "    hf_predictions: str = \"results/predictions/predictions_hf.csv\"\n",
    "    ensemble_predictions: str = \"results/predictions/predictions_ens.csv\"\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Main pipeline configuration combining all components\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    output: OutputConfig = field(default_factory=OutputConfig)\n",
    "    \n",
    "    # Gemini configuration\n",
    "    gemini_api_key: str = \"\"\n",
    "    \n",
    "    # Pipeline settings\n",
    "    batch_size: int = 32\n",
    "    max_workers: int = 4\n",
    "    cache_predictions: bool = True\n",
    "    verbose_logging: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Create directories if they don't exist\"\"\"\n",
    "        directories = [\n",
    "            self.data.raw_data_dir,\n",
    "            self.data.processed_data_dir,\n",
    "            self.data.sample_data_dir,\n",
    "            self.data.pseudo_label_dir,\n",
    "            self.data.training_dir,\n",
    "            self.data.testing_dir,\n",
    "            self.output.predictions_dir,\n",
    "            self.output.evaluations_dir,\n",
    "            self.output.reports_dir\n",
    "        ]\n",
    "        \n",
    "        for directory in directories:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Global configuration instance\n",
    "config = PipelineConfig()\n",
    "'''\n",
    "\n",
    "with open('src/config/pipeline_config.py', 'w') as f:\n",
    "    f.write(config_code)\n",
    "\n",
    "print(\"‚úÖ Configuration created matching production pipeline!\")\n",
    "\n",
    "# Test configuration\n",
    "exec(config_code)\n",
    "test_config = PipelineConfig()\n",
    "print(f\"Data directory: {test_config.data.sample_data_dir}\")\n",
    "print(f\"HF Zero-shot model: {test_config.model.hf_zero_shot_model}\")\n",
    "print(f\"Ensemble tau: {test_config.model.ensemble_tau}\")\n",
    "print(f\"Predictions output: {test_config.output.hf_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf61f4",
   "metadata": {},
   "source": [
    "## 5. Constants and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b648a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants and prompts created with Gemini support!\n"
     ]
    }
   ],
   "source": [
    "# Create constants and prompts matching the actual pipeline\n",
    "constants_code = '''\n",
    "\"\"\"\n",
    "Core Constants - Matching Production Pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Policy Categories (matching actual pipeline)\n",
    "POLICY_CATEGORIES = {\n",
    "    'NO_ADS': 'No_Ads',\n",
    "    'IRRELEVANT': 'Irrelevant', \n",
    "    'RANT_NO_VISIT': 'Rant_No_Visit',\n",
    "    'NONE': 'None'\n",
    "}\n",
    "\n",
    "# Label Types (matching actual pipeline)\n",
    "LABELS = {\n",
    "    'APPROVE': 'APPROVE',\n",
    "    'REJECT': 'REJECT'\n",
    "}\n",
    "\n",
    "# Default Models (matching actual pipeline)\n",
    "DEFAULT_MODELS = {\n",
    "    'SENTIMENT': \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    'TOXIC': \"unitary/toxic-bert\", \n",
    "    'ZERO_SHOT': \"facebook/bart-large-mnli\",\n",
    "    'GEMINI_DEFAULT': \"gemini-2.5-flash-lite\"\n",
    "}\n",
    "\n",
    "# Zero-shot Classification Labels (matching actual pipeline)\n",
    "ZERO_SHOT_LABELS = [\n",
    "    \"an advertisement or promotional solicitation for this business (promo code, referral, links, contact to buy)\",\n",
    "    \"off-topic or unrelated to this business (e.g., politics, crypto, chain messages, personal stories not about this place)\",\n",
    "    \"a generic negative rant about this business without evidence of a visit (short insults, 'scam', 'overpriced', 'worst ever')\",\n",
    "    \"a relevant on-topic description of a visit or experience at this business\"\n",
    "]\n",
    "\n",
    "# Mapping zero-shot labels to policy categories\n",
    "ZERO_SHOT_TO_POLICY = {\n",
    "    ZERO_SHOT_LABELS[0]: POLICY_CATEGORIES['NO_ADS'],\n",
    "    ZERO_SHOT_LABELS[1]: POLICY_CATEGORIES['IRRELEVANT'],\n",
    "    ZERO_SHOT_LABELS[2]: POLICY_CATEGORIES['RANT_NO_VISIT'],\n",
    "    ZERO_SHOT_LABELS[3]: POLICY_CATEGORIES['NONE']\n",
    "}\n",
    "\n",
    "# Confidence Thresholds\n",
    "CONFIDENCE_THRESHOLDS = {\n",
    "    'HIGH': 0.8,\n",
    "    'MEDIUM': 0.6,\n",
    "    'LOW': 0.4,\n",
    "    'DEFAULT': 0.55\n",
    "}\n",
    "'''\n",
    "\n",
    "with open('src/core/constants.py', 'w') as f:\n",
    "    f.write(constants_code)\n",
    "\n",
    "# Create prompt templates (matching actual pipeline)\n",
    "prompts_code = '''\n",
    "\"\"\"\n",
    "Policy Prompts - Matching Production Pipeline\n",
    "\"\"\"\n",
    "\n",
    "# JSON schema all prompts must return\n",
    "TEMPLATE_JSON = \"\"\"Return ONLY JSON with no extra text:\n",
    "{\"label\":\"<APPROVE|REJECT>\",\"category\":\"<No_Ads|Irrelevant|Rant_No_Visit|None>\",\n",
    " \"rationale\":\"<short>\",\"confidence\":<0.0-1.0>,\n",
    " \"flags\":{\"links\":false,\"coupon\":false,\"visit_claimed\":false}}\n",
    "\"\"\"\n",
    "\n",
    "# ===== 1) NO ADS / PROMOTIONAL =====\n",
    "NO_ADS_SYSTEM = \"\"\"You are a content policy checker for location reviews.\n",
    "If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0. Do not reject for other policies.\n",
    "Reject ONLY if the review contains clear advertising or promotional solicitation:\n",
    "- referral/promo/coupon codes, price lists, booking/ordering links, contact-for-order (DM me / WhatsApp / Telegram / email / call), affiliate pitches.\n",
    "Do NOT mark generic off-topic content (e.g., crypto/politics) as Ads unless it includes explicit solicitation to buy or contact.\n",
    "Approve normal experiences even if positive or mentioning 'cheap' or 'good deal'.\n",
    "Output the required JSON only.\n",
    "\"\"\"\n",
    "\n",
    "# ===== 2) IRRELEVANT CONTENT =====\n",
    "IRRELEVANT_SYSTEM = \"\"\"You are checking ONLY for the 'Irrelevant' policy.\n",
    "\n",
    "Decision rule (mutually exclusive):\n",
    "- If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0.\n",
    "- Do not reject for other policies (e.g., Ads or Rant_No_Visit).\n",
    "\n",
    "Reject as Irrelevant when the text is off-topic and unrelated to THIS venue/service:\n",
    "- unrelated politics/news/crypto hype/chain messages/personal stories\n",
    "- generic advice not tied to this place (e.g., 'buy BTC now', 'vote X'), etc.\n",
    "- content about another business or location without discussing this one\n",
    "\n",
    "Return ONLY JSON with fields: label, category, rationale, confidence (0.0‚Äì1.0), flags.\n",
    "\"\"\"\n",
    "\n",
    "# ===== 3) RANTS WITHOUT VISIT =====\n",
    "RANT_NO_VISIT_SYSTEM = \"\"\"Reject generic rants or accusations clearly targeting THIS place but with no evidence of a visit.\n",
    "These rants are often:\n",
    "- Short and emotional (e.g., 'Terrible place', 'Worst ever', 'Overpriced scammers')\n",
    "- Broad accusations ('scam', 'rip-off', 'fraud')\n",
    "- Negative judgments about pricing, quality, or character of the venue\n",
    "Reject them even if the reviewer does not explicitly say 'this place/restaurant' ‚Äî assume negativity is directed at the business being reviewed.\n",
    "Approve only if the reviewer provides concrete evidence of a visit (date, food ordered, staff interaction).\n",
    "Output JSON only.\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(system_text: str, review_text: str, fewshots):\n",
    "    demo = \"\\\\n\\\\n\".join(\n",
    "        [f\"Review:\\\\n{r}\\\\nExpected JSON:\\\\n{j}\" for r,j in fewshots]\n",
    "    )\n",
    "    return f\"\"\"{system_text}\n",
    "\n",
    "{TEMPLATE_JSON}\n",
    "\n",
    "{demo}\n",
    "\n",
    "Now classify this review. Return ONLY JSON.\n",
    "\n",
    "Review:\n",
    "{review_text}\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "with open('prompts/policy_prompts.py', 'w') as f:\n",
    "    f.write(prompts_code)\n",
    "\n",
    "print(\"‚úÖ Constants and prompts created matching production pipeline!\")\n",
    "\n",
    "# Test constants\n",
    "exec(constants_code)\n",
    "print(f\"Policy categories: {list(POLICY_CATEGORIES.values())}\")\n",
    "print(f\"Zero-shot model: {DEFAULT_MODELS['ZERO_SHOT']}\")\n",
    "print(f\"Default confidence threshold: {CONFIDENCE_THRESHOLDS['DEFAULT']}\")\n",
    "print(f\"Zero-shot labels configured: {len(ZERO_SHOT_LABELS)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015eaed",
   "metadata": {},
   "source": [
    "## 6. Gemini API Configuration and Pseudo-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa5b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Set up Gemini API key\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSetting up Gemini API key...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Gemini API Key Setup and Pseudo-Labeling Implementation\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Option A: Try Colab secrets first (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    print(\"‚úÖ Gemini API key loaded from Colab secrets\")\n",
    "    api_source = \"secrets\"\n",
    "except Exception as e:\n",
    "    print(f\"Colab secrets not found: {e}\")\n",
    "    \n",
    "    # Option B: Manual input fallback\n",
    "    print(\"Please enter your Gemini API key manually:\")\n",
    "    print(\"\")\n",
    "    print(\"Instructions:\")\n",
    "    print(\"1. Go to: https://aistudio.google.com/app/apikey\")\n",
    "    print(\"2. Click 'Create API Key'\")\n",
    "    print(\"3. Copy the key\")\n",
    "    print(\"\")\n",
    "    \n",
    "    GEMINI_API_KEY = input(\"Enter your Gemini API key: \").strip()\n",
    "    api_source = \"manual\"\n",
    "\n",
    "# Configure Gemini\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    # Test the API\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
    "        test_response = model.generate_content(\"Test: Say 'API working'\")\n",
    "        print(f\"‚úÖ Gemini API test successful!\")\n",
    "        print(f\"   Source: {api_source}\")\n",
    "        print(f\"   Model: gemini-2.5-flash-lite\")\n",
    "        gemini_available = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gemini test failed: {e}\")\n",
    "        print(\"Check your API key and quota limits\")\n",
    "        gemini_available = False\n",
    "else:\n",
    "    print(\"No API key provided\")\n",
    "    print(\"Pipeline will run without Gemini pseudo-labeling\")\n",
    "    print(\"HuggingFace components will still work perfectly!\")\n",
    "    gemini_available = False\n",
    "\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"   Gemini Available: {'‚úÖ Yes' if gemini_available else '‚ùå No'}\")\n",
    "print(f\"   HuggingFace: ‚úÖ Ready\")\n",
    "print(f\"   Pipeline Mode: {'Full' if gemini_available else 'HuggingFace Only'}\")\n",
    "\n",
    "# Gemini Pseudo-Labeling Implementation (for Training Data Generation)\n",
    "def classify_with_gemini(text: str, model_name=\"gemini-2.0-flash-exp\"):\n",
    "    \"\"\"Generate pseudo-labels using Gemini for training data\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        \n",
    "        # Enhanced prompt for better pseudo-labeling\n",
    "        prompt = f\"\"\"\n",
    "You are a Google review policy expert. Classify this review and provide a JSON response.\n",
    "\n",
    "Review: \"{text}\"\n",
    "\n",
    "Policy Categories:\n",
    "- No_Ads: Contains advertisements, promotional content, promo codes, contact information\n",
    "- Irrelevant: Off-topic content (politics, crypto, unrelated businesses)  \n",
    "- Rant_No_Visit: Generic negative rants without evidence of visiting\n",
    "- None: Legitimate review about an actual visit/experience\n",
    "\n",
    "Respond with JSON only:\n",
    "{{\"label\": \"APPROVE\" or \"REJECT\", \"category\": \"policy_name\", \"confidence\": 0.0-1.0, \"rationale\": \"brief_explanation\"}}\n",
    "\"\"\"\n",
    "        \n",
    "        response = model.generate_content(prompt)\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        if response_text.startswith('```json'):\n",
    "            response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
    "        elif response_text.startswith('```'):\n",
    "            response_text = response_text.replace('```', '').strip()\n",
    "            \n",
    "        try:\n",
    "            result = json.loads(response_text)\n",
    "            \n",
    "            # Validate required fields\n",
    "            if all(key in result for key in ['label', 'category', 'confidence']):\n",
    "                return {\n",
    "                    \"label\": result['label'],\n",
    "                    \"category\": result['category'], \n",
    "                    \"confidence\": float(result['confidence']),\n",
    "                    \"rationale\": result.get('rationale', 'Gemini classification')\n",
    "                }\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "            \n",
    "        # Fallback parsing if JSON fails\n",
    "        if 'REJECT' in response_text.upper():\n",
    "            # Try to extract category\n",
    "            if 'No_Ads' in response_text or 'advertisement' in response_text.lower():\n",
    "                category = 'No_Ads'\n",
    "            elif 'Irrelevant' in response_text or 'off-topic' in response_text.lower():\n",
    "                category = 'Irrelevant'\n",
    "            elif 'Rant_No_Visit' in response_text or 'rant' in response_text.lower():\n",
    "                category = 'Rant_No_Visit'\n",
    "            else:\n",
    "                category = 'No_Ads'  # Default\n",
    "                \n",
    "            return {\"label\": \"REJECT\", \"category\": category, \"confidence\": 0.7, \"rationale\": \"Parsed from text\"}\n",
    "        else:\n",
    "            return {\"label\": \"APPROVE\", \"category\": \"None\", \"confidence\": 0.7, \"rationale\": \"Parsed from text\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Gemini API error: {e}\")\n",
    "        return {\"label\": \"APPROVE\", \"category\": \"None\", \"confidence\": 0.0, \"rationale\": f\"API error: {e}\"}\n",
    "\n",
    "def generate_pseudo_labels_with_gemini(unlabeled_df, confidence_threshold=0.8, max_labels=50):\n",
    "    \"\"\"Generate high-quality pseudo-labels for training data\"\"\"\n",
    "    if not gemini_available:\n",
    "        print(\"‚ùå Gemini not available for pseudo-labeling\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Generating pseudo-labels for training data...\")\n",
    "    print(f\"Confidence threshold: {confidence_threshold}\")\n",
    "    print(f\"Max labels to generate: {max_labels}\")\n",
    "    \n",
    "    pseudo_labels = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(f\"Processing {min(len(unlabeled_df), max_labels)} reviews...\")\n",
    "    \n",
    "    for _, row in tqdm(unlabeled_df.iterrows(), total=min(len(unlabeled_df), max_labels), desc=\"Generating pseudo-labels\"):\n",
    "        if processed_count >= max_labels:\n",
    "            break\n",
    "            \n",
    "        text = str(row['text'])\n",
    "        result = classify_with_gemini(text)\n",
    "        \n",
    "        # Only include high-confidence predictions for training\n",
    "        if result['confidence'] >= confidence_threshold:\n",
    "            pseudo_labels.append({\n",
    "                'id': row.get('id', processed_count + 100),\n",
    "                'text': text,\n",
    "                'pred_label': result['label'],\n",
    "                'pred_category': result['category'],\n",
    "                'confidence': result['confidence'],\n",
    "                'rationale': result['rationale'],\n",
    "                'source': 'gemini_pseudo'\n",
    "            })\n",
    "        \n",
    "        processed_count += 1\n",
    "        time.sleep(0.1)  # Rate limiting for API\n",
    "        \n",
    "        # Progress update\n",
    "        if processed_count % 5 == 0:\n",
    "            print(f\"   Processed {processed_count} reviews, generated {len(pseudo_labels)} high-confidence labels\")\n",
    "    \n",
    "    pseudo_df = pd.DataFrame(pseudo_labels)\n",
    "    \n",
    "    if len(pseudo_df) > 0:\n",
    "        # Save to proper directory for training\n",
    "        output_path = 'data/pseudo-label/gemini_pseudo_labels.csv'\n",
    "        pseudo_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(pseudo_df)} high-confidence pseudo-labels for training\")\n",
    "        print(f\"Saved to: {output_path}\")\n",
    "        print(f\"Label distribution: {pseudo_df['pred_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Category distribution: {pseudo_df['pred_category'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Quality metrics\n",
    "        avg_confidence = pseudo_df['confidence'].mean()\n",
    "        print(f\"Average confidence: {avg_confidence:.3f}\")\n",
    "        \n",
    "        return pseudo_df\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No high-confidence pseudo-labels generated\")\n",
    "        print(\"Try lowering confidence threshold or checking API responses\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Generate Training Data with Pseudo-Labels\n",
    "if gemini_available:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING TRAINING DATA WITH PSEUDO-LABELS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create diverse unlabeled data for pseudo-labeling\n",
    "    unlabeled_data = {\n",
    "        'id': list(range(101, 121)),\n",
    "        'text': [\n",
    "            \"Amazing food and service, definitely coming back!\",\n",
    "            \"Visit our website for exclusive deals and discounts - use code SAVE20\",\n",
    "            \"The worst experience ever, everything was terrible, total scam\",\n",
    "            \"Staff was friendly, food was fresh and tasty, good value for money\",\n",
    "            \"This place is overpriced, never going back, waste of money\",\n",
    "            \"Great atmosphere, perfect for family dinner, ordered the set meal and dessert\",\n",
    "            \"Follow my Instagram @foodie123 for more reviews and promos\",\n",
    "            \"Bitcoin is going to the moon! Buy now before it's too late!\",\n",
    "            \"Had the chicken rice here yesterday, portion was generous and taste was authentic\",\n",
    "            \"DM me for discount codes! Also selling crypto courses online\",\n",
    "            \"Terrible service, rude staff, food was cold when it arrived\",\n",
    "            \"The laksa here reminds me of my grandmother's cooking, very nostalgic\",\n",
    "            \"Check out my YouTube channel for food reviews and crypto tips\",\n",
    "            \"Went here for lunch with colleagues, everyone enjoyed their meals\",\n",
    "            \"Overpriced tourist trap, locals know better places nearby\",\n",
    "            \"Made reservation for 6pm, got seated immediately, excellent service throughout\",\n",
    "            \"Politics in this country is corrupt, restaurants like this are part of the problem\",\n",
    "            \"Their signature dish was perfectly seasoned, will definitely recommend to friends\",\n",
    "            \"Worst restaurant in Singapore, total ripoff, avoid at all costs\",\n",
    "            \"Celebrated my birthday here last week, staff even brought out a cake\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    unlabeled_df = pd.DataFrame(unlabeled_data)\n",
    "    print(f\"Created {len(unlabeled_df)} diverse reviews for pseudo-labeling\")\n",
    "    \n",
    "    # Generate pseudo-labels for training\n",
    "    print(f\"\\nStarting Gemini pseudo-labeling for training data...\")\n",
    "    pseudo_labels_df = generate_pseudo_labels_with_gemini(\n",
    "        unlabeled_df, \n",
    "        confidence_threshold=0.7,  # Lower threshold for more training data\n",
    "        max_labels=20\n",
    "    )\n",
    "    \n",
    "    if len(pseudo_labels_df) > 0:\n",
    "        print(f\"\\n‚úÖ PSEUDO-LABELING COMPLETE\")\n",
    "        print(f\"Generated {len(pseudo_labels_df)} high-quality labels for training\")\n",
    "        print(f\"Ready for HuggingFace model fine-tuning!\")\n",
    "        \n",
    "        # Preview the training data\n",
    "        print(f\"\\nTraining Data Preview:\")\n",
    "        display_cols = ['text', 'pred_label', 'pred_category', 'confidence']\n",
    "        for idx, row in pseudo_labels_df.head(3).iterrows():\n",
    "            text_preview = row['text'][:60] + \"...\" if len(row['text']) > 60 else row['text']\n",
    "            print(f\"   {row['pred_label']} | {row['pred_category']} | {row['confidence']:.2f} | {text_preview}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No pseudo-labels generated - will use pre-trained models only\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SKIPPING PSEUDO-LABELING (Gemini not available)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Will proceed with pre-trained HuggingFace models only\")\n",
    "    pseudo_labels_df = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nTraining data preparation: {'‚úÖ Complete' if len(pseudo_labels_df) > 0 else '‚ùå Skipped'}\")\n",
    "print(f\"Ready for HuggingFace model training: {'‚úÖ Yes' if len(pseudo_labels_df) > 0 else '‚ö†Ô∏è Pre-trained only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255cad8",
   "metadata": {},
   "source": [
    "## 7. HuggingFace Model Training with Pseudo-Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Model Training with Pseudo-Labels\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"HUGGINGFACE MODEL TRAINING WITH PSEUDO-LABELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load constants\n",
    "exec(open('src/core/constants.py').read())\n",
    "\n",
    "# Check if we have training data\n",
    "has_training_data = 'pseudo_labels_df' in locals() and len(pseudo_labels_df) > 0\n",
    "\n",
    "if has_training_data:\n",
    "    print(f\"‚úÖ Training data available: {len(pseudo_labels_df)} pseudo-labeled examples\")\n",
    "    training_mode = \"fine-tuning\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pseudo-labels available - using pre-trained models only\")\n",
    "    training_mode = \"pre-trained\"\n",
    "\n",
    "# Model configuration\n",
    "BASE_MODEL = \"distilbert-base-uncased\"\n",
    "SENTIMENT_MODEL = DEFAULT_MODELS['SENTIMENT']\n",
    "TOXIC_MODEL = DEFAULT_MODELS['TOXIC']\n",
    "ZERO_SHOT_MODEL = DEFAULT_MODELS['ZERO_SHOT']\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for training evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def prepare_training_data(pseudo_df):\n",
    "    \"\"\"Prepare pseudo-labeled data for training\"\"\"\n",
    "    print(f\"Preparing training data from {len(pseudo_df)} pseudo-labels...\")\n",
    "    \n",
    "    # Convert labels to numeric\n",
    "    train_texts = pseudo_df['text'].tolist()\n",
    "    train_labels = [1 if label == 'REJECT' else 0 for label in pseudo_df['pred_label']]\n",
    "    \n",
    "    print(f\"   Texts: {len(train_texts)}\")\n",
    "    print(f\"   Labels: REJECT={sum(train_labels)}, APPROVE={len(train_labels)-sum(train_labels)}\")\n",
    "    \n",
    "    return train_texts, train_labels\n",
    "\n",
    "def train_custom_classification_model(train_texts, train_labels, model_name=\"review-classifier\"):\n",
    "    \"\"\"Fine-tune a model for review classification\"\"\"\n",
    "    print(f\"\\nüöÄ Training custom model: {model_name}\")\n",
    "    print(f\"Base model: {BASE_MODEL}\")\n",
    "    print(f\"Training samples: {len(train_texts)}\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=2)\n",
    "    \n",
    "    # Tokenize training data\n",
    "    print(\"Tokenizing training data...\")\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input_ids': train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask'],\n",
    "        'labels': train_labels\n",
    "    })\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./models/fine-tuned/{model_name}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs/{model_name}',\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=train_dataset,  # Using same data for simplicity\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    training_result = trainer.train()\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    model_save_path = f'./models/fine-tuned/{model_name}'\n",
    "    trainer.save_model(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    print(f\"‚úÖ Model training completed!\")\n",
    "    print(f\"   Training loss: {training_result.training_loss:.4f}\")\n",
    "    print(f\"   Model saved to: {model_save_path}\")\n",
    "    \n",
    "    return model, tokenizer, model_save_path\n",
    "\n",
    "# Load pre-trained pipeline models for auxiliary tasks\n",
    "def load_auxiliary_pipelines(device=None):\n",
    "    \"\"\"Load auxiliary models (sentiment, toxicity, zero-shot)\"\"\"\n",
    "    print(\"Loading auxiliary pipelines...\")\n",
    "    \n",
    "    if device is None:\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "    \n",
    "    try:\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        sentiment = pipeline(\"sentiment-analysis\", \n",
    "                           model=SENTIMENT_MODEL, \n",
    "                           device=device)\n",
    "        \n",
    "        toxic = pipeline(\"text-classification\", \n",
    "                        model=TOXIC_MODEL, \n",
    "                        top_k=None, \n",
    "                        device=device)\n",
    "        \n",
    "        zshot = pipeline(\"zero-shot-classification\", \n",
    "                        model=ZERO_SHOT_MODEL, \n",
    "                        device=device)\n",
    "        \n",
    "        print(\"‚úÖ Auxiliary pipelines loaded successfully!\")\n",
    "        return sentiment, toxic, zshot\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading auxiliary pipelines: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Execute training based on available data\n",
    "if training_mode == \"fine-tuning\":\n",
    "    print(f\"\\nüéØ FINE-TUNING MODE\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_texts, train_labels = prepare_training_data(pseudo_labels_df)\n",
    "    \n",
    "    # Train custom model\n",
    "    trained_model, trained_tokenizer, model_path = train_custom_classification_model(\n",
    "        train_texts, train_labels, \"review-policy-classifier\"\n",
    "    )\n",
    "    \n",
    "    # Load auxiliary models\n",
    "    sentiment_pipeline, toxic_pipeline, zshot_pipeline = load_auxiliary_pipelines(device)\n",
    "    \n",
    "    print(f\"\\n‚úÖ TRAINING COMPLETE\")\n",
    "    print(f\"   Custom model: ‚úÖ Trained and saved\")\n",
    "    print(f\"   Auxiliary models: ‚úÖ Loaded\")\n",
    "    print(f\"   Ready for inference and evaluation\")\n",
    "    \n",
    "    # Store model info for later use\n",
    "    TRAINED_MODELS = {\n",
    "        'custom_classifier': {\n",
    "            'model_path': model_path,\n",
    "            'model': trained_model,\n",
    "            'tokenizer': trained_tokenizer\n",
    "        },\n",
    "        'auxiliary': {\n",
    "            'sentiment': sentiment_pipeline,\n",
    "            'toxic': toxic_pipeline,\n",
    "            'zero_shot': zshot_pipeline\n",
    "        }\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nüîÑ PRE-TRAINED MODE\")\n",
    "    print(\"Using pre-trained models without fine-tuning...\")\n",
    "    \n",
    "    # Load all pre-trained models\n",
    "    sentiment_pipeline, toxic_pipeline, zshot_pipeline = load_auxiliary_pipelines(device)\n",
    "    \n",
    "    print(f\"‚úÖ PRE-TRAINED MODELS LOADED\")\n",
    "    print(f\"   Using existing models without training\")\n",
    "    \n",
    "    # Store model info for later use\n",
    "    TRAINED_MODELS = {\n",
    "        'custom_classifier': None,  # No custom training\n",
    "        'auxiliary': {\n",
    "            'sentiment': sentiment_pipeline,\n",
    "            'toxic': toxic_pipeline,\n",
    "            'zero_shot': zshot_pipeline\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Define inference functions for the trained/loaded models\n",
    "def policy_zero_shot(zshot, text: str, tau: float = 0.5):\n",
    "    \"\"\"Run zero-shot classification for policy violations\"\"\"\n",
    "    res = zshot(\n",
    "        text,\n",
    "        candidate_labels=ZERO_SHOT_LABELS,\n",
    "        hypothesis_template=\"This review is {}.\",\n",
    "        multi_label=True,\n",
    "    )\n",
    "    \n",
    "    scores = {lab: float(scr) for lab, scr in zip(res[\"labels\"], res[\"scores\"])}\n",
    "    \n",
    "    # Consider rejecting policies\n",
    "    reject_scores = {\n",
    "        ZERO_SHOT_TO_POLICY[ZERO_SHOT_LABELS[0]]: scores[ZERO_SHOT_LABELS[0]],\n",
    "        ZERO_SHOT_TO_POLICY[ZERO_SHOT_LABELS[1]]: scores[ZERO_SHOT_LABELS[1]],\n",
    "        ZERO_SHOT_TO_POLICY[ZERO_SHOT_LABELS[2]]: scores[ZERO_SHOT_LABELS[2]],\n",
    "    }\n",
    "    \n",
    "    best_cat, best_score = max(reject_scores.items(), key=lambda kv: kv[1])\n",
    "    \n",
    "    if best_score >= tau:\n",
    "        return best_cat, best_score\n",
    "    \n",
    "    return POLICY_CATEGORIES['NONE'], scores.get(ZERO_SHOT_LABELS[3], 1.0 - best_score)\n",
    "\n",
    "def predict_with_custom_model(text, model, tokenizer):\n",
    "    \"\"\"Use fine-tuned model for prediction\"\"\"\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        confidence = float(torch.max(predictions))\n",
    "        predicted_class = int(torch.argmax(predictions))\n",
    "    \n",
    "    label = \"REJECT\" if predicted_class == 1 else \"APPROVE\"\n",
    "    return label, confidence\n",
    "\n",
    "def run_inference_pipeline(df, tau=0.55):\n",
    "    \"\"\"Run inference using trained/loaded models\"\"\"\n",
    "    print(f\"\\nRunning inference pipeline...\")\n",
    "    print(f\"Mode: {training_mode}\")\n",
    "    print(f\"Samples: {len(df)}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing reviews\"):\n",
    "        txt = str(row['text'])\n",
    "        \n",
    "        # Primary classification\n",
    "        if TRAINED_MODELS['custom_classifier'] is not None:\n",
    "            # Use fine-tuned model\n",
    "            pred_label, confidence = predict_with_custom_model(\n",
    "                txt, \n",
    "                TRAINED_MODELS['custom_classifier']['model'],\n",
    "                TRAINED_MODELS['custom_classifier']['tokenizer']\n",
    "            )\n",
    "            policy_category = \"Custom_Trained\"  # Custom model doesn't predict categories\n",
    "        else:\n",
    "            # Use zero-shot classification\n",
    "            policy_category, confidence = policy_zero_shot(\n",
    "                TRAINED_MODELS['auxiliary']['zero_shot'], txt, tau=tau\n",
    "            )\n",
    "            pred_label = LABELS['REJECT'] if policy_category != POLICY_CATEGORIES['NONE'] else LABELS['APPROVE']\n",
    "        \n",
    "        # Auxiliary predictions\n",
    "        try:\n",
    "            s_result = TRAINED_MODELS['auxiliary']['sentiment'](txt)\n",
    "            s = s_result[0] if isinstance(s_result, list) and len(s_result) > 0 else {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
    "            \n",
    "            tox_result = TRAINED_MODELS['auxiliary']['toxic'](txt)\n",
    "            if isinstance(tox_result, list) and len(tox_result) > 0:\n",
    "                tox_label = tox_result[0].get(\"label\", \"NONE\")\n",
    "                tox_score = float(tox_result[0].get(\"score\", 0.0))\n",
    "            else:\n",
    "                tox_label, tox_score = \"NONE\", 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in auxiliary models: {e}\")\n",
    "            s = {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
    "            tox_label, tox_score = \"NONE\", 0.0\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": row.get('id', len(results) + 1),\n",
    "            \"text\": txt,\n",
    "            \"pred_label\": pred_label,\n",
    "            \"pred_category\": policy_category,\n",
    "            \"confidence\": round(float(confidence), 4),\n",
    "            \"sentiment_label\": s.get(\"label\", \"NEUTRAL\"),\n",
    "            \"sentiment_score\": round(float(s.get(\"score\", 0.5)), 4),\n",
    "            \"toxicity_label\": tox_label,\n",
    "            \"toxicity_score\": round(float(tox_score), 4),\n",
    "            \"model_type\": training_mode\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    output_path = f'results/predictions/predictions_{training_mode}.csv'\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Inference completed!\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    print(f\"Processed {len(results_df)} reviews\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Test the trained models on sample data\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING TRAINED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use available data for testing\n",
    "if 'df' in locals() and len(df) > 0:\n",
    "    test_data = df\n",
    "    data_source = \"Sample data\"\n",
    "elif 'pseudo_labels_df' in locals() and len(pseudo_labels_df) > 0:\n",
    "    test_data = pseudo_labels_df[['id', 'text']].head(5)  # Test on subset\n",
    "    data_source = \"Pseudo-labeled data\"\n",
    "else:\n",
    "    # Create minimal test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'id': [1, 2], \n",
    "        'text': [\n",
    "            \"Great food and service!\",\n",
    "            \"Use my promo code SAVE20 for discounts!\"\n",
    "        ]\n",
    "    })\n",
    "    data_source = \"Minimal test data\"\n",
    "\n",
    "print(f\"Testing with: {data_source}\")\n",
    "print(f\"Samples: {len(test_data)}\")\n",
    "\n",
    "# Run inference\n",
    "hf_results = run_inference_pipeline(test_data, tau=CONFIDENCE_THRESHOLDS['DEFAULT'])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìä INFERENCE RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "display_cols = ['id', 'text', 'pred_label', 'pred_category', 'confidence', 'model_type']\n",
    "\n",
    "# Truncate text for display\n",
    "display_df = hf_results.copy()\n",
    "display_df['text'] = display_df['text'].apply(lambda x: x[:50] + \"...\" if len(x) > 50 else x)\n",
    "\n",
    "if len(display_df) <= 10:\n",
    "    print(display_df[display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(display_df[display_cols].head(10).to_string(index=False))\n",
    "    print(f\"... and {len(display_df) - 10} more rows\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà RESULTS SUMMARY\")\n",
    "label_dist = hf_results['pred_label'].value_counts()\n",
    "print(f\"Label Distribution:\")\n",
    "for label, count in label_dist.items():\n",
    "    print(f\"   {label}: {count} reviews\")\n",
    "\n",
    "avg_conf = hf_results['confidence'].mean()\n",
    "print(f\"Average Confidence: {avg_conf:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL TRAINING AND TESTING COMPLETE\")\n",
    "print(f\"Training Mode: {training_mode.upper()}\")\n",
    "print(f\"Models Ready: {'‚úÖ Fine-tuned + Auxiliary' if training_mode == 'fine-tuning' else '‚úÖ Pre-trained'}\")\n",
    "print(f\"Ready for Model Persistence: ‚úÖ Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c020a20",
   "metadata": {},
   "source": [
    "## 9. Model Persistence and Export (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54193be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Persistence and Training Data Export\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "print(\"MODEL PERSISTENCE AND DATA EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('models/saved_models', exist_ok=True)\n",
    "os.makedirs('data/training', exist_ok=True)\n",
    "os.makedirs('data/predictions', exist_ok=True)\n",
    "os.makedirs('results/model_info', exist_ok=True)\n",
    "\n",
    "# Generate timestamp for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Model information\n",
    "model_info = {\n",
    "    \"training_timestamp\": timestamp,\n",
    "    \"training_mode\": training_mode,\n",
    "    \"base_model\": BASE_MODEL if training_mode == \"fine-tuning\" else \"pre-trained-only\",\n",
    "    \"auxiliary_models\": {\n",
    "        \"sentiment\": SENTIMENT_MODEL,\n",
    "        \"toxicity\": TOXIC_MODEL,\n",
    "        \"zero_shot\": ZERO_SHOT_MODEL\n",
    "    },\n",
    "    \"training_data_size\": len(pseudo_labels_df) if has_training_data else 0,\n",
    "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
    "    \"performance_metrics\": {}\n",
    "}\n",
    "\n",
    "print(f\"Training Mode: {training_mode}\")\n",
    "print(f\"Timestamp: {timestamp}\")\n",
    "\n",
    "# Save trained models\n",
    "if training_mode == \"fine-tuning\" and TRAINED_MODELS['custom_classifier'] is not None:\n",
    "    print(f\"\\nüíæ SAVING FINE-TUNED MODELS\")\n",
    "    \n",
    "    # Custom model is already saved during training\n",
    "    custom_model_path = TRAINED_MODELS['custom_classifier']['model_path']\n",
    "    final_model_path = f'models/saved_models/review_classifier_{timestamp}'\n",
    "    \n",
    "    # Copy to final location with timestamp\n",
    "    if os.path.exists(custom_model_path):\n",
    "        shutil.copytree(custom_model_path, final_model_path, dirs_exist_ok=True)\n",
    "        print(f\"‚úÖ Fine-tuned model copied to: {final_model_path}\")\n",
    "        \n",
    "        model_info[\"custom_model_path\"] = final_model_path\n",
    "        model_info[\"model_files\"] = {\n",
    "            \"config\": f\"{final_model_path}/config.json\",\n",
    "            \"model\": f\"{final_model_path}/pytorch_model.bin\",\n",
    "            \"tokenizer\": f\"{final_model_path}/tokenizer.json\"\n",
    "        }\n",
    "        \n",
    "        # Test model loading\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "            test_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "            test_model = AutoModelForSequenceClassification.from_pretrained(final_model_path)\n",
    "            print(\"‚úÖ Model loading test successful\")\n",
    "            model_info[\"model_loadable\"] = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model loading test failed: {e}\")\n",
    "            model_info[\"model_loadable\"] = False\n",
    "    else:\n",
    "        print(f\"‚ùå Custom model path not found: {custom_model_path}\")\n",
    "        model_info[\"custom_model_path\"] = None\n",
    "\n",
    "else:\n",
    "    print(f\"\\nüì¶ SAVING PRE-TRAINED MODEL REFERENCES\")\n",
    "    model_info[\"custom_model_path\"] = None\n",
    "    model_info[\"model_files\"] = None\n",
    "\n",
    "# Save auxiliary model pipeline state (for consistency in inference)\n",
    "auxiliary_info = {\n",
    "    \"sentiment_model\": SENTIMENT_MODEL,\n",
    "    \"toxicity_model\": TOXIC_MODEL, \n",
    "    \"zero_shot_model\": ZERO_SHOT_MODEL,\n",
    "    \"device_used\": 0 if torch.cuda.is_available() else -1,\n",
    "    \"models_loaded\": True if TRAINED_MODELS['auxiliary']['sentiment'] is not None else False\n",
    "}\n",
    "\n",
    "# Save training data if available\n",
    "if has_training_data:\n",
    "    print(f\"\\nüíæ SAVING TRAINING DATA\")\n",
    "    \n",
    "    # Save pseudo-labeled training data\n",
    "    training_data_path = f'data/training/pseudo_labels_{timestamp}.csv'\n",
    "    pseudo_labels_df.to_csv(training_data_path, index=False)\n",
    "    print(f\"‚úÖ Training data saved: {training_data_path}\")\n",
    "    \n",
    "    model_info[\"training_data_path\"] = training_data_path\n",
    "    model_info[\"training_data_size\"] = len(pseudo_labels_df)\n",
    "    \n",
    "    # Save label distribution\n",
    "    label_dist = pseudo_labels_df['pred_label'].value_counts().to_dict()\n",
    "    model_info[\"training_label_distribution\"] = label_dist\n",
    "    print(f\"   Training Labels: {label_dist}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No training data to save\")\n",
    "    model_info[\"training_data_path\"] = None\n",
    "\n",
    "# Save prediction results\n",
    "if 'hf_results' in locals():\n",
    "    print(f\"\\nüíæ SAVING PREDICTION RESULTS\")\n",
    "    \n",
    "    predictions_path = f'data/predictions/predictions_{timestamp}.csv'\n",
    "    hf_results.to_csv(predictions_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved: {predictions_path}\")\n",
    "    \n",
    "    model_info[\"predictions_path\"] = predictions_path\n",
    "    model_info[\"predictions_count\"] = len(hf_results)\n",
    "    \n",
    "    # Add performance metrics\n",
    "    pred_dist = hf_results['pred_label'].value_counts().to_dict()\n",
    "    avg_confidence = hf_results['confidence'].mean()\n",
    "    \n",
    "    model_info[\"performance_metrics\"] = {\n",
    "        \"prediction_distribution\": pred_dist,\n",
    "        \"average_confidence\": round(float(avg_confidence), 4),\n",
    "        \"total_processed\": len(hf_results)\n",
    "    }\n",
    "    print(f\"   Prediction Labels: {pred_dist}\")\n",
    "    print(f\"   Average Confidence: {avg_confidence:.4f}\")\n",
    "\n",
    "# Save comprehensive model information\n",
    "model_info_path = f'results/model_info/model_info_{timestamp}.json'\n",
    "with open(model_info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Model info saved: {model_info_path}\")\n",
    "\n",
    "# Create deployment-ready structure for inference pipeline\n",
    "print(f\"\\nüöÄ CREATING DEPLOYMENT STRUCTURE\")\n",
    "\n",
    "# Create data/actual with sample data for inference pipeline\n",
    "os.makedirs('data/actual', exist_ok=True)\n",
    "\n",
    "# If we have results, save a sample to data/actual for the inference pipeline\n",
    "if 'hf_results' in locals() and len(hf_results) > 0:\n",
    "    # Create sample data for inference testing\n",
    "    sample_actual = hf_results[['id', 'text']].head(3).copy()\n",
    "    sample_actual.to_csv('data/actual/sample_reviews.csv', index=False)\n",
    "    print(\"‚úÖ Sample data for inference: data/actual/sample_reviews.csv\")\n",
    "\n",
    "# Save latest model paths for inference pipeline\n",
    "latest_model_config = {\n",
    "    \"latest_model_info\": model_info_path,\n",
    "    \"training_mode\": training_mode,\n",
    "    \"custom_model_path\": model_info.get(\"custom_model_path\"),\n",
    "    \"auxiliary_models\": auxiliary_info,\n",
    "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
    "    \"timestamp\": timestamp\n",
    "}\n",
    "\n",
    "with open('models/saved_models/latest_config.json', 'w') as f:\n",
    "    json.dump(latest_model_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Latest model config: models/saved_models/latest_config.json\")\n",
    "\n",
    "# Create model loading utilities for inference\n",
    "model_loader_code = '''\"\"\"\n",
    "Model Loading Utilities for Review Classification Pipeline\n",
    "Generated automatically during training\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "def load_latest_models():\n",
    "    \"\"\"Load the most recently trained models\"\"\"\n",
    "    \n",
    "    config_path = 'models/saved_models/latest_config.json'\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(\"No trained models found. Run training pipeline first.\")\n",
    "    \n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Load custom model if available\n",
    "    if config['custom_model_path'] and os.path.exists(config['custom_model_path']):\n",
    "        models['custom'] = {\n",
    "            'tokenizer': AutoTokenizer.from_pretrained(config['custom_model_path']),\n",
    "            'model': AutoModelForSequenceClassification.from_pretrained(config['custom_model_path'])\n",
    "        }\n",
    "    \n",
    "    # Load auxiliary models\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    aux_config = config['auxiliary_models']\n",
    "    \n",
    "    models['auxiliary'] = {\n",
    "        'sentiment': pipeline(\"sentiment-analysis\", \n",
    "                            model=aux_config['sentiment_model'], device=device),\n",
    "        'toxicity': pipeline(\"text-classification\", \n",
    "                           model=aux_config['toxicity_model'], device=device),\n",
    "        'zero_shot': pipeline(\"zero-shot-classification\", \n",
    "                            model=aux_config['zero_shot_model'], device=device)\n",
    "    }\n",
    "    \n",
    "    return models, config\n",
    "\n",
    "def predict_review(text, models, config):\n",
    "    \"\"\"Make prediction using loaded models\"\"\"\n",
    "    \n",
    "    if 'custom' in models:\n",
    "        # Use fine-tuned model\n",
    "        inputs = models['custom']['tokenizer'](\n",
    "            text, truncation=True, padding=True, \n",
    "            max_length=256, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = models['custom']['model'](**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            confidence = float(torch.max(predictions))\n",
    "            predicted_class = int(torch.argmax(predictions))\n",
    "        \n",
    "        label = \"REJECT\" if predicted_class == 1 else \"APPROVE\"\n",
    "        \n",
    "    else:\n",
    "        # Use zero-shot classification\n",
    "        # Implementation would mirror the training notebook logic\n",
    "        label = \"APPROVE\"  # Placeholder\n",
    "        confidence = 0.5\n",
    "    \n",
    "    return {\n",
    "        'label': label,\n",
    "        'confidence': confidence,\n",
    "        'model_type': config['training_mode']\n",
    "    }\n",
    "'''\n",
    "\n",
    "with open('src/utils/model_loader.py', 'w') as f:\n",
    "    f.write(model_loader_code)\n",
    "\n",
    "print(\"‚úÖ Model loader utilities: src/utils/model_loader.py\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"PERSISTENCE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"‚úÖ Training Mode: {training_mode}\")\n",
    "print(f\"‚úÖ Timestamp: {timestamp}\")\n",
    "\n",
    "if model_info.get(\"custom_model_path\"):\n",
    "    print(f\"‚úÖ Custom Model: {model_info['custom_model_path']}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Custom Model: None (using pre-trained)\")\n",
    "\n",
    "print(f\"‚úÖ Model Info: {model_info_path}\")\n",
    "print(f\"‚úÖ Latest Config: models/saved_models/latest_config.json\")\n",
    "\n",
    "if has_training_data:\n",
    "    print(f\"‚úÖ Training Data: {model_info['training_data_path']}\")\n",
    "    print(f\"   Size: {model_info['training_data_size']} examples\")\n",
    "\n",
    "if 'hf_results' in locals():\n",
    "    print(f\"‚úÖ Predictions: {model_info['predictions_path']}\")\n",
    "    print(f\"   Processed: {model_info['predictions_count']} reviews\")\n",
    "\n",
    "print(f\"\\nüéØ READY FOR INFERENCE PIPELINE\")\n",
    "print(f\"   Use: models/saved_models/latest_config.json\")\n",
    "print(f\"   Data: data/actual/sample_reviews.csv\")\n",
    "print(f\"   Load: src/utils/model_loader.py\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL TRAINING AND PERSISTENCE COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d961c",
   "metadata": {},
   "source": [
    "## 10. Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e21722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Performance Analysis (Production-Quality)\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model_performance(predictions_df, output_dir='results/predictions'):\n",
    "    \"\"\"Comprehensive model evaluation matching production standards\"\"\"\n",
    "    \n",
    "    print(f\"Model Performance Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_predictions = len(predictions_df)\n",
    "    print(f\"Total predictions analyzed: {total_predictions}\")\n",
    "    \n",
    "    # Determine column names (flexible for different variable names)\n",
    "    label_col = None\n",
    "    category_col = None\n",
    "    confidence_col = None\n",
    "    \n",
    "    # Check for different possible column names\n",
    "    for col in predictions_df.columns:\n",
    "        if 'label' in col.lower() and label_col is None:\n",
    "            label_col = col\n",
    "        if 'category' in col.lower() and category_col is None:\n",
    "            category_col = col\n",
    "        if 'confidence' in col.lower() and confidence_col is None:\n",
    "            confidence_col = col\n",
    "    \n",
    "    if not all([label_col, category_col, confidence_col]):\n",
    "        print(f\"‚ùå Missing required columns. Available columns: {list(predictions_df.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Using columns: {label_col}, {category_col}, {confidence_col}\")\n",
    "    \n",
    "    # Label distribution analysis\n",
    "    label_dist = predictions_df[label_col].value_counts()\n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    for label, count in label_dist.items():\n",
    "        percentage = (count / total_predictions) * 100\n",
    "        print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Category distribution analysis\n",
    "    category_dist = predictions_df[category_col].value_counts()\n",
    "    print(f\"\\nPolicy Category Distribution:\")\n",
    "    for category, count in category_dist.items():\n",
    "        percentage = (count / total_predictions) * 100\n",
    "        print(f\"   {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Confidence analysis\n",
    "    confidence_stats = predictions_df[confidence_col].describe()\n",
    "    print(f\"\\nConfidence Score Statistics:\")\n",
    "    print(f\"   Mean: {confidence_stats['mean']:.3f}\")\n",
    "    print(f\"   Median: {confidence_stats['50%']:.3f}\")\n",
    "    print(f\"   Std Dev: {confidence_stats['std']:.3f}\")\n",
    "    print(f\"   Min: {confidence_stats['min']:.3f}\")\n",
    "    print(f\"   Max: {confidence_stats['max']:.3f}\")\n",
    "    \n",
    "    # High confidence analysis\n",
    "    high_conf_threshold = 0.8\n",
    "    high_conf_predictions = predictions_df[predictions_df[confidence_col] >= high_conf_threshold]\n",
    "    high_conf_percentage = (len(high_conf_predictions) / total_predictions) * 100\n",
    "    \n",
    "    print(f\"\\nHigh Confidence Analysis (>= {high_conf_threshold}):\")\n",
    "    print(f\"   High confidence predictions: {len(high_conf_predictions)} ({high_conf_percentage:.1f}%)\")\n",
    "    \n",
    "    if len(high_conf_predictions) > 0:\n",
    "        high_conf_labels = high_conf_predictions[label_col].value_counts()\n",
    "        print(f\"   High confidence by label:\")\n",
    "        for label, count in high_conf_labels.items():\n",
    "            print(f\"      {label}: {count}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Label distribution pie chart\n",
    "    axes[0, 0].pie(label_dist.values, labels=label_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Label Distribution')\n",
    "    \n",
    "    # Category distribution bar chart\n",
    "    category_dist.plot(kind='bar', ax=axes[0, 1], color='skyblue')\n",
    "    axes[0, 1].set_title('Policy Category Distribution')\n",
    "    axes[0, 1].set_xlabel('Category')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Confidence score histogram\n",
    "    axes[1, 0].hist(predictions_df[confidence_col], bins=20, color='lightgreen', alpha=0.7)\n",
    "    axes[1, 0].axvline(x=high_conf_threshold, color='red', linestyle='--', label=f'High Conf Threshold ({high_conf_threshold})')\n",
    "    axes[1, 0].set_title('Confidence Score Distribution')\n",
    "    axes[1, 0].set_xlabel('Confidence Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Confidence by label boxplot\n",
    "    sns.boxplot(data=predictions_df, x=label_col, y=confidence_col, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Confidence by Label')\n",
    "    axes[1, 1].set_xlabel('Label')\n",
    "    axes[1, 1].set_ylabel('Confidence Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(f\"\\nQuality Assessment:\")\n",
    "    \n",
    "    # Check for balanced predictions\n",
    "    label_balance = min(label_dist.values) / max(label_dist.values)\n",
    "    balance_status = \"‚úÖ Balanced\" if label_balance > 0.3 else \"‚ùå Imbalanced\"\n",
    "    print(f\"   Label balance ratio: {label_balance:.3f} ({balance_status})\")\n",
    "    \n",
    "    # Check confidence levels\n",
    "    avg_confidence = predictions_df[confidence_col].mean()\n",
    "    conf_status = \"‚úÖ High\" if avg_confidence > 0.7 else \"‚ùå Low\" if avg_confidence < 0.5 else \"‚ö†Ô∏è Medium\"\n",
    "    print(f\"   Average confidence: {avg_confidence:.3f} ({conf_status})\")\n",
    "    \n",
    "    # Check high confidence rate\n",
    "    high_conf_rate = high_conf_percentage / 100\n",
    "    hc_status = \"‚úÖ Good\" if high_conf_rate > 0.5 else \"‚ùå Low\" if high_conf_rate < 0.2 else \"‚ö†Ô∏è Moderate\"\n",
    "    print(f\"   High confidence rate: {high_conf_percentage:.1f}% ({hc_status})\")\n",
    "    \n",
    "    # Model readiness assessment\n",
    "    print(f\"\\nModel Readiness for Production:\")\n",
    "    \n",
    "    readiness_score = 0\n",
    "    max_score = 5\n",
    "    \n",
    "    # Criteria 1: Sufficient predictions\n",
    "    if total_predictions >= 5:\n",
    "        readiness_score += 1\n",
    "        print(f\"   ‚úÖ Sufficient predictions ({total_predictions})\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Insufficient predictions ({total_predictions})\")\n",
    "    \n",
    "    # Criteria 2: Reasonable confidence\n",
    "    if avg_confidence >= 0.6:\n",
    "        readiness_score += 1\n",
    "        print(f\"   ‚úÖ Reasonable average confidence ({avg_confidence:.3f})\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Low average confidence ({avg_confidence:.3f})\")\n",
    "    \n",
    "    # Criteria 3: High confidence predictions available\n",
    "    if high_conf_percentage >= 30:\n",
    "        readiness_score += 1\n",
    "        print(f\"   ‚úÖ Good high-confidence rate ({high_conf_percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Low high-confidence rate ({high_conf_percentage:.1f}%)\")\n",
    "    \n",
    "    # Criteria 4: Category coverage\n",
    "    if len(category_dist) >= 2:\n",
    "        readiness_score += 1\n",
    "        print(f\"   ‚úÖ Good category coverage ({len(category_dist)} categories)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Limited category coverage ({len(category_dist)} categories)\")\n",
    "    \n",
    "    # Criteria 5: No extreme imbalance\n",
    "    if label_balance >= 0.1:\n",
    "        readiness_score += 1\n",
    "        print(f\"   ‚úÖ Acceptable label balance ({label_balance:.3f})\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Extreme label imbalance ({label_balance:.3f})\")\n",
    "    \n",
    "    # Overall readiness\n",
    "    readiness_percentage = (readiness_score / max_score) * 100\n",
    "    if readiness_percentage >= 80:\n",
    "        readiness_status = \"‚úÖ Ready for Production\"\n",
    "    elif readiness_percentage >= 60:\n",
    "        readiness_status = \"‚ö†Ô∏è Needs Minor Improvements\"\n",
    "    else:\n",
    "        readiness_status = \"‚ùå Needs Major Improvements\"\n",
    "    \n",
    "    print(f\"\\nOverall Readiness: {readiness_score}/{max_score} ({readiness_percentage:.0f}%) - {readiness_status}\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    evaluation_summary = {\n",
    "        'total_predictions': total_predictions,\n",
    "        'label_distribution': label_dist.to_dict(),\n",
    "        'category_distribution': category_dist.to_dict(),\n",
    "        'average_confidence': avg_confidence,\n",
    "        'high_confidence_rate': high_conf_percentage,\n",
    "        'readiness_score': readiness_score,\n",
    "        'readiness_percentage': readiness_percentage,\n",
    "        'readiness_status': readiness_status\n",
    "    }\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save detailed results\n",
    "    evaluation_path = os.path.join(output_dir, 'model_evaluation.json')\n",
    "    with open(evaluation_path, 'w') as f:\n",
    "        json.dump(evaluation_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nEvaluation results saved to: {evaluation_path}\")\n",
    "    \n",
    "    return evaluation_summary\n",
    "\n",
    "# Check for available prediction data and run evaluation\n",
    "print(\"Checking for available prediction data...\")\n",
    "\n",
    "# Look for different possible variable names\n",
    "prediction_vars = []\n",
    "available_vars = dir()\n",
    "\n",
    "# Check for common prediction variable names\n",
    "possible_names = ['hf_results', 'all_predictions_df', 'predictions_df', 'results_df', 'df']\n",
    "for var_name in possible_names:\n",
    "    if var_name in available_vars:\n",
    "        var_value = globals()[var_name]\n",
    "        if hasattr(var_value, 'shape') and len(var_value) > 0:\n",
    "            # Check if it looks like prediction data\n",
    "            columns = list(var_value.columns) if hasattr(var_value, 'columns') else []\n",
    "            if any('label' in col.lower() or 'pred' in col.lower() for col in columns):\n",
    "                prediction_vars.append((var_name, var_value))\n",
    "                print(f\"   Found prediction data: {var_name} ({len(var_value)} rows)\")\n",
    "\n",
    "if prediction_vars:\n",
    "    # Use the most likely prediction dataset\n",
    "    var_name, predictions_df = prediction_vars[0]\n",
    "    print(f\"\\nUsing prediction data from: {var_name}\")\n",
    "    print(f\"Columns available: {list(predictions_df.columns)}\")\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    print(f\"\\nRunning comprehensive model evaluation...\")\n",
    "    evaluation_results = evaluate_model_performance(predictions_df)\n",
    "    \n",
    "    if evaluation_results:\n",
    "        print(f\"\\nEvaluation Complete!\")\n",
    "        print(f\"   Model performance: {'‚úÖ Satisfactory' if evaluation_results['readiness_percentage'] >= 60 else '‚ùå Needs improvement'}\")\n",
    "        print(f\"   Ready for deployment: {'‚úÖ Yes' if evaluation_results['readiness_percentage'] >= 80 else '‚ùå No'}\")\n",
    "        print(f\"   Integration ready: ‚úÖ Yes (structured output format)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No prediction data available for evaluation\")\n",
    "    print(\"Available variables:\", [var for var in available_vars if not var.startswith('_')])\n",
    "    print(\"\\nTo fix this issue:\")\n",
    "    print(\"1. Run the HuggingFace Pipeline cell (cell 7) first\")\n",
    "    print(\"2. Make sure the pipeline completes successfully\")\n",
    "    print(\"3. Then run this evaluation cell\")\n",
    "    print(\"\\nThe HuggingFace pipeline should create a 'hf_results' variable with predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ac73c",
   "metadata": {},
   "source": [
    "## 11. Pipeline Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Pipeline Summary and Architecture Validation\n",
    "print(\"REVIEW-RATER PIPELINE ARCHITECTURE 2.0\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Environment Summary\n",
    "print(f\"\\n1. ENVIRONMENT SETUP\")\n",
    "print(f\"   Platform: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"   GPU Available: {'‚úÖ Yes' if torch.cuda.is_available() else '‚ùå No'}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Directory Structure Validation\n",
    "print(f\"\\n2. DIRECTORY STRUCTURE\")\n",
    "expected_dirs = [\n",
    "    'data/raw', 'data/clean', 'data/pseudo-label', 'data/training', \n",
    "    'data/testing', 'data/actual', 'data/sample',\n",
    "    'models/saved_models', 'models/cache',\n",
    "    'results/predictions', 'results/inference'\n",
    "]\n",
    "\n",
    "for directory in expected_dirs:\n",
    "    status = \"‚úÖ\" if os.path.exists(directory) else \"‚ùå\"\n",
    "    print(f\"   {status} {directory}\")\n",
    "\n",
    "# Pipeline Architecture Summary\n",
    "print(f\"\\n3. PIPELINE ARCHITECTURE\")\n",
    "print(f\"   Training Flow (00_ipynb):\")\n",
    "print(f\"      data/raw ‚Üí (external) ‚Üí data/clean\")\n",
    "print(f\"      data/clean ‚Üí (gemini) ‚Üí data/pseudo-label\") \n",
    "print(f\"      data/pseudo-label ‚Üí data/testing + data/training\")\n",
    "print(f\"      data/clean ‚Üí data/training (combined)\")\n",
    "print(f\"      HuggingFace training on data/training with feedback loop\")\n",
    "print(f\"      Trained models ‚Üí models/saved_models\")\n",
    "print(f\"\")\n",
    "print(f\"   Inference Flow (01_ipynb):\")\n",
    "print(f\"      data/actual ‚Üí models/saved_models ‚Üí inference ‚Üí results/inference\")\n",
    "\n",
    "# Component Status\n",
    "print(f\"\\n4. COMPONENT STATUS\")\n",
    "components = {\n",
    "    'Constants loaded': 'DEFAULT_MODELS' in globals(),\n",
    "    'Sample data ready': 'df' in locals() or 'sample_df' in locals(),\n",
    "    'HuggingFace ready': True,  # Installed in environment setup\n",
    "    'Gemini available': 'gemini_available' in locals() and locals().get('gemini_available', False),\n",
    "    'Directory structure': all(os.path.exists(d) for d in ['data/clean', 'data/pseudo-label', 'data/actual']),\n",
    "}\n",
    "\n",
    "for component, status in components.items():\n",
    "    print(f\"   {'‚úÖ' if status else '‚ùå'} {component}\")\n",
    "\n",
    "# Model Performance Summary\n",
    "print(f\"\\n5. MODEL PERFORMANCE\")\n",
    "prediction_data = None\n",
    "for var_name in ['hf_results', 'all_predictions_df', 'predictions_df', 'results_df']:\n",
    "    if var_name in globals():\n",
    "        var_value = globals()[var_name]\n",
    "        if hasattr(var_value, 'shape') and len(var_value) > 0:\n",
    "            prediction_data = var_value\n",
    "            break\n",
    "\n",
    "if prediction_data is not None:\n",
    "    print(f\"   ‚úÖ Predictions available: {len(prediction_data)} reviews\")\n",
    "    if 'confidence' in prediction_data.columns:\n",
    "        avg_conf = prediction_data['confidence'].mean()\n",
    "        print(f\"   ‚úÖ Average confidence: {avg_conf:.3f}\")\n",
    "    if 'pred_label' in prediction_data.columns:\n",
    "        label_dist = prediction_data['pred_label'].value_counts()\n",
    "        print(f\"   ‚úÖ Label distribution: {dict(label_dist)}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå No prediction data available\")\n",
    "\n",
    "# Integration Readiness\n",
    "print(f\"\\n6. INTEGRATION READINESS\")\n",
    "integration_checks = {\n",
    "    'Structured output': prediction_data is not None,\n",
    "    'Spam detection ready': True,  # Architecture supports it\n",
    "    'Production deployment': os.path.exists('data/actual'),\n",
    "    'Model persistence': 'save_trained_pipeline' in globals(),\n",
    "    'Inference pipeline': os.path.exists('notebooks/01_inference_pipeline.ipynb'),\n",
    "}\n",
    "\n",
    "for check, status in integration_checks.items():\n",
    "    print(f\"   {'‚úÖ' if status else '‚ùå'} {check}\")\n",
    "\n",
    "# Next Steps\n",
    "print(f\"\\n7. NEXT STEPS\")\n",
    "print(f\"   Training Phase (This Notebook):\")\n",
    "print(f\"   1. ‚úÖ Environment setup complete\")\n",
    "print(f\"   2. ‚úÖ Directory structure created\")\n",
    "print(f\"   3. ‚úÖ Pipeline architecture established\")\n",
    "print(f\"   4. üîÑ Run HuggingFace pipeline (cell 8)\")\n",
    "print(f\"   5. üîÑ Export trained models (cell 9)\")\n",
    "print(f\"\")\n",
    "print(f\"   Production Phase:\")\n",
    "print(f\"   1. üìã Place actual review data in data/actual/\")\n",
    "print(f\"   2. üìã Run 01_inference_pipeline.ipynb\")\n",
    "print(f\"   3. üìã Check results in results/inference/\")\n",
    "\n",
    "# Final Status\n",
    "print(f\"\\n8. OVERALL STATUS\")\n",
    "overall_ready = all([\n",
    "    os.path.exists('data/clean'),\n",
    "    os.path.exists('data/actual'), \n",
    "    'DEFAULT_MODELS' in globals(),\n",
    "    'save_trained_pipeline' in globals()\n",
    "])\n",
    "\n",
    "if overall_ready:\n",
    "    print(f\"   üöÄ PIPELINE READY FOR PRODUCTION\")\n",
    "    print(f\"   ‚úÖ Training architecture: Complete\")\n",
    "    print(f\"   ‚úÖ Inference architecture: Complete\") \n",
    "    print(f\"   ‚úÖ Data flow: Established\")\n",
    "    print(f\"   ‚úÖ Integration points: Ready\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  PIPELINE SETUP IN PROGRESS\")\n",
    "    print(f\"   Run all cells to complete setup\")\n",
    "\n",
    "print(f\"\\nPIPELINE ARCHITECTURE 2.0 SUMMARY\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"‚úÖ data/raw ‚Üí data/clean ‚Üí data/pseudo-label ‚Üí data/training/testing\")\n",
    "print(f\"‚úÖ HuggingFace training with Gemini feedback loop\")\n",
    "print(f\"‚úÖ models/saved_models for production deployment\")\n",
    "print(f\"‚úÖ data/actual ‚Üí 01_ipynb ‚Üí results/inference\")\n",
    "print(f\"‚úÖ Spam detection integration ready\")\n",
    "print(f\"‚úÖ Complete separation of training and inference phases\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
