{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tathan08/review-rater/blob/main/notebooks/00_colab_complete_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6da5986",
      "metadata": {
        "id": "c6da5986"
      },
      "source": [
        "# Pipeline Architecture 2.0 - Training Phase\n",
        "\n",
        "## Data Flow Overview\n",
        "\n",
        "```\n",
        "data/raw ‚Üí (external processing) ‚Üí data/clean\n",
        "data/clean ‚Üí (00_ipynb + gemini) ‚Üí data/pseudo-label  \n",
        "data/pseudo-label ‚Üí data/testing + data/training\n",
        "data/clean ‚Üí data/training (combined)\n",
        "HuggingFace model trained on data/training with feedback loop against gemini\n",
        "Trained models ‚Üí models/saved_models\n",
        "```\n",
        "\n",
        "## Directory Structure\n",
        "\n",
        "### Data Directories\n",
        "- **`data/raw`**: Raw input data (processed externally)\n",
        "- **`data/clean`**: Cleaned/processed data from data/raw\n",
        "- **`data/pseudo-label`**: Pseudo-labeled data generated by Gemini from data/clean\n",
        "- **`data/training`**: Training data (combination of data/clean + data/pseudo-label)\n",
        "- **`data/testing`**: Testing data split from data/pseudo-label\n",
        "- **`data/actual`**: Production data for inference (used by 01_inference_pipeline.ipynb)\n",
        "\n",
        "### Model Directories\n",
        "- **`models/saved_models`**: Trained models ready for production\n",
        "- **`models/cache`**: Model cache files\n",
        "\n",
        "### Results Directories\n",
        "- **`results/predictions`**: Training predictions and evaluations\n",
        "- **`results/inference`**: Production inference results\n",
        "\n",
        "## Pipeline Components\n",
        "\n",
        "### Training Phase (This Notebook - 00_ipynb)\n",
        "1. **Environment Setup**: Install packages, configure GPU\n",
        "2. **Data Processing**: Create directory structure, load sample data\n",
        "3. **Gemini Pseudo-Labeling**: Generate high-quality labels for training\n",
        "4. **HuggingFace Training**: Train models with feedback loop against Gemini\n",
        "5. **Model Export**: Save trained models to models/saved_models\n",
        "\n",
        "### Inference Phase (01_ipynb)\n",
        "1. **Load Trained Models**: From models/saved_models\n",
        "2. **Process Production Data**: From data/actual\n",
        "3. **Generate Predictions**: Using trained pipeline\n",
        "4. **Save Results**: To results/inference\n",
        "\n",
        "## Integration Points\n",
        "- **Spam Detection**: Pipeline ready for spam detection model integration\n",
        "- **Feedback Loop**: HuggingFace model iteratively improved against Gemini predictions\n",
        "- **Production Ready**: Complete separation of training and inference phases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b975af5e",
      "metadata": {
        "id": "b975af5e"
      },
      "source": [
        "# Review Classification Pipeline - Complete Google Colab Implementation\n",
        "\n",
        "This notebook implements the complete review classification pipeline for detecting Google review policy violations, fully configured for Google Colab.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "### Phase 1: Environment Setup and Data Structure\n",
        "- Install all required packages (transformers, torch, google-generativeai, etc.)\n",
        "- Create proper directory structure (data/clean, data/pseudo-label, etc.)\n",
        "- Load sample data for demonstration\n",
        "\n",
        "### Phase 2: Core Pipeline Components\n",
        "- **Ollama Pipeline**: Local LLM classification (for reference, not runnable in Colab)\n",
        "- **HuggingFace Pipeline**: Zero-shot classification using pre-trained models\n",
        "- **Gemini Pseudo-Labeling**: High-quality label generation for training data\n",
        "- **Ensemble Method**: Combines multiple approaches for best results\n",
        "\n",
        "### Phase 3: Future Spam Detection Integration\n",
        "- Pipeline output will be piped into a spam detection model\n",
        "- Structured JSON output format for downstream processing\n",
        "- Confidence scoring for reliable filtering\n",
        "\n",
        "### Phase 4: Evaluation and Analysis\n",
        "- Comprehensive performance metrics\n",
        "- Policy category accuracy assessment\n",
        "- Model comparison and improvement recommendations\n",
        "\n",
        "**Key Features:**\n",
        "- **Policy Categories**: No_Ads, Irrelevant, Rant_No_Visit detection\n",
        "- **Zero Setup**: Everything configured for Google Colab\n",
        "- **Extensible**: Ready for spam detection integration\n",
        "- **Production Ready**: Structured output and comprehensive evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5462d932",
      "metadata": {
        "id": "5462d932"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96f35169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96f35169",
        "outputId": "31362232-870d-474b-ccc1-e975548af09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m‚úÖ Core packages installed successfully!\n",
            "Using device: cpu\n",
            "Using CPU - models will run slower but still functional\n",
            "Environment configured for optimal performance\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for the complete pipeline\n",
        "!pip install -q transformers==4.43.3 torch pandas scikit-learn\n",
        "!pip install -q google-generativeai tqdm datasets accelerate\n",
        "!pip install -q ipywidgets matplotlib seaborn wordcloud\n",
        "\n",
        "print(\"‚úÖ Core packages installed successfully!\")\n",
        "\n",
        "# Check GPU availability and setup device\n",
        "import torch\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"Using CPU - models will run slower but still functional\")\n",
        "\n",
        "# Set environment for optimal performance\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid warnings\n",
        "print(\"Environment configured for optimal performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d42837",
      "metadata": {
        "id": "77d42837"
      },
      "source": [
        "## 2. Project Structure Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c0558d7e",
      "metadata": {
        "id": "c0558d7e",
        "outputId": "7609c7ff-1da3-4054-d40b-25c78b9bb645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Complete directory structure created!\n",
            "Created 20 directories\n",
            "‚úÖ data/clean\n",
            "‚úÖ data/pseudo-label\n",
            "‚úÖ data/sample\n",
            "‚úÖ results/predictions\n",
            "\n",
            "Directory structure matches production pipeline!\n"
          ]
        }
      ],
      "source": [
        "# Create complete directory structure matching the actual pipeline\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create all necessary directories (matching actual pipeline structure)\n",
        "directories = [\n",
        "    # Source code structure\n",
        "    'src/config', 'src/core', 'src/pseudo_labelling', 'src/pipeline', 'src/integration',\n",
        "\n",
        "    # Data directories (matching actual structure)\n",
        "    'data/raw',           # For raw input data\n",
        "    'data/clean',         # For cleaned/processed data (from data/raw)\n",
        "    'data/pseudo-label',  # For pseudo-labeled data from Gemini (from data/clean)\n",
        "    'data/training',      # For training data split (from data/clean + data/pseudo-label)\n",
        "    'data/testing',       # For testing data split (from data/pseudo-label)\n",
        "    'data/actual',        # For actual production data to be processed by 01_inference_pipeline.ipynb\n",
        "    'data/sample',        # For sample data\n",
        "\n",
        "    # Results directories\n",
        "    'results/predictions',   # All predictions\n",
        "    'results/evaluations',   # For evaluation results\n",
        "    'results/reports',       # For generated reports\n",
        "\n",
        "    # Other directories\n",
        "    'models/saved_models',   # For trained models\n",
        "    'models/cache',          # For model cache\n",
        "    'logs/pipeline_logs',    # For pipeline logs\n",
        "    'prompts',               # Prompt engineering\n",
        "    'docs'                   # Documentation\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    # Create __init__.py files for Python packages\n",
        "    if directory.startswith('src/'):\n",
        "        with open(f'{directory}/__init__.py', 'w') as f:\n",
        "            f.write('# Review Classification Pipeline Package\\n')\n",
        "\n",
        "print(\"‚úÖ Complete directory structure created!\")\n",
        "print(f\"Created {len(directories)} directories\")\n",
        "\n",
        "# Verify critical directories exist\n",
        "critical_dirs = ['data/clean', 'data/pseudo-label', 'data/sample', 'results/predictions']\n",
        "for dir_name in critical_dirs:\n",
        "    if os.path.exists(dir_name):\n",
        "        print(f\"‚úÖ {dir_name}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {dir_name} - MISSING!\")\n",
        "\n",
        "print(\"\\nDirectory structure matches production pipeline!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc18cdbb",
      "metadata": {
        "id": "bc18cdbb"
      },
      "source": [
        "## 3. Sample Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8856980d",
      "metadata": {
        "id": "8856980d",
        "outputId": "d07ec10f-8891-4d27-9636-997224f166b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Production sample data loaded!\n",
            "\n",
            "Sample Data Overview:\n",
            " id                                                             text gold_label gold_category\n",
            "  1          Use my promo code EAT10 for 10% off! DM me on WhatsApp.     REJECT        No_Ads\n",
            "  2     Great laksa; broth was rich and staff friendly. Will return.    APPROVE          None\n",
            "  3 Crypto is the future. Buy BTC now! Nothing to do with this cafe.     REJECT    Irrelevant\n",
            "  4                          Overpriced scammers. Society is doomed.     REJECT Rant_No_Visit\n",
            "  5 Visited on 18 Aug, ordered set A; cashier fixed a double-charge.    APPROVE          None\n",
            "\n",
            "Label Distribution:\n",
            "APPROVE: 2 reviews\n",
            "REJECT:  3 reviews\n",
            "\n",
            "Category Distribution:\n",
            "None: 2 reviews\n",
            "No_Ads: 1 reviews\n",
            "Irrelevant: 1 reviews\n",
            "Rant_No_Visit: 1 reviews\n",
            "\n",
            "This data demonstrates all policy violation types:\n",
            "‚Ä¢ No_Ads: Promotional codes and contact solicitation\n",
            "‚Ä¢ Irrelevant: Off-topic content unrelated to business\n",
            "‚Ä¢ Rant_No_Visit: Generic negative comments without visit evidence\n",
            "‚Ä¢ None: Legitimate reviews that should be approved\n"
          ]
        }
      ],
      "source": [
        "# Load actual sample data from the production pipeline\n",
        "sample_data = {\n",
        "    'id': [1, 2, 3, 4, 5],\n",
        "    'text': [\n",
        "        \"Use my promo code EAT10 for 10% off! DM me on WhatsApp.\",\n",
        "        \"Great laksa; broth was rich and staff friendly. Will return.\",\n",
        "        \"Crypto is the future. Buy BTC now! Nothing to do with this cafe.\",\n",
        "        \"Overpriced scammers. Society is doomed.\",\n",
        "        \"Visited on 18 Aug, ordered set A; cashier fixed a double-charge.\"\n",
        "    ],\n",
        "    'gold_label': ['REJECT', 'APPROVE', 'REJECT', 'REJECT', 'APPROVE'],\n",
        "    'gold_category': ['No_Ads', 'None', 'Irrelevant', 'Rant_No_Visit', 'None']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "df.to_csv('data/sample/sample_reviews.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Production sample data loaded!\")\n",
        "print(\"\\nSample Data Overview:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "print(f\"APPROVE: {len(df[df['gold_label'] == 'APPROVE'])} reviews\")\n",
        "print(f\"REJECT:  {len(df[df['gold_label'] == 'REJECT'])} reviews\")\n",
        "\n",
        "print(f\"\\nCategory Distribution:\")\n",
        "for category in df['gold_category'].value_counts().index:\n",
        "    count = df['gold_category'].value_counts()[category]\n",
        "    print(f\"{category}: {count} reviews\")\n",
        "\n",
        "print(f\"\\nThis data demonstrates all policy violation types:\")\n",
        "print(\"‚Ä¢ No_Ads: Promotional codes and contact solicitation\")\n",
        "print(\"‚Ä¢ Irrelevant: Off-topic content unrelated to business\")\n",
        "print(\"‚Ä¢ Rant_No_Visit: Generic negative comments without visit evidence\")\n",
        "print(\"‚Ä¢ None: Legitimate reviews that should be approved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1014facd",
      "metadata": {
        "id": "1014facd"
      },
      "source": [
        "## 4. Configuration Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a97baccb",
      "metadata": {
        "id": "a97baccb",
        "outputId": "ade7d239-6492-4c15-cba0-e6e0bf3fb479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration created matching production pipeline!\n",
            "Data directory: data/sample\n",
            "HF Zero-shot model: facebook/bart-large-mnli\n",
            "Ensemble tau: 0.55\n",
            "Predictions output: results/predictions/predictions_hf.csv\n"
          ]
        }
      ],
      "source": [
        "# Create configuration classes matching the actual pipeline\n",
        "config_code = '''\n",
        "\"\"\"\n",
        "Pipeline Configuration Classes - Matching Production Structure\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for model settings\"\"\"\n",
        "    # HuggingFace models (matching actual pipeline)\n",
        "    hf_sentiment_model: str = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "    hf_toxicity_model: str = \"unitary/toxic-bert\"\n",
        "    hf_zero_shot_model: str = \"facebook/bart-large-mnli\"\n",
        "\n",
        "    # Gemini configuration\n",
        "    gemini_model: str = \"gemini-2.5-flash-lite\"\n",
        "\n",
        "    # Confidence thresholds (matching actual pipeline)\n",
        "    sentiment_threshold: float = 0.7\n",
        "    toxicity_threshold: float = 0.5\n",
        "    zero_shot_threshold: float = 0.7\n",
        "    ensemble_tau: float = 0.55\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    \"\"\"Configuration for data paths and settings\"\"\"\n",
        "    data_dir: str = \"data\"\n",
        "    raw_data_dir: str = \"data/raw\"\n",
        "    processed_data_dir: str = \"data/clean\"  # Matches actual structure\n",
        "    sample_data_dir: str = \"data/sample\"\n",
        "    pseudo_label_dir: str = \"data/pseudo-label\"  # Matches actual structure\n",
        "    training_dir: str = \"data/training\"\n",
        "    testing_dir: str = \"data/testing\"\n",
        "\n",
        "    # Default input file\n",
        "    sample_reviews_file: str = \"data/sample/sample_reviews.csv\"\n",
        "\n",
        "@dataclass\n",
        "class OutputConfig:\n",
        "    \"\"\"Configuration for output paths\"\"\"\n",
        "    results_dir: str = \"results\"\n",
        "    predictions_dir: str = \"results/predictions\"\n",
        "    evaluations_dir: str = \"results/evaluations\"\n",
        "    reports_dir: str = \"results/reports\"\n",
        "\n",
        "    # Default output files (matching actual pipeline)\n",
        "    hf_predictions: str = \"results/predictions/predictions_hf.csv\"\n",
        "    ensemble_predictions: str = \"results/predictions/predictions_ens.csv\"\n",
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    \"\"\"Main pipeline configuration combining all components\"\"\"\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    output: OutputConfig = field(default_factory=OutputConfig)\n",
        "\n",
        "    # Gemini configuration\n",
        "    gemini_api_key: str = \"\"\n",
        "\n",
        "    # Pipeline settings\n",
        "    batch_size: int = 32\n",
        "    max_workers: int = 4\n",
        "    cache_predictions: bool = True\n",
        "    verbose_logging: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create directories if they don't exist\"\"\"\n",
        "        directories = [\n",
        "            self.data.raw_data_dir,\n",
        "            self.data.processed_data_dir,\n",
        "            self.data.sample_data_dir,\n",
        "            self.data.pseudo_label_dir,\n",
        "            self.data.training_dir,\n",
        "            self.data.testing_dir,\n",
        "            self.output.predictions_dir,\n",
        "            self.output.evaluations_dir,\n",
        "            self.output.reports_dir\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Global configuration instance\n",
        "config = PipelineConfig()\n",
        "'''\n",
        "\n",
        "with open('src/config/pipeline_config.py', 'w') as f:\n",
        "    f.write(config_code)\n",
        "\n",
        "print(\"‚úÖ Configuration created matching production pipeline!\")\n",
        "\n",
        "# Test configuration\n",
        "exec(config_code)\n",
        "test_config = PipelineConfig()\n",
        "print(f\"Data directory: {test_config.data.sample_data_dir}\")\n",
        "print(f\"HF Zero-shot model: {test_config.model.hf_zero_shot_model}\")\n",
        "print(f\"Ensemble tau: {test_config.model.ensemble_tau}\")\n",
        "print(f\"Predictions output: {test_config.output.hf_predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "becf61f4",
      "metadata": {
        "id": "becf61f4"
      },
      "source": [
        "## 5. Constants and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f5b648a5",
      "metadata": {
        "id": "f5b648a5",
        "outputId": "f790b7c6-efeb-4322-eeed-a601b244e850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Constants and prompts created matching production pipeline!\n",
            "Policy categories: ['No_Ads', 'Irrelevant', 'Rant_No_Visit', 'None']\n",
            "Zero-shot model: facebook/bart-large-mnli\n",
            "Default confidence threshold: 0.55\n",
            "Zero-shot labels configured: 4 categories\n"
          ]
        }
      ],
      "source": [
        "# Create constants and prompts matching the actual pipeline\n",
        "constants_code = '''\n",
        "\"\"\"\n",
        "Core Constants - Matching Production Pipeline\n",
        "\"\"\"\n",
        "\n",
        "# Policy Categories (matching actual pipeline)\n",
        "POLICY_CATEGORIES = {\n",
        "    'NO_ADS': 'No_Ads',\n",
        "    'IRRELEVANT': 'Irrelevant',\n",
        "    'RANT_NO_VISIT': 'Rant_No_Visit',\n",
        "    'NONE': 'None'\n",
        "}\n",
        "\n",
        "# Label Types (matching actual pipeline)\n",
        "LABELS = {\n",
        "    'APPROVE': 'APPROVE',\n",
        "    'REJECT': 'REJECT'\n",
        "}\n",
        "\n",
        "# Default Models (matching actual pipeline)\n",
        "DEFAULT_MODELS = {\n",
        "    'SENTIMENT': \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    'TOXIC': \"unitary/toxic-bert\",\n",
        "    'ZERO_SHOT': \"facebook/bart-large-mnli\",\n",
        "    'GEMINI_DEFAULT': \"gemini-2.5-flash-lite\"\n",
        "}\n",
        "\n",
        "# Zero-shot Classification Labels (matching actual pipeline)\n",
        "ZERO_SHOT_LABELS = [\n",
        "    \"an advertisement or promotional solicitation for this business (promo code, referral, links, contact to buy)\",\n",
        "    \"off-topic or unrelated to this business (e.g., politics, crypto, chain messages, personal stories not about this place)\",\n",
        "    \"a generic negative rant about this business without evidence of a visit (short insults, 'scam', 'overpriced', 'worst ever')\",\n",
        "    \"a relevant on-topic description of a visit or experience at this business\"\n",
        "]\n",
        "\n",
        "# Mapping zero-shot labels to policy categories\n",
        "ZERO_SHOT_TO_POLICY = {\n",
        "    ZERO_SHOT_LABELS[0]: POLICY_CATEGORIES['NO_ADS'],\n",
        "    ZERO_SHOT_LABELS[1]: POLICY_CATEGORIES['IRRELEVANT'],\n",
        "    ZERO_SHOT_LABELS[2]: POLICY_CATEGORIES['RANT_NO_VISIT'],\n",
        "    ZERO_SHOT_LABELS[3]: POLICY_CATEGORIES['NONE']\n",
        "}\n",
        "\n",
        "# Confidence Thresholds\n",
        "CONFIDENCE_THRESHOLDS = {\n",
        "    'HIGH': 0.8,\n",
        "    'MEDIUM': 0.6,\n",
        "    'LOW': 0.4,\n",
        "    'DEFAULT': 0.55\n",
        "}\n",
        "'''\n",
        "\n",
        "with open('src/core/constants.py', 'w') as f:\n",
        "    f.write(constants_code)\n",
        "\n",
        "# Create prompt templates (matching actual pipeline)\n",
        "prompts_code = '''\n",
        "\"\"\"\n",
        "Policy Prompts - Matching Production Pipeline\n",
        "\"\"\"\n",
        "\n",
        "# JSON schema all prompts must return\n",
        "TEMPLATE_JSON = \"\"\"Return ONLY JSON with no extra text:\n",
        "{\"label\":\"<APPROVE|REJECT>\",\"category\":\"<No_Ads|Irrelevant|Rant_No_Visit|None>\",\n",
        " \"rationale\":\"<short>\",\"confidence\":<0.0-1.0>,\n",
        " \"flags\":{\"links\":false,\"coupon\":false,\"visit_claimed\":false}}\n",
        "\"\"\"\n",
        "\n",
        "# ===== 1) NO ADS / PROMOTIONAL =====\n",
        "NO_ADS_SYSTEM = \"\"\"You are a content policy checker for location reviews.\n",
        "If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0. Do not reject for other policies.\n",
        "Reject ONLY if the review contains clear advertising or promotional solicitation:\n",
        "- referral/promo/coupon codes, price lists, booking/ordering links, contact-for-order (DM me / WhatsApp / Telegram / email / call), affiliate pitches.\n",
        "Do NOT mark generic off-topic content (e.g., crypto/politics) as Ads unless it includes explicit solicitation to buy or contact.\n",
        "Approve normal experiences even if positive or mentioning 'cheap' or 'good deal'.\n",
        "Output the required JSON only.\n",
        "\"\"\"\n",
        "\n",
        "# ===== 2) IRRELEVANT CONTENT =====\n",
        "IRRELEVANT_SYSTEM = \"\"\"You are checking ONLY for the 'Irrelevant' policy.\n",
        "\n",
        "Decision rule (mutually exclusive):\n",
        "- If this specific policy does NOT clearly apply, return APPROVE with category \"None\" and confidence 0.0.\n",
        "- Do not reject for other policies (e.g., Ads or Rant_No_Visit).\n",
        "\n",
        "Reject as Irrelevant when the text is off-topic and unrelated to THIS venue/service:\n",
        "- unrelated politics/news/crypto hype/chain messages/personal stories\n",
        "- generic advice not tied to this place (e.g., 'buy BTC now', 'vote X'), etc.\n",
        "- content about another business or location without discussing this one\n",
        "\n",
        "Return ONLY JSON with fields: label, category, rationale, confidence (0.0‚Äì1.0), flags.\n",
        "\"\"\"\n",
        "\n",
        "# ===== 3) RANTS WITHOUT VISIT =====\n",
        "RANT_NO_VISIT_SYSTEM = \"\"\"Reject generic rants or accusations clearly targeting THIS place but with no evidence of a visit.\n",
        "These rants are often:\n",
        "- Short and emotional (e.g., 'Terrible place', 'Worst ever', 'Overpriced scammers')\n",
        "- Broad accusations ('scam', 'rip-off', 'fraud')\n",
        "- Negative judgments about pricing, quality, or character of the venue\n",
        "Reject them even if the reviewer does not explicitly say 'this place/restaurant' ‚Äî assume negativity is directed at the business being reviewed.\n",
        "Approve only if the reviewer provides concrete evidence of a visit (date, food ordered, staff interaction).\n",
        "Output JSON only.\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(system_text: str, review_text: str, fewshots):\n",
        "    demo = \"\\\\n\\\\n\".join(\n",
        "        [f\"Review:\\\\n{r}\\\\nExpected JSON:\\\\n{j}\" for r,j in fewshots]\n",
        "    )\n",
        "    return f\"\"\"{system_text}\n",
        "\n",
        "{TEMPLATE_JSON}\n",
        "\n",
        "{demo}\n",
        "\n",
        "Now classify this review. Return ONLY JSON.\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\"\n",
        "'''\n",
        "\n",
        "with open('prompts/policy_prompts.py', 'w') as f:\n",
        "    f.write(prompts_code)\n",
        "\n",
        "print(\"‚úÖ Constants and prompts created matching production pipeline!\")\n",
        "\n",
        "# Test constants\n",
        "exec(constants_code)\n",
        "print(f\"Policy categories: {list(POLICY_CATEGORIES.values())}\")\n",
        "print(f\"Zero-shot model: {DEFAULT_MODELS['ZERO_SHOT']}\")\n",
        "print(f\"Default confidence threshold: {CONFIDENCE_THRESHOLDS['DEFAULT']}\")\n",
        "print(f\"Zero-shot labels configured: {len(ZERO_SHOT_LABELS)} categories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6015eaed",
      "metadata": {
        "id": "6015eaed"
      },
      "source": [
        "## 6. Gemini API Configuration and Pseudo-Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "33aa5b43",
      "metadata": {
        "id": "33aa5b43",
        "outputId": "123f27ea-92e7-4722-bd95-86aa2e039e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab secrets not found: Secret GEMINI_API_KEY does not exist.\n",
            "Please enter your Gemini API key manually:\n",
            "1) https://aistudio.google.com/app/apikey  2) Create key  3) Paste here\n",
            "Enter your Gemini API key: AIzaSyDgrW_D32Ywj-nvpNsymGcdKUXMRCmwA64\n",
            "‚úÖ Gemini API test successful!\n",
            "   Source: manual\n",
            "   Model: gemini-2.5-flash-lite\n",
            "\n",
            "Configuration Summary:\n",
            "   Gemini Available: ‚úÖ Yes\n",
            "   Pipeline Mode: Full\n",
            "üì¶ Loading cached pseudo-labels from data/pseudo-label/gemini_pseudo_labels.csv\n",
            "‚úÖ Using cached pseudo-labels (n=245)\n",
            "\n",
            "Training data preparation: ‚úÖ Complete\n",
            "Ready for HuggingFace model training: ‚úÖ Yes\n"
          ]
        }
      ],
      "source": [
        "# Gemini API Key Setup and Pseudo-Labeling Implementation (deduped + cache-aware)\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Config & cache paths ---\n",
        "PSEUDO_DIR = \"data/pseudo-label\"\n",
        "os.makedirs(PSEUDO_DIR, exist_ok=True)\n",
        "PSEUDO_PATH = os.path.join(PSEUDO_DIR, \"gemini_pseudo_labels.csv\")\n",
        "\n",
        "def load_cached_pseudo_labels():\n",
        "    if os.path.exists(PSEUDO_PATH):\n",
        "        print(f\"üì¶ Loading cached pseudo-labels from {PSEUDO_PATH}\")\n",
        "        return pd.read_csv(PSEUDO_PATH)\n",
        "    return None\n",
        "\n",
        "# --- API key (Colab secrets first, manual fallback) ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"‚úÖ Gemini API key loaded from Colab secrets\")\n",
        "    api_source = \"secrets\"\n",
        "except Exception as e:\n",
        "    print(f\"Colab secrets not found: {e}\")\n",
        "    print(\"Please enter your Gemini API key manually:\")\n",
        "    print(\"1) https://aistudio.google.com/app/apikey  2) Create key  3) Paste here\")\n",
        "    GEMINI_API_KEY = input(\"Enter your Gemini API key: \").strip()\n",
        "    api_source = \"manual\"\n",
        "\n",
        "# --- Configure & test Gemini ---\n",
        "if GEMINI_API_KEY:\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    try:\n",
        "        _test_model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "        _ = _test_model.generate_content(\"Test: Say 'API working'\")\n",
        "        print(\"‚úÖ Gemini API test successful!\")\n",
        "        print(f\"   Source: {api_source}\")\n",
        "        print(f\"   Model: gemini-2.5-flash-lite\")\n",
        "        gemini_available = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Gemini test failed: {e}\")\n",
        "        print(\"Check your API key and quota limits\")\n",
        "        gemini_available = False\n",
        "else:\n",
        "    print(\"No API key provided ‚Äî skipping Gemini pseudo-labeling.\")\n",
        "    gemini_available = False\n",
        "\n",
        "print(\"\\nConfiguration Summary:\")\n",
        "print(f\"   Gemini Available: {'‚úÖ Yes' if gemini_available else '‚ùå No'}\")\n",
        "print(f\"   Pipeline Mode: {'Full' if gemini_available else 'HuggingFace Only'}\")\n",
        "\n",
        "# --- Pseudo-label helpers ---\n",
        "def classify_with_gemini(text: str, model_name=\"gemini-2.0-flash-exp\"):\n",
        "    \"\"\"Call Gemini to classify a review into policy categories.\"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel(model_name)\n",
        "        prompt = f\"\"\"\n",
        "You are a Google review policy expert. Classify this review and provide a JSON response.\n",
        "\n",
        "Review: \"{text}\"\n",
        "\n",
        "Policy Categories:\n",
        "- No_Ads: Contains advertisements, promotional content, promo codes, contact information\n",
        "- Irrelevant: Off-topic content (politics, crypto, unrelated businesses)\n",
        "- Rant_No_Visit: Generic negative rants without evidence of visiting\n",
        "- None: Legitimate review about an actual visit/experience\n",
        "\n",
        "Respond with JSON only:\n",
        "{{\"label\": \"APPROVE\" or \"REJECT\", \"category\": \"policy_name\", \"confidence\": 0.0-1.0, \"rationale\": \"brief_explanation\"}}\n",
        "\"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "        resp = response.text.strip()\n",
        "\n",
        "        # unwrap code fences if present\n",
        "        if resp.startswith('```json'):\n",
        "            resp = resp.replace('```json', '').replace('```', '').strip()\n",
        "        elif resp.startswith('```'):\n",
        "            resp = resp.replace('```', '').strip()\n",
        "\n",
        "        # try strict JSON\n",
        "        try:\n",
        "            obj = json.loads(resp)\n",
        "            if all(k in obj for k in ('label','category','confidence')):\n",
        "                return {\n",
        "                    \"label\": obj['label'],\n",
        "                    \"category\": obj['category'],\n",
        "                    \"confidence\": float(obj['confidence']),\n",
        "                    \"rationale\": obj.get('rationale','Gemini classification')\n",
        "                }\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        # fallback: detect REJECT/APPROVE heuristically\n",
        "        if 'REJECT' in resp.upper():\n",
        "            if 'No_Ads' in resp or 'advert' in resp.lower():\n",
        "                cat = 'No_Ads'\n",
        "            elif 'Irrelevant' in resp or 'off-topic' in resp.lower():\n",
        "                cat = 'Irrelevant'\n",
        "            elif 'Rant_No_Visit' in resp or 'rant' in resp.lower():\n",
        "                cat = 'Rant_No_Visit'\n",
        "            else:\n",
        "                cat = 'No_Ads'\n",
        "            return {\"label\":\"REJECT\",\"category\":cat,\"confidence\":0.7,\"rationale\":\"Parsed from text\"}\n",
        "        else:\n",
        "            return {\"label\":\"APPROVE\",\"category\":\"None\",\"confidence\":0.7,\"rationale\":\"Parsed from text\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gemini API error: {e}\")\n",
        "        return {\"label\":\"APPROVE\",\"category\":\"None\",\"confidence\":0.0,\"rationale\":f\"API error: {e}\"}\n",
        "\n",
        "def generate_pseudo_labels_with_gemini(unlabeled_df, confidence_threshold=0.8, max_labels=50):\n",
        "    \"\"\"Generate high-confidence pseudo-labels and save to cache.\"\"\"\n",
        "    if not gemini_available:\n",
        "        print(\"‚ùå Gemini not available for pseudo-labeling\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(\"Generating pseudo-labels for training data‚Ä¶\")\n",
        "    print(f\"  Confidence threshold: {confidence_threshold}\")\n",
        "    print(f\"  Max labels: {max_labels}\")\n",
        "\n",
        "    rows = []\n",
        "    processed = 0\n",
        "    total = min(len(unlabeled_df), max_labels)\n",
        "    print(f\"Processing {total} reviews‚Ä¶\")\n",
        "\n",
        "    for _, row in tqdm(unlabeled_df.head(total).iterrows(), total=total, desc=\"Generating pseudo-labels\"):\n",
        "        txt = str(row['text'])\n",
        "        res = classify_with_gemini(txt)\n",
        "        if res['confidence'] >= confidence_threshold:\n",
        "            rows.append({\n",
        "                'id': row.get('id', processed + 100),\n",
        "                'text': txt,\n",
        "                'pred_label': res['label'],\n",
        "                'pred_category': res['category'],\n",
        "                'confidence': res['confidence'],\n",
        "                'rationale': res['rationale'],\n",
        "                'source': 'gemini_pseudo'\n",
        "            })\n",
        "        processed += 1\n",
        "        time.sleep(0.1)  # rate limit\n",
        "\n",
        "    out = pd.DataFrame(rows)\n",
        "    if len(out) > 0:\n",
        "        out.to_csv(PSEUDO_PATH, index=False)\n",
        "        print(f\"‚úÖ Generated {len(out)} pseudo-labels\")\n",
        "        print(f\"üíæ Cached to {PSEUDO_PATH}\")\n",
        "        try:\n",
        "            print(f\"Label distribution: {out['pred_label'].value_counts().to_dict()}\")\n",
        "            print(f\"Category distribution: {out['pred_category'].value_counts().to_dict()}\")\n",
        "            print(f\"Average confidence: {out['confidence'].mean():.3f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        print(\"‚ùå No high-confidence pseudo-labels generated\")\n",
        "    return out\n",
        "\n",
        "# --- Use cache if present; otherwise, generate ---\n",
        "cached = load_cached_pseudo_labels()\n",
        "if cached is not None and len(cached) > 0:\n",
        "    pseudo_labels_df = cached\n",
        "    print(f\"‚úÖ Using cached pseudo-labels (n={len(pseudo_labels_df)})\")\n",
        "else:\n",
        "    if gemini_available:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"GENERATING TRAINING DATA WITH PSEUDO-LABELS\")\n",
        "        print(\"=\"*60)\n",
        "        unlabeled_df = pd.DataFrame({\n",
        "            'id': list(range(101, 121)),\n",
        "            'text': [\n",
        "                \"Amazing food and service, definitely coming back!\",\n",
        "                \"Visit our website for exclusive deals and discounts - use code SAVE20\",\n",
        "                \"The worst experience ever, everything was terrible, total scam\",\n",
        "                \"Staff was friendly, food was fresh and tasty, good value for money\",\n",
        "                \"This place is overpriced, never going back, waste of money\",\n",
        "                \"Great atmosphere, perfect for family dinner, ordered the set meal and dessert\",\n",
        "                \"Follow my Instagram @foodie123 for more reviews and promos\",\n",
        "                \"Bitcoin is going to the moon! Buy now before it's too late!\",\n",
        "                \"Had the chicken rice here yesterday, portion was generous and taste was authentic\",\n",
        "                \"DM me for discount codes! Also selling crypto courses online\",\n",
        "                \"Terrible service, rude staff, food was cold when it arrived\",\n",
        "                \"The laksa here reminds me of my grandmother's cooking, very nostalgic\",\n",
        "                \"Check out my YouTube channel for food reviews and crypto tips\",\n",
        "                \"Went here for lunch with colleagues, everyone enjoyed their meals\",\n",
        "                \"Overpriced tourist trap, locals know better places nearby\",\n",
        "                \"Made reservation for 6pm, got seated immediately, excellent service throughout\",\n",
        "                \"Politics in this country is corrupt, restaurants like this are part of the problem\",\n",
        "                \"Their signature dish was perfectly seasoned, will definitely recommend to friends\",\n",
        "                \"Worst restaurant in Singapore, total ripoff, avoid at all costs\",\n",
        "                \"Celebrated my birthday here last week, staff even brought out a cake\"\n",
        "            ]\n",
        "        })\n",
        "        pseudo_labels_df = generate_pseudo_labels_with_gemini(\n",
        "            unlabeled_df, confidence_threshold=0.7, max_labels=20\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"SKIPPING PSEUDO-LABELING (Gemini not available)\")\n",
        "        print(\"=\"*60)\n",
        "        pseudo_labels_df = pd.DataFrame()\n",
        "\n",
        "print(f\"\\nTraining data preparation: {'‚úÖ Complete' if len(pseudo_labels_df) > 0 else '‚ùå Skipped'}\")\n",
        "print(f\"Ready for HuggingFace model training: {'‚úÖ Yes' if len(pseudo_labels_df) > 0 else '‚ö†Ô∏è Pre-trained only'}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f255cad8",
      "metadata": {
        "id": "f255cad8"
      },
      "source": [
        "## 7. HuggingFace Model Training with Pseudo-Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2235ac4d",
      "metadata": {
        "id": "2235ac4d",
        "collapsed": true,
        "outputId": "d160d605-163a-4bd5-8968-bc93b4ce8697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mHUGGINGFACE MODEL TRAINING WITH PSEUDO-LABELS (no sentiment, fused policy logic)\n",
            "============================================================\n",
            "üì¶ Loading cached pseudo-labels from data/pseudo-label/gemini_pseudo_labels.csv\n",
            "‚úÖ Loaded 245 pseudo-labeled rows\n",
            "Mode: fine-tuning  |  Pseudo-label rows: 245\n",
            "\n",
            "üéØ FINE-TUNING MODE\n",
            "Training rows kept: 245 (REJECT=8, APPROVE=237)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3702617290.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [93/93 25:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.350700</td>\n",
              "      <td>0.199891</td>\n",
              "      <td>0.967347</td>\n",
              "      <td>0.951291</td>\n",
              "      <td>0.935760</td>\n",
              "      <td>0.967347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.091300</td>\n",
              "      <td>0.122037</td>\n",
              "      <td>0.967347</td>\n",
              "      <td>0.951291</td>\n",
              "      <td>0.935760</td>\n",
              "      <td>0.967347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.149900</td>\n",
              "      <td>0.077904</td>\n",
              "      <td>0.967347</td>\n",
              "      <td>0.951291</td>\n",
              "      <td>0.935760</td>\n",
              "      <td>0.967347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model saved to: ./models/fine-tuned/review-policy-classifier\n",
            "\n",
            "============================================================\n",
            "TESTING MODELS\n",
            "============================================================\n",
            "Testing with: Pseudo-labeled data (n=245)\n",
            "\n",
            "Running inference‚Ä¶ (fine-tuning)  samples=245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Processing reviews: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245/245 [01:57<00:00,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Inference saved ‚Üí results/predictions/predictions_fine-tuning.csv  rows=245\n",
            "\n",
            "üìä INFERENCE RESULTS\n",
            "==================================================\n",
            " id                                                  text pred_label  pred_category  confidence  model_type toxicity_label  toxicity_score\n",
            "  1 Call them now, book now! The team was amazing, fun...    APPROVE Custom_Trained      0.8799 fine-tuning          toxic          0.0008\n",
            "  2 This team couldn't have been more perfect. I did a...    APPROVE Custom_Trained      0.8878 fine-tuning          toxic          0.0012\n",
            "  3 Dive Oahu runs an awesome program!!!!! The instruc...    APPROVE Custom_Trained      0.8756 fine-tuning          toxic          0.0016\n",
            "  4 We had a great time snorkeling on our honeymoon. T...    APPROVE Custom_Trained      0.8771 fine-tuning          toxic          0.0010\n",
            "  5 Early morning dive on the Sea Tiger and Pipe, when...    APPROVE Custom_Trained      0.8816 fine-tuning          toxic          0.0007\n",
            "  6 Great experience! Captain and dive master were kno...    APPROVE Custom_Trained      0.8788 fine-tuning          toxic          0.0006\n",
            "  7 We had a great time diving the Sea Tiger ship and ...    APPROVE Custom_Trained      0.8830 fine-tuning          toxic          0.0005\n",
            "  8 Captain and crew were very friendly and welcoming....    APPROVE Custom_Trained      0.8874 fine-tuning          toxic          0.0006\n",
            "  9 We had a great experience with Dive Oahu. It was v...    APPROVE Custom_Trained      0.8828 fine-tuning          toxic          0.0005\n",
            " 10 Jake is just the absolute cutest Divemaster I have...    APPROVE Custom_Trained      0.8668 fine-tuning          toxic          0.0055\n",
            "\n",
            "üìà RESULTS SUMMARY\n",
            "pred_label\n",
            "APPROVE    245\n",
            "Name: count, dtype: int64\n",
            "Average Confidence (fine-tuned only): 0.864\n",
            "\n",
            "‚úÖ MODEL TRAINING AND TESTING COMPLETE\n",
            "Training Mode: FINE-TUNING\n",
            "Ready for Model Persistence: ‚úÖ Yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers -q\n",
        "!pip install -qU \"transformers>=4.45.0\" \"accelerate>=0.34.0\" \"huggingface_hub>=0.23.0\"\n",
        "\n",
        "# 7. HuggingFace Model Training with Pseudo-Labels\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding, pipeline as hf_pipeline,\n",
        "    __version__ as HF_VER,   # <-- for version-aware args\n",
        ")\n",
        "from packaging.version import parse                # <-- for version-aware args\n",
        "from datasets import Dataset\n",
        "import torch, numpy as np, pandas as pd, re, os\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"HUGGINGFACE MODEL TRAINING WITH PSEUDO-LABELS (no sentiment, fused policy logic)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ---- import your constants (as provided)\n",
        "from src.core.constants import (\n",
        "    DEFAULT_MODELS, ZERO_SHOT_LABELS, ZERO_SHOT_TO_POLICY,\n",
        "    POLICY_CATEGORIES, LABELS, CONFIDENCE_THRESHOLDS\n",
        ")\n",
        "\n",
        "import os, pandas as pd\n",
        "\n",
        "\n",
        "PSEUDO_DIR = \"data/pseudo-label\"\n",
        "PSEUDO_PATH = os.path.join(PSEUDO_DIR, \"gemini_pseudo_labels.csv\")\n",
        "os.makedirs(PSEUDO_DIR, exist_ok=True)\n",
        "\n",
        "def load_cached_pseudo_labels(path=PSEUDO_PATH):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"üì¶ Loading cached pseudo-labels from {path}\")\n",
        "        df_cached = pd.read_csv(path)\n",
        "        need = {\"id\",\"text\",\"pred_label\",\"pred_category\",\"confidence\"}\n",
        "        missing = need - set(df_cached.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Cached pseudo-label file missing columns: {missing}\")\n",
        "        print(f\"‚úÖ Loaded {len(df_cached)} pseudo-labeled rows\")\n",
        "        return df_cached\n",
        "    print(f\"‚ÑπÔ∏è No cache at {path}.\")\n",
        "    return pd.DataFrame(columns=[\"id\",\"text\",\"pred_label\",\"pred_category\",\"confidence\"])\n",
        "\n",
        "# make pseudo_labels_df available for training-mode decision\n",
        "pseudo_labels_df = load_cached_pseudo_labels()\n",
        "\n",
        "# decide mode only AFTER loading the cache\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "has_training_data = len(pseudo_labels_df) > 0\n",
        "training_mode = \"fine-tuning\" if has_training_data else \"pre-trained\"\n",
        "print(f\"Mode: {training_mode}  |  Pseudo-label rows: {len(pseudo_labels_df)}\")\n",
        "\n",
        "# ---- models (NO sentiment)\n",
        "BASE_MODEL      = \"distilbert-base-uncased\"\n",
        "TOXIC_MODEL     = DEFAULT_MODELS['TOXIC']\n",
        "ZERO_SHOT_MODEL = DEFAULT_MODELS['ZERO_SHOT']\n",
        "\n",
        "# ========= new policy helpers (ad evidence + toxicity gate) =========\n",
        "AD_PATTERNS = [\n",
        "    r\"https?://\", r\"\\bwww\\.\", r\"\\.[a-z]{2,6}\\b\",\n",
        "    r\"\\b(?:\\+?\\d[\\s\\-()]*){7,}\\b\",\n",
        "    r\"\\bpromo(?:\\s*code)?\\b\", r\"\\bdiscount\\b\", r\"\\bcoupon\\b\",\n",
        "    r\"\\breferral\\b\", r\"\\buse\\s*code\\b\", r\"\\benter\\s*code\\b\",\n",
        "    r\"\\bwhatsapp\\b\",\n",
        "    r\"\\bdm\\s+(?:me|us)\\b\",\n",
        "    r\"\\bcontact\\s+(?:us|me)\\b\", r\"\\bcall\\s+(?:us|me)\\b\",\n",
        "]\n",
        "AD_REGEX = re.compile(\"|\".join(AD_PATTERNS), flags=re.IGNORECASE)\n",
        "TOX_TO_RANT = {\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\"}\n",
        "TOX_TO_IRRELEVANT = {\"identity_hate\"}\n",
        "\n",
        "def ad_evidence(text: str):\n",
        "    t = text or \"\"\n",
        "    m = AD_REGEX.search(t)\n",
        "    return bool(m), (m.group(0) if m else \"\")\n",
        "\n",
        "def tox_top_label(tox_output):\n",
        "    \"\"\"\n",
        "    Normalize HF toxicity pipeline outputs into (label, score).\n",
        "    Handles dict, list[dict], list[list[dict]].\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(tox_output, dict):\n",
        "            candidates = [tox_output]\n",
        "        elif isinstance(tox_output, list):\n",
        "            if len(tox_output) and isinstance(tox_output[0], dict):\n",
        "                candidates = tox_output\n",
        "            elif len(tox_output) and isinstance(tox_output[0], list):\n",
        "                candidates = tox_output[0]\n",
        "            else:\n",
        "                candidates = []\n",
        "        else:\n",
        "            candidates = []\n",
        "        if not candidates:\n",
        "            return \"NONE\", 0.0\n",
        "        best = max(candidates, key=lambda d: float(d.get(\"score\", 0.0)))\n",
        "        return best.get(\"label\", \"NONE\"), float(best.get(\"score\", 0.0))\n",
        "    except Exception:\n",
        "        return \"NONE\", 0.0\n",
        "\n",
        "\n",
        "def policy_zero_shot_fused(zshot, toxic, text: str,\n",
        "                           tau_irrelevant=0.55, tau_rant=0.55,\n",
        "                           tau_ads=0.70, tox_tau=0.50, ads_margin=0.10):\n",
        "    \"\"\"\n",
        "    1) Toxicity gate:\n",
        "         - identity_hate -> Irrelevant\n",
        "         - toxic/insult/obscene/threat -> Rant_No_Visit\n",
        "    2) Else if Irrelevant/Rant over thresholds -> that category\n",
        "    3) Else if Ads high + ad evidence + margin -> No_Ads\n",
        "    4) Else -> None\n",
        "    Returns: (pred_label 'APPROVE'/'REJECT', pred_category)\n",
        "    \"\"\"\n",
        "    # zero-shot on your label set\n",
        "    zs_res = zshot(text, candidate_labels=ZERO_SHOT_LABELS,\n",
        "                   hypothesis_template=\"This review is {}.\", multi_label=True)\n",
        "    zs = {lab: float(scr) for lab, scr in zip(zs_res[\"labels\"], zs_res[\"scores\"])}\n",
        "    ads  = zs.get(ZERO_SHOT_LABELS[0], 0.0)\n",
        "    irr  = zs.get(ZERO_SHOT_LABELS[1], 0.0)\n",
        "    rant = zs.get(ZERO_SHOT_LABELS[2], 0.0)\n",
        "    none = zs.get(ZERO_SHOT_LABELS[3], 0.0)\n",
        "\n",
        "    # toxicity top label\n",
        "    tox_label, tox_score = tox_top_label(toxic(text))\n",
        "\n",
        "\n",
        "    if tox_label and tox_score >= tox_tau:\n",
        "        if tox_label in TOX_TO_RANT:\n",
        "            return LABELS['REJECT'], POLICY_CATEGORIES['RANT_NO_VISIT']\n",
        "        if tox_label in TOX_TO_IRRELEVANT:\n",
        "            return LABELS['REJECT'], POLICY_CATEGORIES['IRRELEVANT']\n",
        "\n",
        "    if max(irr, rant) >= min(tau_irrelevant, tau_rant):\n",
        "        return LABELS['REJECT'], (POLICY_CATEGORIES['IRRELEVANT'] if irr >= rant else POLICY_CATEGORIES['RANT_NO_VISIT'])\n",
        "\n",
        "    has_ads, _ = ad_evidence(text)\n",
        "    if has_ads and (ads >= tau_ads) and (ads >= max(irr, rant) + ads_margin):\n",
        "        return LABELS['REJECT'], POLICY_CATEGORIES['NO_ADS']\n",
        "\n",
        "    return LABELS['APPROVE'], POLICY_CATEGORIES['NONE']\n",
        "\n",
        "# ========= metrics & training (binary, same as before) =========\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "def prepare_training_data(pseudo_df):\n",
        "    df = pseudo_df.copy()\n",
        "\n",
        "    # Ensure text is a string, strip only truly empty rows\n",
        "    df[\"text\"] = df[\"text\"].astype(str)\n",
        "    df = df[df[\"text\"].str.strip().ne(\"\")]\n",
        "\n",
        "    # Normalize labels: keep APPROVE/REJECT, infer if missing\n",
        "    def _norm_label(row):\n",
        "        lbl = str(row.get(\"pred_label\", \"\")).upper()\n",
        "        if lbl in (\"APPROVE\", \"REJECT\"):\n",
        "            return lbl\n",
        "        # infer from category: if category is None ‚Üí APPROVE, else ‚Üí REJECT\n",
        "        cat = str(row.get(\"pred_category\", \"\")).strip().lower()\n",
        "        if cat in (\"none\", \"\", \"nan\"):\n",
        "            return \"APPROVE\"\n",
        "        return \"REJECT\"\n",
        "\n",
        "    df[\"pred_label\"] = df.apply(_norm_label, axis=1)\n",
        "\n",
        "    # If APPROVE, leave pred_category untouched (even if \"None\")\n",
        "    # If REJECT but category is missing, assign a generic fallback\n",
        "    df.loc[(df[\"pred_label\"] == \"REJECT\") &\n",
        "           (df[\"pred_category\"].isna() | df[\"pred_category\"].str.lower().isin([\"\", \"nan\"])),\n",
        "           \"pred_category\"] = \"Unspecified_Spam\"\n",
        "\n",
        "    # Build training lists\n",
        "    train_texts  = df[\"text\"].tolist()\n",
        "    train_labels = (df[\"pred_label\"].str.upper() == \"REJECT\").astype(int).tolist()\n",
        "\n",
        "    print(f\"Training rows kept: {len(df)} \"\n",
        "          f\"(REJECT={sum(train_labels)}, APPROVE={len(df)-sum(train_labels)})\")\n",
        "\n",
        "    return train_texts, train_labels\n",
        "\n",
        "\n",
        "def train_custom_classification_model(train_texts, train_labels, model_name=\"review-policy-classifier\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=2)\n",
        "    enc = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "    train_ds = Dataset.from_dict({'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask'], 'labels': train_labels})\n",
        "\n",
        "    # ---- version-aware TrainingArguments (v4 vs v5)\n",
        "    base_kwargs = dict(\n",
        "        output_dir=f'./models/fine-tuned/{model_name}',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'./logs/{model_name}',\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Transformers v5+\n",
        "        args = TrainingArguments(**base_kwargs, eval_strategy=\"epoch\")\n",
        "    except TypeError:\n",
        "        # Transformers v4.x\n",
        "        args = TrainingArguments(**base_kwargs, evaluation_strategy=\"epoch\")\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args,\n",
        "        train_dataset=train_ds, eval_dataset=train_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    save_dir = f'./models/fine-tuned/{model_name}'\n",
        "    trainer.save_model(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"‚úÖ Model saved to: {save_dir}\")\n",
        "    return model, tokenizer, save_dir\n",
        "\n",
        "def load_aux_pipelines(device=None):\n",
        "    toxic = hf_pipeline(\"text-classification\", model=TOXIC_MODEL, top_k=None, device=device)\n",
        "    zshot = hf_pipeline(\"zero-shot-classification\", model=ZERO_SHOT_MODEL, device=device)\n",
        "    return toxic, zshot\n",
        "\n",
        "def predict_with_custom_model(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        conf = float(probs.max())\n",
        "        pred = int(probs.argmax())\n",
        "    return (\"REJECT\" if pred == 1 else \"APPROVE\"), conf\n",
        "\n",
        "def run_inference_pipeline(df, tau=CONFIDENCE_THRESHOLDS['DEFAULT']):\n",
        "    print(f\"\\nRunning inference‚Ä¶ ({training_mode})  samples={len(df)}\")\n",
        "    toxic_pipeline, zshot_pipeline = load_aux_pipelines(device)\n",
        "    results = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing reviews\"):\n",
        "        txt = str(row['text'])\n",
        "\n",
        "        # ---- robust access to optional custom model\n",
        "        cc = TRAINED_MODELS.get('custom_classifier')\n",
        "        if cc is not None:\n",
        "            pred_label, confidence = predict_with_custom_model(txt, cc['model'], cc['tokenizer'])\n",
        "            pred_category = \"Custom_Trained\"\n",
        "        else:\n",
        "            pred_label, pred_category = policy_zero_shot_fused(\n",
        "                zshot=zshot_pipeline, toxic=toxic_pipeline, text=txt,\n",
        "                tau_irrelevant=0.55, tau_rant=0.55, tau_ads=0.70, tox_tau=0.50, ads_margin=0.10\n",
        "            )\n",
        "            confidence = None  # rule-based fusion\n",
        "\n",
        "        # Optional: record toxicity top label/score for debugging\n",
        "        try:\n",
        "            tox_label, tox_score = tox_top_label(toxic_pipeline(txt))\n",
        "        except Exception:\n",
        "            tox_label, tox_score = \"NONE\", 0.0\n",
        "\n",
        "        results.append({\n",
        "            \"id\": row.get('id', len(results)+1),\n",
        "            \"text\": txt,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"pred_category\": pred_category,\n",
        "            \"confidence\": (round(float(confidence), 4) if confidence is not None else None),\n",
        "            \"toxicity_label\": tox_label,\n",
        "            \"toxicity_score\": round(float(tox_score), 4),\n",
        "            \"model_type\": training_mode\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    os.makedirs(\"results/predictions\", exist_ok=True)\n",
        "    out_path = f'results/predictions/predictions_{training_mode}.csv'\n",
        "    results_df.to_csv(out_path, index=False)\n",
        "    print(f\"‚úÖ Inference saved ‚Üí {out_path}  rows={len(results_df)}\")\n",
        "    return results_df\n",
        "\n",
        "# ========= Train (if pseudo-labels) or use pre-trained =========\n",
        "TRAINED_MODELS = {\"custom_classifier\": None}   # <-- init key to avoid KeyError\n",
        "if training_mode == \"fine-tuning\":\n",
        "    print(\"\\nüéØ FINE-TUNING MODE\")\n",
        "    tr_texts, tr_labels = prepare_training_data(pseudo_labels_df)\n",
        "    model, tok, model_path = train_custom_classification_model(tr_texts, tr_labels, \"review-policy-classifier\")\n",
        "    TRAINED_MODELS['custom_classifier'] = {'model_path': model_path, 'model': model, 'tokenizer': tok}\n",
        "else:\n",
        "    print(\"\\nüîÑ PRE-TRAINED MODE (no sentiment)\")\n",
        "    TRAINED_MODELS['custom_classifier'] = None\n",
        "\n",
        "# ========= Test on available data (same style) =========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(pseudo_labels_df) > 0:\n",
        "    test_data = pseudo_labels_df[['id','text']]\n",
        "    src = f\"Pseudo-labeled data (n={len(test_data)})\"\n",
        "else:\n",
        "    # fallback minimal sample\n",
        "    test_data = pd.DataFrame({'id':[1,2], 'text':[\n",
        "        \"Great food and service!\",\n",
        "        \"Use my promo code SAVE20 for discounts!\"\n",
        "    ]})\n",
        "    src = \"Minimal test data\"\n",
        "\n",
        "print(f\"Testing with: {src}\")\n",
        "hf_results = run_inference_pipeline(test_data, tau=CONFIDENCE_THRESHOLDS['DEFAULT'])\n",
        "\n",
        "# Pretty print head\n",
        "print(\"\\nüìä INFERENCE RESULTS\")\n",
        "print(\"=\"*50)\n",
        "disp_cols = ['id','text','pred_label','pred_category','confidence','model_type','toxicity_label','toxicity_score']\n",
        "dd = hf_results.copy()\n",
        "dd['text'] = dd['text'].apply(lambda x: x[:50]+\"...\" if isinstance(x,str) and len(x)>50 else x)\n",
        "print(dd[disp_cols].head(10).to_string(index=False))\n",
        "\n",
        "# Summary\n",
        "print(\"\\nüìà RESULTS SUMMARY\")\n",
        "print(hf_results['pred_label'].value_counts())\n",
        "print(f\"Average Confidence (fine-tuned only): {hf_results['confidence'].dropna().mean() if 'confidence' in hf_results else float('nan'):.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ MODEL TRAINING AND TESTING COMPLETE\")\n",
        "print(f\"Training Mode: {training_mode.upper()}\")\n",
        "print(\"Ready for Model Persistence: ‚úÖ Yes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaKbUzYYqHOC"
      },
      "id": "UaKbUzYYqHOC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Unified Spam Detection Model\n"
      ],
      "metadata": {
        "id": "lUbUl5xwsZTZ"
      },
      "id": "lUbUl5xwsZTZ"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Unified spam detection system combining machine learning with pattern analysis.\n",
        "Assumes data has already passed through toxicity/hate speech filtering pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "!pip install textstat\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class DetectionResult:\n",
        "    \"\"\"Container for detection results.\"\"\"\n",
        "    label: str  # 'APPROVE' or 'REJECT'\n",
        "    confidence: float  # 0.0 to 1.0\n",
        "    category: str  # violation category or 'None'\n",
        "    features: Dict  # method-specific features\n",
        "    confidence_interval: Tuple[float, float]\n",
        "\n",
        "class PatternFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract pattern-based features for ML model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Extract pattern features from texts.\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for text in X:\n",
        "            text_features = self._extract_pattern_features(text)\n",
        "\n",
        "            # Convert to feature vector\n",
        "            feature_vector = [\n",
        "                text_features['repetition_ratio'],\n",
        "                text_features['word_diversity_ratio'],\n",
        "                text_features['repeated_ngrams_ratio'],\n",
        "                text_features['caps_ratio'],\n",
        "                text_features['punct_ratio'],\n",
        "                text_features['readability_score'] / 100.0,  # Normalize\n",
        "                text_features['avg_sentence_length'] / 50.0,  # Normalize\n",
        "                text_features['word_count'] / 100.0,  # Normalize\n",
        "                text_features['template_score'],\n",
        "                text_features['phrase_repetition_score'],\n",
        "                text_features['local_repetition_score']\n",
        "            ]\n",
        "            features.append(feature_vector)\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def _extract_pattern_features(self, text: str) -> Dict:\n",
        "        \"\"\"Extract comprehensive pattern features from text.\"\"\"\n",
        "        words = text.split()\n",
        "        word_count = len(words)\n",
        "        char_count = len(text)\n",
        "\n",
        "        # Basic repetition analysis\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            word_lower = word.lower()\n",
        "            word_freq[word_lower] = word_freq.get(word_lower, 0) + 1\n",
        "\n",
        "        repetition_ratio = 0.0\n",
        "        if word_count > 0:\n",
        "            max_repetition = max(word_freq.values()) if word_freq else 1\n",
        "            repetition_ratio = max_repetition / word_count\n",
        "\n",
        "        # Word diversity (unique words / total words)\n",
        "        unique_words = len(set(word.lower() for word in words))\n",
        "        word_diversity_ratio = unique_words / max(word_count, 1)\n",
        "\n",
        "        # N-gram repetition detection (for \"food is good food is great food is nice\")\n",
        "        repeated_ngrams_ratio = self._detect_repeated_ngrams(words)\n",
        "\n",
        "        # Phrase repetition score (detects patterns like \"food is X\" repeating)\n",
        "        phrase_repetition_score = self._detect_phrase_patterns(words)\n",
        "\n",
        "        # Local repetition score (detects repetition in specific text segments)\n",
        "        local_repetition_score = self._detect_local_repetition(words)\n",
        "\n",
        "        # Template detection (common patterns)\n",
        "        template_indicators = [\n",
        "            r'\\b(excellent|amazing|great|good|bad|terrible)\\b.*\\b(food|service|place|restaurant)\\b',\n",
        "            r'\\b(recommend|suggest|try|visit)\\b.*\\b(place|restaurant|here)\\b',\n",
        "            r'\\b(will|would).*\\b(come|go|visit).*\\b(again|back)\\b'\n",
        "        ]\n",
        "\n",
        "        template_matches = sum(1 for pattern in template_indicators\n",
        "                             if re.search(pattern, text, re.IGNORECASE))\n",
        "        template_score = template_matches / len(template_indicators)\n",
        "\n",
        "        # Simple readability proxy (no textstat installed)\n",
        "        words = len(text.split())\n",
        "        sentences = len(re.split(r'[.!?]+', text))\n",
        "        if sentences > 0 and words > 0:\n",
        "            avg_words_per_sentence = words / sentences\n",
        "            readability_score = max(0, min(100, 120 - avg_words_per_sentence * 2))\n",
        "        else:\n",
        "            readability_score = 50.0\n",
        "\n",
        "\n",
        "        # Character-level features\n",
        "        caps_count = sum(1 for c in text if c.isupper())\n",
        "        punct_count = sum(1 for c in text if c in '!?.,;:')\n",
        "\n",
        "        caps_ratio = caps_count / max(char_count, 1)\n",
        "        punct_ratio = punct_count / max(char_count, 1)\n",
        "\n",
        "        # Sentence structure\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        sentence_count = len([s for s in sentences if s.strip()])\n",
        "        avg_sentence_length = word_count / max(sentence_count, 1)\n",
        "\n",
        "        return {\n",
        "            'word_count': word_count,\n",
        "            'char_count': char_count,\n",
        "            'repetition_ratio': repetition_ratio,\n",
        "            'word_diversity_ratio': word_diversity_ratio,\n",
        "            'repeated_ngrams_ratio': repeated_ngrams_ratio,\n",
        "            'phrase_repetition_score': phrase_repetition_score,\n",
        "            'local_repetition_score': local_repetition_score,\n",
        "            'template_score': template_score,\n",
        "            'readability_score': readability_score,\n",
        "            'caps_ratio': caps_ratio,\n",
        "            'punct_ratio': punct_ratio,\n",
        "            'sentence_count': sentence_count,\n",
        "            'avg_sentence_length': avg_sentence_length\n",
        "        }\n",
        "\n",
        "    def _detect_repeated_ngrams(self, words: List[str], n: int = 2) -> float:\n",
        "        \"\"\"Detect repeated n-grams that indicate spam patterns.\"\"\"\n",
        "        if len(words) < n * 2:  # Need at least 2 n-grams to compare\n",
        "            return 0.0\n",
        "\n",
        "        # Generate n-grams\n",
        "        ngrams = []\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngram = ' '.join(words[i:i+n]).lower()\n",
        "            ngrams.append(ngram)\n",
        "\n",
        "        if not ngrams:\n",
        "            return 0.0\n",
        "\n",
        "        # Count n-gram frequencies\n",
        "        ngram_freq = {}\n",
        "        for ngram in ngrams:\n",
        "            ngram_freq[ngram] = ngram_freq.get(ngram, 0) + 1\n",
        "\n",
        "        # Calculate repetition score\n",
        "        total_ngrams = len(ngrams)\n",
        "        repeated_ngrams = sum(1 for freq in ngram_freq.values() if freq > 1)\n",
        "\n",
        "        return repeated_ngrams / total_ngrams if total_ngrams > 0 else 0.0\n",
        "\n",
        "    def _detect_phrase_patterns(self, words: List[str]) -> float:\n",
        "        \"\"\"Detect repeating phrase patterns like 'food is X food is Y food is Z'.\"\"\"\n",
        "        if len(words) < 6:  # Need enough words for pattern detection\n",
        "            return 0.0\n",
        "\n",
        "        # Look for patterns where same 2-word prefix repeats\n",
        "        pattern_scores = []\n",
        "\n",
        "        # Check 2-word patterns\n",
        "        for i in range(len(words) - 3):\n",
        "            prefix = f\"{words[i]} {words[i+1]}\".lower()\n",
        "\n",
        "            # Count how many times this prefix appears\n",
        "            prefix_count = 0\n",
        "            for j in range(i, len(words) - 1):\n",
        "                if j + 1 < len(words):\n",
        "                    candidate = f\"{words[j]} {words[j+1]}\".lower()\n",
        "                    if candidate == prefix:\n",
        "                        prefix_count += 1\n",
        "\n",
        "            if prefix_count > 1:\n",
        "                # This prefix repeats - calculate pattern strength\n",
        "                pattern_strength = prefix_count / (len(words) / 3)  # Normalize by text length\n",
        "                pattern_scores.append(min(pattern_strength, 1.0))\n",
        "\n",
        "        # Also check for local repetition at the end of text (common spam pattern)\n",
        "        local_repetition_score = self._detect_local_repetition(words)\n",
        "\n",
        "        return max(max(pattern_scores) if pattern_scores else 0.0, local_repetition_score)\n",
        "\n",
        "    def _detect_local_repetition(self, words: List[str]) -> float:\n",
        "        \"\"\"Detect repetitive patterns in specific sections of text (e.g., end of text).\"\"\"\n",
        "        if len(words) < 6:\n",
        "            return 0.0\n",
        "\n",
        "        max_score = 0.0\n",
        "\n",
        "        # Check multiple segments: end of text and any suspicious consecutive patterns\n",
        "        segments_to_check = []\n",
        "\n",
        "        # 1. End segment (common spam pattern)\n",
        "        end_segment_size = min(10, max(6, len(words) // 3))\n",
        "        end_words = words[-end_segment_size:]\n",
        "        segments_to_check.append(('end', end_words))\n",
        "\n",
        "        # 2. Look for any consecutive repeated phrases throughout the text\n",
        "        for start_pos in range(len(words) - 6):  # Sliding window\n",
        "            window_size = min(8, len(words) - start_pos)\n",
        "            window_words = words[start_pos:start_pos + window_size]\n",
        "            segments_to_check.append(('window', window_words))\n",
        "\n",
        "        for segment_type, segment_words in segments_to_check:\n",
        "            # Look for exact phrase repetition in this segment\n",
        "            for phrase_len in [2, 3, 4]:  # Check 2-word, 3-word, 4-word phrases\n",
        "                if len(segment_words) < phrase_len * 2:  # Need at least 2 instances\n",
        "                    continue\n",
        "\n",
        "                for i in range(len(segment_words) - phrase_len + 1):\n",
        "                    phrase = ' '.join(segment_words[i:i+phrase_len]).lower()\n",
        "\n",
        "                    # Look for consecutive repetition (more suspicious than scattered)\n",
        "                    consecutive_count = 1\n",
        "                    next_pos = i + phrase_len\n",
        "\n",
        "                    while next_pos + phrase_len <= len(segment_words):\n",
        "                        next_phrase = ' '.join(segment_words[next_pos:next_pos+phrase_len]).lower()\n",
        "                        if next_phrase == phrase:\n",
        "                            consecutive_count += 1\n",
        "                            next_pos += phrase_len\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    if consecutive_count >= 2:  # Found consecutive repeated phrase\n",
        "                        # Higher score for consecutive repetition\n",
        "                        if segment_type == 'end':\n",
        "                            # End segment repetition is more suspicious\n",
        "                            local_score = (consecutive_count * phrase_len * 1.5) / len(segment_words)\n",
        "                        else:\n",
        "                            # General repetition\n",
        "                            local_score = (consecutive_count * phrase_len) / len(segment_words)\n",
        "\n",
        "                        max_score = max(max_score, min(local_score, 1.0))\n",
        "\n",
        "        return max_score\n",
        "\n",
        "\n",
        "class UnifiedSpamDetector:\n",
        "    \"\"\"Unified spam detector combining TF-IDF and pattern analysis in single ML model.\"\"\"\n",
        "\n",
        "    def __init__(self, max_features: int = 5000, ngram_range: Tuple[int, int] = (1, 3),\n",
        "                 spam_threshold: float = 0.3):\n",
        "        self.max_features = max_features\n",
        "        self.ngram_range = ngram_range\n",
        "        self.spam_threshold = spam_threshold\n",
        "        self.pipeline = None\n",
        "        self.calibrated_pipeline = None\n",
        "        self.training_size = 0\n",
        "\n",
        "    def fit(self, texts: List[str], labels: List[str], calibrate: bool = True):\n",
        "        \"\"\"\n",
        "        Fit the unified model combining TF-IDF and pattern features.\n",
        "\n",
        "        Args:\n",
        "            texts: List of review texts\n",
        "            labels: List of labels ('APPROVE' or 'REJECT')\n",
        "            calibrate: Whether to calibrate probabilities\n",
        "        \"\"\"\n",
        "        # Convert labels to binary\n",
        "        y = [1 if label == 'REJECT' else 0 for label in labels]\n",
        "        self.training_size = len(texts)\n",
        "\n",
        "        # For small datasets, disable ML-based classification and rely on patterns only\n",
        "        if len(texts) < 10:\n",
        "            # With very small training data, ML is unreliable - use pattern-only mode\n",
        "            self.effective_threshold = 0.9  # Very high threshold to disable ML classification\n",
        "            self.pattern_only_mode = True\n",
        "            print(f\"‚ö†Ô∏è  Very small training set ({len(texts)} samples). Using pattern-only detection mode.\")\n",
        "        elif len(texts) < 20:\n",
        "            # Small dataset - be more conservative with ML threshold\n",
        "            adjusted_threshold = min(0.6, self.spam_threshold + 0.2)\n",
        "            print(f\"‚ö†Ô∏è  Small training set ({len(texts)} samples). Adjusting threshold: {self.spam_threshold:.2f} ‚Üí {adjusted_threshold:.2f}\")\n",
        "            self.effective_threshold = adjusted_threshold\n",
        "            self.pattern_only_mode = False\n",
        "        else:\n",
        "            self.effective_threshold = self.spam_threshold\n",
        "            self.pattern_only_mode = False\n",
        "\n",
        "        # Create unified pipeline with both TF-IDF and pattern features\n",
        "        tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=self.max_features,\n",
        "            ngram_range=self.ngram_range,\n",
        "            stop_words='english',\n",
        "            lowercase=True,\n",
        "            min_df=1,\n",
        "            max_df=0.95\n",
        "        )\n",
        "\n",
        "        pattern_extractor = PatternFeatureExtractor()\n",
        "\n",
        "        # Combine features using FeatureUnion\n",
        "        feature_union = FeatureUnion([  # type: ignore\n",
        "            ('tfidf', tfidf_vectorizer),\n",
        "            ('patterns', pattern_extractor)\n",
        "        ])\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            ('features', feature_union),\n",
        "            ('classifier', LogisticRegression(\n",
        "                random_state=42,\n",
        "                max_iter=1000,\n",
        "                class_weight='balanced'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # Fit the pipeline\n",
        "        print(\"Training unified ML + Pattern model...\")\n",
        "        self.pipeline.fit(texts, y)\n",
        "\n",
        "        # Calibrate probabilities if requested\n",
        "        if calibrate and len(texts) >= 10:\n",
        "            min_class_size = min(sum(y), len(y) - sum(y))\n",
        "            cv_folds = min(3, max(2, min_class_size))\n",
        "\n",
        "            self.calibrated_pipeline = CalibratedClassifierCV(\n",
        "                self.pipeline,\n",
        "                method='isotonic',\n",
        "                cv=cv_folds\n",
        "            )\n",
        "            self.calibrated_pipeline.fit(texts, y)  # type: ignore\n",
        "        elif calibrate:\n",
        "            print(f\"Skipping calibration: need at least 10 samples, got {len(texts)}\")\n",
        "\n",
        "    def predict(self, texts: List[str], use_calibrated: bool = True) -> List[DetectionResult]:\n",
        "        \"\"\"Predict spam using unified model.\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
        "\n",
        "        pipeline = self.calibrated_pipeline if (use_calibrated and self.calibrated_pipeline) else self.pipeline\n",
        "\n",
        "        # Get predictions and probabilities\n",
        "        proba = pipeline.predict_proba(texts)  # type: ignore\n",
        "        spam_proba = proba[:, 1]  # Probability of spam (REJECT)\n",
        "\n",
        "        results = []\n",
        "        for i, prob in enumerate(spam_proba):\n",
        "            # Extract pattern features for analysis\n",
        "            pattern_extractor = PatternFeatureExtractor()\n",
        "            text_features = pattern_extractor._extract_pattern_features(texts[i])\n",
        "\n",
        "            # Check if we're in pattern-only mode (small training data)\n",
        "            pattern_only_mode = getattr(self, 'pattern_only_mode', False)\n",
        "            threshold = getattr(self, 'effective_threshold', self.spam_threshold)\n",
        "\n",
        "            if pattern_only_mode:\n",
        "                # Pattern-only classification for small datasets\n",
        "                word_count = text_features['word_count']\n",
        "\n",
        "                # Define clear spam patterns\n",
        "                is_obvious_spam = (\n",
        "                    (text_features['repetition_ratio'] > 0.8) or  # Very high word repetition\n",
        "                    (text_features['phrase_repetition_score'] > 0.7) or  # Very high phrase repetition\n",
        "                    (text_features['repeated_ngrams_ratio'] > 0.6) or  # Very high ngram repetition\n",
        "                    (text_features['local_repetition_score'] > 0.4) or  # Local repetition (like \"food is good food is good\")\n",
        "                    (word_count < 3 and text_features['repetition_ratio'] > 0.6)  # Short repetitive text\n",
        "                )\n",
        "\n",
        "                # Special cases for high-confidence spam detection\n",
        "                is_phrase_spam = text_features['phrase_repetition_score'] > 0.9\n",
        "                is_local_spam = text_features['local_repetition_score'] > 0.5\n",
        "\n",
        "                # Additional filters to avoid false positives (but not for phrase spam)\n",
        "                seems_legitimate = (\n",
        "                    text_features['readability_score'] > 30 and\n",
        "                    word_count > 8 and\n",
        "                    text_features['word_diversity_ratio'] > 0.6 and\n",
        "                    text_features['phrase_repetition_score'] < 0.8  # Allow phrase detection override\n",
        "                )\n",
        "\n",
        "                # Flag obvious spam patterns, high phrase repetition, or local repetition\n",
        "                if is_phrase_spam or is_local_spam or (is_obvious_spam and not seems_legitimate):\n",
        "                    label = 'REJECT'\n",
        "                    confidence = max(\n",
        "                        text_features['repetition_ratio'],\n",
        "                        text_features['phrase_repetition_score'],\n",
        "                        text_features['repeated_ngrams_ratio'],\n",
        "                        text_features['local_repetition_score']\n",
        "                    )\n",
        "                else:\n",
        "                    label = 'APPROVE'\n",
        "                    confidence = 1.0 - max(\n",
        "                        text_features['repetition_ratio'] * 0.5,\n",
        "                        text_features['phrase_repetition_score'] * 0.5,\n",
        "                        text_features['repeated_ngrams_ratio'] * 0.5,\n",
        "                        text_features['local_repetition_score'] * 0.5\n",
        "                    )\n",
        "\n",
        "                pattern_override = (label == 'REJECT')\n",
        "\n",
        "            else:\n",
        "                # Normal ML + pattern mode\n",
        "                label = 'REJECT' if prob > threshold else 'APPROVE'\n",
        "\n",
        "                # Pattern-based override for obvious spam\n",
        "                word_count = text_features['word_count']\n",
        "                is_very_repetitive = (\n",
        "                    (text_features['repetition_ratio'] > 0.8) or\n",
        "                    (text_features['phrase_repetition_score'] > 0.7) or\n",
        "                    (text_features['repeated_ngrams_ratio'] > 0.6)\n",
        "                )\n",
        "\n",
        "                has_reasonable_diversity = text_features['word_diversity_ratio'] > 0.3 and word_count > 6\n",
        "                is_likely_review = text_features['readability_score'] > 20 and word_count > 4\n",
        "\n",
        "                if is_very_repetitive and not (has_reasonable_diversity and is_likely_review):\n",
        "                    label = 'REJECT'\n",
        "                    pattern_override = True\n",
        "                    confidence = max(\n",
        "                        text_features['repetition_ratio'],\n",
        "                        text_features['phrase_repetition_score'],\n",
        "                        text_features['repeated_ngrams_ratio']\n",
        "                    )\n",
        "                else:\n",
        "                    pattern_override = False\n",
        "                    confidence = prob if label == 'REJECT' else (1 - prob)\n",
        "\n",
        "            # Determine category based on dominant pattern\n",
        "            category = 'None'\n",
        "            if label == 'REJECT':\n",
        "                if pattern_override:\n",
        "                    if text_features['repetition_ratio'] > 0.6:\n",
        "                        category = 'Repetitive_Spam'\n",
        "                    elif text_features['local_repetition_score'] > 0.4:\n",
        "                        category = 'Local_Repetition_Spam'\n",
        "                    elif text_features['phrase_repetition_score'] > 0.5:\n",
        "                        category = 'Phrase_Pattern_Spam'\n",
        "                    elif text_features['repeated_ngrams_ratio'] > 0.4:\n",
        "                        category = 'NGram_Pattern_Spam'\n",
        "                    else:\n",
        "                        category = 'Pattern_Spam'\n",
        "                else:\n",
        "                    # ML-detected spam\n",
        "                    if text_features['repetition_ratio'] > 0.4 or text_features['phrase_repetition_score'] > 0.4:\n",
        "                        category = 'Repetitive_Spam'\n",
        "                    elif text_features['template_score'] > 0.6:\n",
        "                        category = 'Template_Spam'\n",
        "                    else:\n",
        "                        category = 'ML_Detected_Spam'\n",
        "\n",
        "            # Simple confidence interval calculation\n",
        "            ci_margin = 0.1 * (1 - confidence)  # Smaller margin for higher confidence\n",
        "            ci_lower = max(0.0, confidence - ci_margin)\n",
        "            ci_upper = min(1.0, confidence + ci_margin)\n",
        "\n",
        "            results.append(DetectionResult(\n",
        "                label=label,\n",
        "                confidence=float(confidence),\n",
        "                category=category,\n",
        "                features={\n",
        "                    'spam_probability': float(prob),\n",
        "                    'pattern_features': text_features,\n",
        "                    'threshold_used': float(threshold)\n",
        "                },\n",
        "                confidence_interval=(float(ci_lower), float(ci_upper))\n",
        "            ))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_feature_importance(self, top_k: int = 20) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Get most important features from the unified model.\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not fitted.\")\n",
        "\n",
        "        try:\n",
        "            classifier = self.pipeline.named_steps['classifier']\n",
        "            feature_union = self.pipeline.named_steps['features']\n",
        "\n",
        "            # Get feature names\n",
        "            tfidf_vectorizer = feature_union.transformer_list[0][1]\n",
        "            tfidf_names = list(tfidf_vectorizer.get_feature_names_out())\n",
        "            pattern_names = [\n",
        "                'repetition_ratio', 'word_diversity_ratio', 'repeated_ngrams_ratio',\n",
        "                'caps_ratio', 'punct_ratio', 'readability_score',\n",
        "                'avg_sentence_length', 'word_count', 'template_score',\n",
        "                'phrase_repetition_score', 'local_repetition_score'\n",
        "            ]\n",
        "\n",
        "            all_feature_names = tfidf_names + pattern_names\n",
        "\n",
        "            # Get coefficients\n",
        "            coef = classifier.coef_[0]\n",
        "            top_indices = np.argsort(np.abs(coef))[-top_k:][::-1]\n",
        "\n",
        "            return [(all_feature_names[idx], float(coef[idx])) for idx in top_indices]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not extract feature importance: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "def load_training_data(file_path: str) -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"Load training data from CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Handle different column name variations\n",
        "    text_col = None\n",
        "    for col in ['text', 'review', 'content', 'text_clean']:\n",
        "        if col in df.columns:\n",
        "            text_col = col\n",
        "            break\n",
        "\n",
        "    if text_col is None:\n",
        "        raise ValueError(f\"No text column found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "\n",
        "    # Handle labels - check if column exists, if not create default\n",
        "    if 'gold_label' in df.columns:\n",
        "        labels = df['gold_label'].tolist()\n",
        "    elif 'label' in df.columns:\n",
        "        labels = df['label'].tolist()\n",
        "    else:\n",
        "        labels = ['APPROVE'] * len(texts)\n",
        "\n",
        "    # Handle categories - check if column exists, if not create default\n",
        "    if 'gold_category' in df.columns:\n",
        "        categories = df['gold_category'].tolist()\n",
        "    elif 'category' in df.columns:\n",
        "        categories = df['category'].tolist()\n",
        "    else:\n",
        "        categories = ['None'] * len(texts)\n",
        "\n",
        "    return texts, labels, categories\n",
        "\n",
        "\n",
        "def evaluate_detector(detector: UnifiedSpamDetector,\n",
        "                              texts: List[str],\n",
        "                              true_labels: List[str],\n",
        "                              true_categories: List[str]) -> Dict:\n",
        "    \"\"\"Evaluate the unified detector performance.\"\"\"\n",
        "    results = detector.predict(texts)\n",
        "\n",
        "    # Extract predictions\n",
        "    pred_labels = [r.label for r in results]\n",
        "    pred_categories = [r.category for r in results]\n",
        "\n",
        "    # Binary classification metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, pred_labels, average='binary', pos_label='REJECT'\n",
        "    )\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "    # Confidence analysis\n",
        "    confidences = [r.confidence for r in results]\n",
        "\n",
        "    evaluation_results = {\n",
        "        'binary_classification': {\n",
        "            'accuracy': float(accuracy),\n",
        "            'precision': float(precision),\n",
        "            'recall': float(recall),\n",
        "            'f1': float(f1)\n",
        "        },\n",
        "        'confidence_stats': {\n",
        "            'mean': float(np.mean(confidences)),\n",
        "            'std': float(np.std(confidences)),\n",
        "            'min': float(np.min(confidences)),\n",
        "            'max': float(np.max(confidences))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return evaluation_results"
      ],
      "metadata": {
        "id": "gLefa0zasVqK",
        "outputId": "cc846524-9cc2-48ce-9163-776044899d0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gLefa0zasVqK",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: textstat in /usr/local/lib/python3.12/dist-packages (0.7.10)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.12/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (4.67.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yXwykZFTvj3v"
      },
      "id": "yXwykZFTvj3v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure a dataframe `df` with columns: text, gold_label, gold_category\n",
        "import pandas as pd, os\n",
        "\n",
        "PSEUDO_DIR  = \"data/pseudo-label\"\n",
        "PSEUDO_PATH = os.path.join(PSEUDO_DIR, \"gemini_pseudo_labels.csv\")\n",
        "\n",
        "def choose_training_df():\n",
        "    # 1) Prefer cached pseudo-labels if present\n",
        "    if os.path.exists(PSEUDO_PATH):\n",
        "        print(f\"üì¶ Using cached pseudo-labels ‚Üí {PSEUDO_PATH}\")\n",
        "        tdf = pd.read_csv(PSEUDO_PATH)\n",
        "        # normalize column names from pseudo-labels\n",
        "        rename_map = {\n",
        "            \"pred_label\": \"gold_label\",\n",
        "            \"pred_category\": \"gold_category\"\n",
        "        }\n",
        "        for k, v in rename_map.items():\n",
        "            if k in tdf.columns and v not in tdf.columns:\n",
        "                tdf = tdf.rename(columns={k: v})\n",
        "        # sanity check\n",
        "        need = {\"text\", \"gold_label\", \"gold_category\"}\n",
        "        missing = need - set(tdf.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Pseudo-label file missing columns: {missing}\")\n",
        "        # ensure types\n",
        "        tdf[\"text\"] = tdf[\"text\"].astype(str)\n",
        "        tdf[\"gold_label\"] = tdf[\"gold_label\"].astype(str)\n",
        "        tdf[\"gold_category\"] = tdf[\"gold_category\"].astype(str)\n",
        "        print(f\"‚úÖ Pseudo-label rows: {len(tdf)}\")\n",
        "        return tdf[[\"text\", \"gold_label\", \"gold_category\"]].copy()\n",
        "\n",
        "    # 2) Optional CSV fallbacks if you have a manual train file\n",
        "    for path in [\"data/train.csv\", \"train.csv\", \"data/df.csv\"]:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"üìÑ Loading training data from {path}\")\n",
        "            tdf = pd.read_csv(path)\n",
        "            # normalize possible column variants\n",
        "            if \"label\" in tdf.columns and \"gold_label\" not in tdf.columns:\n",
        "                tdf = tdf.rename(columns={\"label\": \"gold_label\"})\n",
        "            if \"category\" in tdf.columns and \"gold_category\" not in tdf.columns:\n",
        "                tdf = tdf.rename(columns={\"category\": \"gold_category\"})\n",
        "            need = {\"text\", \"gold_label\", \"gold_category\"}\n",
        "            missing = need - set(tdf.columns)\n",
        "            if missing:\n",
        "                raise ValueError(f\"Training CSV missing columns: {missing}\")\n",
        "            tdf[\"text\"] = tdf[\"text\"].astype(str)\n",
        "            return tdf[[\"text\", \"gold_label\", \"gold_category\"]].copy()\n",
        "\n",
        "    # 3) Last resort: tiny demo set\n",
        "    print(\"‚ÑπÔ∏è No pseudo-labels or CSV found; using small demo dataset (5 rows).\")\n",
        "    return pd.DataFrame({\n",
        "        \"text\": [\n",
        "            \"Food is good food is great food is nice food is good\",\n",
        "            \"Service was quick and friendly. Would return.\",\n",
        "            \"Amazing place! Highly recommend this restaurant.\",\n",
        "            \"Visit my site http://spam.biz ‚Äì use promo code NOW!\",\n",
        "            \"Terrible service. Will not come back.\"\n",
        "        ],\n",
        "        \"gold_label\": [\"REJECT\", \"APPROVE\", \"APPROVE\", \"REJECT\", \"APPROVE\"],\n",
        "        \"gold_category\": [\"Repetitive_Spam\", \"None\", \"Template_Spam\", \"ML_Detected_Spam\", \"None\"]\n",
        "    })\n",
        "\n",
        "# Important: don't keep an old global df around\n",
        "if 'df' in globals():\n",
        "    del df\n",
        "\n",
        "train_df = choose_training_df()\n",
        "print(train_df.head(5))\n",
        "# -----------------------"
      ],
      "metadata": {
        "id": "hga5kenfvkNV",
        "outputId": "403ec164-9c91-47e8-88af-c0f0fee633f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hga5kenfvkNV",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Using cached pseudo-labels ‚Üí data/pseudo-label/gemini_pseudo_labels.csv\n",
            "‚úÖ Pseudo-label rows: 245\n",
            "                                                text gold_label  \\\n",
            "0  Call them now, book now! The team was amazing,...    APPROVE   \n",
            "1  This team couldn't have been more perfect. I d...    APPROVE   \n",
            "2  Dive Oahu runs an awesome program!!!!! The ins...    APPROVE   \n",
            "3  We had a great time snorkeling on our honeymoo...    APPROVE   \n",
            "4  Early morning dive on the Sea Tiger and Pipe, ...    APPROVE   \n",
            "\n",
            "          gold_category  \n",
            "0  Authentic Experience  \n",
            "1  Authentic Experience  \n",
            "2  Authentic Experience  \n",
            "3  Authentic Experience  \n",
            "4  Authentic Experience  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts      = train_df['text'].astype(str).tolist()\n",
        "labels     = train_df['gold_label'].tolist()\n",
        "categories = train_df['gold_category'].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test, cat_train, cat_test = train_test_split(\n",
        "    texts, labels, categories, test_size=0.3, random_state=42, stratify=labels if len(set(labels)) > 1 else None\n",
        ")\n",
        "detector = UnifiedSpamDetector(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 3),\n",
        "    spam_threshold=0.3\n",
        ")\n",
        "\n",
        "detector.fit(X_train, y_train, calibrate=True)\n",
        "\n",
        "eval_results = evaluate_detector(detector, X_test, y_test, cat_test)\n",
        "print(\"Unified Spam Detector Evaluation:\")\n",
        "eval_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T41uylNzvpS_",
        "outputId": "3ec19092-cb74-4db3-c380-6e36a74672d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "T41uylNzvpS_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training unified ML + Pattern model...\n",
            "Unified Spam Detector Evaluation:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'binary_classification': {'accuracy': 0.972972972972973,\n",
              "  'precision': 0.0,\n",
              "  'recall': 0.0,\n",
              "  'f1': 0.0},\n",
              " 'confidence_stats': {'mean': 0.977177519892053,\n",
              "  'std': 0.007717953980366864,\n",
              "  'min': 0.9714242286061712,\n",
              "  'max': 1.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c020a20",
      "metadata": {
        "id": "4c020a20"
      },
      "source": [
        "## 9. Model Persistence and Export (After Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "54193be4",
      "metadata": {
        "id": "54193be4",
        "outputId": "01d760aa-0049-4b2f-e947-dca47233bcc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL PERSISTENCE AND DATA EXPORT\n",
            "============================================================\n",
            "‚úÖ UnifiedSpamDetector saved at models/saved_models/unified_spam_detector_20250830_195222.joblib\n",
            "Training Mode: fine-tuning\n",
            "Timestamp: 20250830_195222\n",
            "\n",
            "üíæ SAVING FINE-TUNED MODELS\n",
            "‚úÖ Fine-tuned model copied to: models/saved_models/review_classifier_20250830_195222\n",
            "‚úÖ Model loading test successful\n",
            "\n",
            "üíæ SAVING TRAINING DATA\n",
            "‚úÖ Training data saved: data/training/pseudo_labels_20250830_195222.csv\n",
            "   Training Labels: {'APPROVE': 237, 'REJECT': 8}\n",
            "\n",
            "üíæ SAVING PREDICTION RESULTS\n",
            "‚úÖ Predictions saved: data/predictions/predictions_20250830_195222.csv\n",
            "   Prediction Labels: {'APPROVE': 245}\n",
            "   Average Confidence: 0.8640\n",
            "‚úÖ Model info saved: results/model_info/model_info_20250830_195222.json\n",
            "\n",
            "üöÄ CREATING DEPLOYMENT STRUCTURE\n",
            "‚úÖ Sample data for inference: data/actual/sample_reviews.csv\n",
            "‚úÖ Latest model config: models/saved_models/latest_config.json\n",
            "‚úÖ Model loader utilities: src/utils/model_loader.py\n",
            "\n",
            "============================================================\n",
            "PERSISTENCE COMPLETE\n",
            "============================================================\n",
            "‚úÖ Training Mode: fine-tuning\n",
            "‚úÖ Timestamp: 20250830_195222\n",
            "‚úÖ Custom Model: models/saved_models/review_classifier_20250830_195222\n",
            "‚úÖ Model Info: results/model_info/model_info_20250830_195222.json\n",
            "‚úÖ Latest Config: models/saved_models/latest_config.json\n",
            "‚úÖ Training Data: data/training/pseudo_labels_20250830_195222.csv\n",
            "   Size: 245 examples\n",
            "‚úÖ Predictions: data/predictions/predictions_20250830_195222.csv\n",
            "   Processed: 245 reviews\n",
            "\n",
            "üéØ READY FOR INFERENCE PIPELINE\n",
            "   Use: models/saved_models/latest_config.json\n",
            "   Data: data/actual/sample_reviews.csv\n",
            "   Load: src/utils/model_loader.py\n",
            "\n",
            "‚úÖ MODEL TRAINING AND PERSISTENCE COMPLETE!\n"
          ]
        }
      ],
      "source": [
        "# Model Persistence and Training Data Export\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"MODEL PERSISTENCE AND DATA EXPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('models/saved_models', exist_ok=True)\n",
        "os.makedirs('data/training', exist_ok=True)\n",
        "os.makedirs('data/predictions', exist_ok=True)\n",
        "os.makedirs('results/model_info', exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "\n",
        "# Generate timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Model information\n",
        "model_info = {\n",
        "    \"training_timestamp\": timestamp,\n",
        "    \"training_mode\": training_mode,\n",
        "    \"base_model\": BASE_MODEL if training_mode == \"fine-tuning\" else \"pre-trained-only\",\n",
        "    \"auxiliary_models\": {\n",
        "      \"toxicity_model\": TOXIC_MODEL,\n",
        "      \"zero_shot_model\": ZERO_SHOT_MODEL\n",
        "  },\n",
        "    \"training_data_size\": len(pseudo_labels_df) if has_training_data else 0,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
        "    \"performance_metrics\": {}\n",
        "}\n",
        "\n",
        "# === Save UnifiedSpamDetector (scikit-learn pipeline) ===\n",
        "model_filename = f\"models/saved_models/unified_spam_detector_{timestamp}.joblib\"\n",
        "joblib.dump(detector, model_filename)\n",
        "\n",
        "print(f\"‚úÖ UnifiedSpamDetector saved at {model_filename}\")\n",
        "\n",
        "# Track it in model_info\n",
        "model_info[\"sklearn_model_path\"] = model_filename\n",
        "\n",
        "\n",
        "# Generate timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Training Mode: {training_mode}\")\n",
        "print(f\"Timestamp: {timestamp}\")\n",
        "\n",
        "# Save trained models\n",
        "if training_mode == \"fine-tuning\" and TRAINED_MODELS['custom_classifier'] is not None:\n",
        "    print(f\"\\nüíæ SAVING FINE-TUNED MODELS\")\n",
        "\n",
        "    # Custom model is already saved during training\n",
        "    custom_model_path = TRAINED_MODELS['custom_classifier']['model_path']\n",
        "    final_model_path = f'models/saved_models/review_classifier_{timestamp}'\n",
        "\n",
        "    # Copy to final location with timestamp\n",
        "    if os.path.exists(custom_model_path):\n",
        "        shutil.copytree(custom_model_path, final_model_path, dirs_exist_ok=True)\n",
        "        print(f\"‚úÖ Fine-tuned model copied to: {final_model_path}\")\n",
        "\n",
        "        model_info[\"custom_model_path\"] = final_model_path\n",
        "        model_info[\"model_files\"] = {\n",
        "            \"config\": f\"{final_model_path}/config.json\",\n",
        "            \"model\": f\"{final_model_path}/pytorch_model.bin\",\n",
        "            \"tokenizer\": f\"{final_model_path}/tokenizer.json\"\n",
        "        }\n",
        "\n",
        "        # Test model loading\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "            test_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
        "            test_model = AutoModelForSequenceClassification.from_pretrained(final_model_path)\n",
        "            print(\"‚úÖ Model loading test successful\")\n",
        "            model_info[\"model_loadable\"] = True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model loading test failed: {e}\")\n",
        "            model_info[\"model_loadable\"] = False\n",
        "    else:\n",
        "        print(f\"‚ùå Custom model path not found: {custom_model_path}\")\n",
        "        model_info[\"custom_model_path\"] = None\n",
        "\n",
        "else:\n",
        "    print(f\"\\nüì¶ SAVING PRE-TRAINED MODEL REFERENCES\")\n",
        "    model_info[\"custom_model_path\"] = None\n",
        "    model_info[\"model_files\"] = None\n",
        "\n",
        "# Save auxiliary model pipeline state (for consistency in inference)\n",
        "auxiliary_info = {\n",
        "    \"toxicity_model\": TOXIC_MODEL,\n",
        "    \"zero_shot_model\": ZERO_SHOT_MODEL,\n",
        "    \"device_used\": 0 if torch.cuda.is_available() else -1,\n",
        "    \"models_loaded\": False  # we store names; loader will instantiate\n",
        "}\n",
        "\n",
        "\n",
        "# Save training data if available\n",
        "if has_training_data:\n",
        "    print(f\"\\nüíæ SAVING TRAINING DATA\")\n",
        "\n",
        "    # Save pseudo-labeled training data\n",
        "    training_data_path = f'data/training/pseudo_labels_{timestamp}.csv'\n",
        "    pseudo_labels_df.to_csv(training_data_path, index=False)\n",
        "    print(f\"‚úÖ Training data saved: {training_data_path}\")\n",
        "\n",
        "    model_info[\"training_data_path\"] = training_data_path\n",
        "    model_info[\"training_data_size\"] = len(pseudo_labels_df)\n",
        "\n",
        "    # Save label distribution\n",
        "    label_dist = pseudo_labels_df['pred_label'].value_counts().to_dict()\n",
        "    model_info[\"training_label_distribution\"] = label_dist\n",
        "    print(f\"   Training Labels: {label_dist}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è No training data to save\")\n",
        "    model_info[\"training_data_path\"] = None\n",
        "\n",
        "# Save prediction results\n",
        "if 'hf_results' in locals():\n",
        "    print(f\"\\nüíæ SAVING PREDICTION RESULTS\")\n",
        "\n",
        "    predictions_path = f'data/predictions/predictions_{timestamp}.csv'\n",
        "    hf_results.to_csv(predictions_path, index=False)\n",
        "    print(f\"‚úÖ Predictions saved: {predictions_path}\")\n",
        "\n",
        "    model_info[\"predictions_path\"] = predictions_path\n",
        "    model_info[\"predictions_count\"] = len(hf_results)\n",
        "\n",
        "    # Add performance metrics\n",
        "    pred_dist = hf_results['pred_label'].value_counts().to_dict()\n",
        "    avg_confidence = hf_results['confidence'].mean()\n",
        "\n",
        "    model_info[\"performance_metrics\"] = {\n",
        "        \"prediction_distribution\": pred_dist,\n",
        "        \"average_confidence\": round(float(avg_confidence), 4),\n",
        "        \"total_processed\": len(hf_results)\n",
        "    }\n",
        "    print(f\"   Prediction Labels: {pred_dist}\")\n",
        "    print(f\"   Average Confidence: {avg_confidence:.4f}\")\n",
        "\n",
        "# Save comprehensive model information\n",
        "model_info_path = f'results/model_info/model_info_{timestamp}.json'\n",
        "with open(model_info_path, 'w') as f:\n",
        "    json.dump(model_info, f, indent=2, default=str)\n",
        "\n",
        "print(f\"‚úÖ Model info saved: {model_info_path}\")\n",
        "\n",
        "# Create deployment-ready structure for inference pipeline\n",
        "print(f\"\\nüöÄ CREATING DEPLOYMENT STRUCTURE\")\n",
        "\n",
        "# Create data/actual with sample data for inference pipeline\n",
        "os.makedirs('data/actual', exist_ok=True)\n",
        "\n",
        "# If we have results, save a sample to data/actual for the inference pipeline\n",
        "if 'hf_results' in locals() and len(hf_results) > 0:\n",
        "    # Create sample data for inference testing\n",
        "    sample_actual = hf_results[['id', 'text']].head(3).copy()\n",
        "    sample_actual.to_csv('data/actual/sample_reviews.csv', index=False)\n",
        "    print(\"‚úÖ Sample data for inference: data/actual/sample_reviews.csv\")\n",
        "\n",
        "# Save latest model paths for inference pipeline\n",
        "latest_model_config = {\n",
        "    \"latest_model_info\": model_info_path,\n",
        "    \"training_mode\": training_mode,\n",
        "    \"custom_model_path\": model_info.get(\"custom_model_path\"),\n",
        "    \"auxiliary_models\": auxiliary_info,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLDS['DEFAULT'],\n",
        "    \"timestamp\": timestamp\n",
        "}\n",
        "\n",
        "with open('models/saved_models/latest_config.json', 'w') as f:\n",
        "    json.dump(latest_model_config, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Latest model config: models/saved_models/latest_config.json\")\n",
        "\n",
        "# Create model loading utilities for inference\n",
        "model_loader_code = '''\"\"\"\n",
        "Model Loading Utilities for Review Classification Pipeline\n",
        "Generated automatically during training\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "def load_latest_models():\n",
        "    \"\"\"Load the most recently trained models\"\"\"\n",
        "\n",
        "    config_path = 'models/saved_models/latest_config.json'\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(\"No trained models found. Run training pipeline first.\")\n",
        "\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # Load custom model if available\n",
        "    if config['custom_model_path'] and os.path.exists(config['custom_model_path']):\n",
        "        models['custom'] = {\n",
        "            'tokenizer': AutoTokenizer.from_pretrained(config['custom_model_path']),\n",
        "            'model': AutoModelForSequenceClassification.from_pretrained(config['custom_model_path'])\n",
        "        }\n",
        "\n",
        "    # Load auxiliary models\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    aux_config = config['auxiliary_models']\n",
        "\n",
        "    models['auxiliary'] = {\n",
        "    'toxicity': pipeline(\"text-classification\",\n",
        "                         model=aux_config['toxicity_model'], device=device),\n",
        "    'zero_shot': pipeline(\"zero-shot-classification\",\n",
        "                          model=aux_config['zero_shot_model'], device=device),\n",
        "}\n",
        "\n",
        "\n",
        "    return models, config\n",
        "\n",
        "def predict_review(text, models, config):\n",
        "    \"\"\"Make prediction using loaded models\"\"\"\n",
        "\n",
        "    if 'custom' in models:\n",
        "        # Use fine-tuned model\n",
        "        inputs = models['custom']['tokenizer'](\n",
        "            text, truncation=True, padding=True,\n",
        "            max_length=256, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = models['custom']['model'](**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            confidence = float(torch.max(predictions))\n",
        "            predicted_class = int(torch.argmax(predictions))\n",
        "\n",
        "        label = \"REJECT\" if predicted_class == 1 else \"APPROVE\"\n",
        "\n",
        "    else:\n",
        "        # Use zero-shot classification\n",
        "        # Implementation would mirror the training notebook logic\n",
        "        label = \"APPROVE\"  # Placeholder\n",
        "        confidence = 0.5\n",
        "\n",
        "    return {\n",
        "        'label': label,\n",
        "        'confidence': confidence,\n",
        "        'model_type': config['training_mode']\n",
        "    }\n",
        "'''\n",
        "\n",
        "Path(\"src/utils\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"src/__init__.py\").touch(exist_ok=True)\n",
        "Path(\"src/utils/__init__.py\").touch(exist_ok=True)\n",
        "\n",
        "with open('src/utils/model_loader.py', 'w') as f:\n",
        "    f.write(model_loader_code)\n",
        "\n",
        "print(\"‚úÖ Model loader utilities: src/utils/model_loader.py\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"PERSISTENCE COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"‚úÖ Training Mode: {training_mode}\")\n",
        "print(f\"‚úÖ Timestamp: {timestamp}\")\n",
        "\n",
        "if model_info.get(\"custom_model_path\"):\n",
        "    print(f\"‚úÖ Custom Model: {model_info['custom_model_path']}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Custom Model: None (using pre-trained)\")\n",
        "\n",
        "print(f\"‚úÖ Model Info: {model_info_path}\")\n",
        "print(f\"‚úÖ Latest Config: models/saved_models/latest_config.json\")\n",
        "\n",
        "if has_training_data:\n",
        "    print(f\"‚úÖ Training Data: {model_info['training_data_path']}\")\n",
        "    print(f\"   Size: {model_info['training_data_size']} examples\")\n",
        "\n",
        "if 'hf_results' in locals():\n",
        "    print(f\"‚úÖ Predictions: {model_info['predictions_path']}\")\n",
        "    print(f\"   Processed: {model_info['predictions_count']} reviews\")\n",
        "\n",
        "print(f\"\\nüéØ READY FOR INFERENCE PIPELINE\")\n",
        "print(f\"   Use: models/saved_models/latest_config.json\")\n",
        "print(f\"   Data: data/actual/sample_reviews.csv\")\n",
        "print(f\"   Load: src/utils/model_loader.py\")\n",
        "\n",
        "print(f\"\\n‚úÖ MODEL TRAINING AND PERSISTENCE COMPLETE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "981d961c",
      "metadata": {
        "id": "981d961c"
      },
      "source": [
        "## 10. Model Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e7e21722",
      "metadata": {
        "id": "e7e21722",
        "outputId": "6066930b-157e-440d-b602-6dfa5a2511d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance Evaluation\n",
            "========================================\n",
            "Overall Accuracy: 0.967\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     APPROVE       0.97      1.00      0.98       237\n",
            "      REJECT       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.97       245\n",
            "   macro avg       0.48      0.50      0.49       245\n",
            "weighted avg       0.94      0.97      0.95       245\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error Analysis (8 mistakes / 245 rows)\n",
            "==================================================\n",
            "ID 10: true=REJECT pred=APPROVE | Jake is just the absolute cutest Divemaster I have ever seen!! He is super caring, handsome, and treats every dive with ‚Ä¶\n",
            "ID 43: true=REJECT pred=APPROVE | I've been to a lot of dive shops but this was the worst experience I've ever had. The lady behind the counter was rude, ‚Ä¶\n",
            "ID 49: true=REJECT pred=APPROVE | Blatant bait&switch. This company will upcharge you on PADI e-learning, and then reschedule your training (unless you pa‚Ä¶\n",
            "ID 75: true=REJECT pred=APPROVE | Can't even give directions over the phone. Never been there. Probably gonna get lost going there\n",
            "ID 83: true=REJECT pred=APPROVE | Ok ,as he was taken out by the hurrican in Sept. Nice but small.\n",
            "ID 108: true=REJECT pred=APPROVE | nan\n",
            "ID 109: true=REJECT pred=APPROVE | nan\n",
            "ID 167: true=REJECT pred=APPROVE | Very clean! Very EXXXXXXPENSIVE!!! VERRRRRRRRRY POOR CUSTOMER SERVICE! The old ladies, (white hair) told me to pick up m‚Ä¶\n"
          ]
        }
      ],
      "source": [
        "# --- Model Performance Evaluation with Ground Truth (robust) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# 1) Load ground truth = the pseudo-labels you trained on\n",
        "GT_DF = pd.read_csv(\"data/pseudo-label/gemini_pseudo_labels.csv\")\n",
        "\n",
        "# normalize to evaluator's expected names\n",
        "GT_DF = GT_DF.rename(columns={\n",
        "    \"pred_label\": \"gold_label\",\n",
        "    \"pred_category\": \"gold_category\"\n",
        "})\n",
        "\n",
        "# ensure there is an id (stable by order if file has none)\n",
        "if \"id\" not in GT_DF.columns:\n",
        "    GT_DF = GT_DF.reset_index(drop=False).rename(columns={\"index\": \"id\"})\n",
        "\n",
        "def _to_str_label(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    if isinstance(x, (int, np.integer, float, np.floating)):\n",
        "        return \"REJECT\" if int(x) == 1 else \"APPROVE\"\n",
        "    s = str(x).strip().upper()\n",
        "    if s in {\"APPROVE\",\"REJECT\"}: return s\n",
        "    if s in {\"YES\",\"SPAM\",\"BLOCK\"}: return \"REJECT\"\n",
        "    if s in {\"NO\",\"HAM\",\"ALLOW\"}:  return \"APPROVE\"\n",
        "    return np.nan\n",
        "\n",
        "def evaluate_model_with_ground_truth(pred_df: pd.DataFrame, gt_df: pd.DataFrame | None):\n",
        "    print(\"\\nModel Performance Evaluation\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    df_eval = pred_df.copy()\n",
        "\n",
        "    # 2) Attach ground truth if needed\n",
        "    if gt_df is not None and \"gold_label\" not in df_eval.columns:\n",
        "        merged = None\n",
        "        if \"id\" in df_eval.columns and \"id\" in gt_df.columns:\n",
        "            merged = df_eval.merge(gt_df[[\"id\",\"gold_label\",\"gold_category\"]], on=\"id\", how=\"left\")\n",
        "        if (merged is None or \"gold_label\" not in merged.columns) and \"text\" in df_eval.columns and \"text\" in gt_df.columns:\n",
        "            # fallback merge on text\n",
        "            merged = df_eval.merge(\n",
        "                gt_df[[\"text\",\"gold_label\",\"gold_category\"]].drop_duplicates(\"text\"),\n",
        "                on=\"text\", how=\"left\"\n",
        "            )\n",
        "        if merged is None or \"gold_label\" not in merged.columns:\n",
        "            print(\"No ground truth column 'gold_label' could be attached (missing id/text).\")\n",
        "            return None\n",
        "        df_eval = merged\n",
        "\n",
        "    if \"gold_label\" not in df_eval.columns:\n",
        "        print(\"No ground truth column 'gold_label' found after merge.\")\n",
        "        return None\n",
        "\n",
        "    # 3) Normalize labels and drop NaNs\n",
        "    df_eval[\"gold_label_norm\"] = df_eval[\"gold_label\"].apply(_to_str_label)\n",
        "    df_eval[\"pred_label_norm\"] = df_eval[\"pred_label\"].apply(_to_str_label)\n",
        "\n",
        "    before = len(df_eval)\n",
        "    df_eval = df_eval.dropna(subset=[\"gold_label_norm\",\"pred_label_norm\"])\n",
        "    after = len(df_eval)\n",
        "    if after < before:\n",
        "        print(f\"‚ÑπÔ∏è Dropped {before-after} rows with missing/unknown labels after normalization.\")\n",
        "\n",
        "    # 4) Metrics\n",
        "    y_true = df_eval[\"gold_label_norm\"]\n",
        "    y_pred = df_eval[\"pred_label_norm\"]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred, labels=[\"APPROVE\",\"REJECT\"])\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[\"APPROVE\",\"REJECT\"])\n",
        "\n",
        "    print(f\"Overall Accuracy: {acc:.3f}\\n\")\n",
        "    print(\"Detailed Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(6,5))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                    xticklabels=[\"APPROVE\",\"REJECT\"],\n",
        "                    yticklabels=[\"APPROVE\",\"REJECT\"])\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.ylabel(\"Actual\"); plt.xlabel(\"Predicted\")\n",
        "        plt.show()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    errs = df_eval[y_true != y_pred]\n",
        "    if len(errs):\n",
        "        print(f\"\\nError Analysis ({len(errs)} mistakes / {len(df_eval)} rows)\")\n",
        "        print(\"=\"*50)\n",
        "        for _, r in errs.iterrows():\n",
        "            txt = r[\"text\"][:120] + (\"‚Ä¶\" if isinstance(r[\"text\"], str) and len(r[\"text\"])>120 else \"\")\n",
        "            print(f\"ID {r.get('id','?')}: true={r['gold_label_norm']} pred={r['pred_label_norm']} | {txt}\")\n",
        "    else:\n",
        "        print(\"\\nNo classification errors found.\")\n",
        "\n",
        "    return {\"accuracy\": acc, \"n\": len(df_eval), \"errors\": int(len(errs))}\n",
        "\n",
        "# ---- Run evaluation on your predictions ----\n",
        "if \"hf_results\" in globals():\n",
        "    eval_out = evaluate_model_with_ground_truth(hf_results, GT_DF)\n",
        "else:\n",
        "    print(\"No prediction results found - run inference first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/review_classifier_195222_checkpoint93.zip /content/models/saved_models/review_classifier_20250830_195222/checkpoint-93\n"
      ],
      "metadata": {
        "id": "2ExLppNanAW4",
        "outputId": "eb3c9c74-3e0c-464b-9f0a-af3f8a683b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2ExLppNanAW4",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/ (stored 0%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/tokenizer.json (deflated 71%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/training_args.bin (deflated 54%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/optimizer.pt (deflated 38%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/vocab.txt (deflated 53%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/scheduler.pt (deflated 61%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/rng_state.pth (deflated 26%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/model.safetensors (deflated 8%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/trainer_state.json (deflated 73%)\n",
            "  adding: content/models/saved_models/review_classifier_20250830_195222/checkpoint-93/config.json (deflated 45%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qNVoqv2rnIXR"
      },
      "id": "qNVoqv2rnIXR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "598ac73c",
      "metadata": {
        "id": "598ac73c"
      },
      "source": [
        "## 11. Pipeline Summary and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5638aff5",
      "metadata": {
        "id": "5638aff5",
        "outputId": "35728703-ef8b-41ae-ef00-3d1ab0c9e00e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REVIEW-RATER PIPELINE ARCHITECTURE 2.0\n",
            "======================================================================\n",
            "\n",
            "1. ENVIRONMENT SETUP\n",
            "   Platform: Google Colab\n",
            "   GPU Available: ‚ùå No\n",
            "   Device: -1\n",
            "\n",
            "2. DIRECTORY STRUCTURE\n",
            "   ‚úÖ data/raw\n",
            "   ‚úÖ data/clean\n",
            "   ‚úÖ data/pseudo-label\n",
            "   ‚úÖ data/training\n",
            "   ‚úÖ data/testing\n",
            "   ‚úÖ data/actual\n",
            "   ‚úÖ data/sample\n",
            "   ‚úÖ models/saved_models\n",
            "   ‚úÖ models/cache\n",
            "   ‚úÖ results/predictions\n",
            "   ‚ùå results/inference\n",
            "\n",
            "3. PIPELINE ARCHITECTURE\n",
            "   Training Flow (00_ipynb):\n",
            "      data/raw ‚Üí (external) ‚Üí data/clean\n",
            "      data/clean ‚Üí (gemini) ‚Üí data/pseudo-label\n",
            "      data/pseudo-label ‚Üí data/testing + data/training\n",
            "      data/clean ‚Üí data/training (combined)\n",
            "      HuggingFace training on data/training with feedback loop\n",
            "      Trained models ‚Üí models/saved_models\n",
            "\n",
            "   Inference Flow (01_ipynb):\n",
            "      data/actual ‚Üí models/saved_models ‚Üí inference ‚Üí results/inference\n",
            "\n",
            "4. COMPONENT STATUS\n",
            "   ‚úÖ Constants loaded\n",
            "   ‚ùå Sample data ready\n",
            "   ‚úÖ HuggingFace ready\n",
            "   ‚úÖ Gemini available\n",
            "   ‚úÖ Directory structure\n",
            "\n",
            "5. MODEL PERFORMANCE\n",
            "   ‚úÖ Predictions available: 245 reviews\n",
            "   ‚úÖ Average confidence: 0.864\n",
            "   ‚úÖ Label distribution: {'APPROVE': np.int64(245)}\n",
            "\n",
            "6. INTEGRATION READINESS\n",
            "   ‚úÖ Structured output\n",
            "   ‚úÖ Spam detection ready\n",
            "   ‚úÖ Production deployment\n",
            "   ‚ùå Model persistence\n",
            "   ‚ùå Inference pipeline\n",
            "\n",
            "7. NEXT STEPS\n",
            "   Training Phase (This Notebook):\n",
            "   1. ‚úÖ Environment setup complete\n",
            "   2. ‚úÖ Directory structure created\n",
            "   3. ‚úÖ Pipeline architecture established\n",
            "   4. üîÑ Run HuggingFace pipeline (cell 8)\n",
            "   5. üîÑ Export trained models (cell 9)\n",
            "\n",
            "   Production Phase:\n",
            "   1. üìã Place actual review data in data/actual/\n",
            "   2. üìã Run 01_inference_pipeline.ipynb\n",
            "   3. üìã Check results in results/inference/\n",
            "\n",
            "8. OVERALL STATUS\n",
            "   ‚ö†Ô∏è  PIPELINE SETUP IN PROGRESS\n",
            "   Run all cells to complete setup\n",
            "\n",
            "PIPELINE ARCHITECTURE 2.0 SUMMARY\n",
            "======================================================================\n",
            "‚úÖ data/raw ‚Üí data/clean ‚Üí data/pseudo-label ‚Üí data/training/testing\n",
            "‚úÖ HuggingFace training with Gemini feedback loop\n",
            "‚úÖ models/saved_models for production deployment\n",
            "‚úÖ data/actual ‚Üí 01_ipynb ‚Üí results/inference\n",
            "‚úÖ Spam detection integration ready\n",
            "‚úÖ Complete separation of training and inference phases\n"
          ]
        }
      ],
      "source": [
        "# Complete Pipeline Summary and Architecture Validation\n",
        "print(\"REVIEW-RATER PIPELINE ARCHITECTURE 2.0\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we're in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Environment Summary\n",
        "print(f\"\\n1. ENVIRONMENT SETUP\")\n",
        "print(f\"   Platform: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"   GPU Available: {'‚úÖ Yes' if torch.cuda.is_available() else '‚ùå No'}\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Directory Structure Validation\n",
        "print(f\"\\n2. DIRECTORY STRUCTURE\")\n",
        "expected_dirs = [\n",
        "    'data/raw', 'data/clean', 'data/pseudo-label', 'data/training',\n",
        "    'data/testing', 'data/actual', 'data/sample',\n",
        "    'models/saved_models', 'models/cache',\n",
        "    'results/predictions', 'results/inference'\n",
        "]\n",
        "\n",
        "for directory in expected_dirs:\n",
        "    status = \"‚úÖ\" if os.path.exists(directory) else \"‚ùå\"\n",
        "    print(f\"   {status} {directory}\")\n",
        "\n",
        "# Pipeline Architecture Summary\n",
        "print(f\"\\n3. PIPELINE ARCHITECTURE\")\n",
        "print(f\"   Training Flow (00_ipynb):\")\n",
        "print(f\"      data/raw ‚Üí (external) ‚Üí data/clean\")\n",
        "print(f\"      data/clean ‚Üí (gemini) ‚Üí data/pseudo-label\")\n",
        "print(f\"      data/pseudo-label ‚Üí data/testing + data/training\")\n",
        "print(f\"      data/clean ‚Üí data/training (combined)\")\n",
        "print(f\"      HuggingFace training on data/training with feedback loop\")\n",
        "print(f\"      Trained models ‚Üí models/saved_models\")\n",
        "print(f\"\")\n",
        "print(f\"   Inference Flow (01_ipynb):\")\n",
        "print(f\"      data/actual ‚Üí models/saved_models ‚Üí inference ‚Üí results/inference\")\n",
        "\n",
        "# Component Status\n",
        "print(f\"\\n4. COMPONENT STATUS\")\n",
        "components = {\n",
        "    'Constants loaded': 'DEFAULT_MODELS' in globals(),\n",
        "    'Sample data ready': 'df' in locals() or 'sample_df' in locals(),\n",
        "    'HuggingFace ready': True,  # Installed in environment setup\n",
        "    'Gemini available': 'gemini_available' in locals() and locals().get('gemini_available', False),\n",
        "    'Directory structure': all(os.path.exists(d) for d in ['data/clean', 'data/pseudo-label', 'data/actual']),\n",
        "}\n",
        "\n",
        "for component, status in components.items():\n",
        "    print(f\"   {'‚úÖ' if status else '‚ùå'} {component}\")\n",
        "\n",
        "# Model Performance Summary\n",
        "print(f\"\\n5. MODEL PERFORMANCE\")\n",
        "prediction_data = None\n",
        "for var_name in ['hf_results', 'all_predictions_df', 'predictions_df', 'results_df']:\n",
        "    if var_name in globals():\n",
        "        var_value = globals()[var_name]\n",
        "        if hasattr(var_value, 'shape') and len(var_value) > 0:\n",
        "            prediction_data = var_value\n",
        "            break\n",
        "\n",
        "if prediction_data is not None:\n",
        "    print(f\"   ‚úÖ Predictions available: {len(prediction_data)} reviews\")\n",
        "    if 'confidence' in prediction_data.columns:\n",
        "        avg_conf = prediction_data['confidence'].mean()\n",
        "        print(f\"   ‚úÖ Average confidence: {avg_conf:.3f}\")\n",
        "    if 'pred_label' in prediction_data.columns:\n",
        "        label_dist = prediction_data['pred_label'].value_counts()\n",
        "        print(f\"   ‚úÖ Label distribution: {dict(label_dist)}\")\n",
        "else:\n",
        "    print(f\"   ‚ùå No prediction data available\")\n",
        "\n",
        "# Integration Readiness\n",
        "print(f\"\\n6. INTEGRATION READINESS\")\n",
        "integration_checks = {\n",
        "    'Structured output': prediction_data is not None,\n",
        "    'Spam detection ready': True,  # Architecture supports it\n",
        "    'Production deployment': os.path.exists('data/actual'),\n",
        "    'Model persistence': 'save_trained_pipeline' in globals(),\n",
        "    'Inference pipeline': os.path.exists('notebooks/01_inference_pipeline.ipynb'),\n",
        "}\n",
        "\n",
        "for check, status in integration_checks.items():\n",
        "    print(f\"   {'‚úÖ' if status else '‚ùå'} {check}\")\n",
        "\n",
        "# Next Steps\n",
        "print(f\"\\n7. NEXT STEPS\")\n",
        "print(f\"   Training Phase (This Notebook):\")\n",
        "print(f\"   1. ‚úÖ Environment setup complete\")\n",
        "print(f\"   2. ‚úÖ Directory structure created\")\n",
        "print(f\"   3. ‚úÖ Pipeline architecture established\")\n",
        "print(f\"   4. üîÑ Run HuggingFace pipeline (cell 8)\")\n",
        "print(f\"   5. üîÑ Export trained models (cell 9)\")\n",
        "print(f\"\")\n",
        "print(f\"   Production Phase:\")\n",
        "print(f\"   1. üìã Place actual review data in data/actual/\")\n",
        "print(f\"   2. üìã Run 01_inference_pipeline.ipynb\")\n",
        "print(f\"   3. üìã Check results in results/inference/\")\n",
        "\n",
        "# Final Status\n",
        "print(f\"\\n8. OVERALL STATUS\")\n",
        "overall_ready = all([\n",
        "    os.path.exists('data/clean'),\n",
        "    os.path.exists('data/actual'),\n",
        "    'DEFAULT_MODELS' in globals(),\n",
        "    'save_trained_pipeline' in globals()\n",
        "])\n",
        "\n",
        "if overall_ready:\n",
        "    print(f\"   üöÄ PIPELINE READY FOR PRODUCTION\")\n",
        "    print(f\"   ‚úÖ Training architecture: Complete\")\n",
        "    print(f\"   ‚úÖ Inference architecture: Complete\")\n",
        "    print(f\"   ‚úÖ Data flow: Established\")\n",
        "    print(f\"   ‚úÖ Integration points: Ready\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  PIPELINE SETUP IN PROGRESS\")\n",
        "    print(f\"   Run all cells to complete setup\")\n",
        "\n",
        "print(f\"\\nPIPELINE ARCHITECTURE 2.0 SUMMARY\")\n",
        "print(f\"=\" * 70)\n",
        "print(f\"‚úÖ data/raw ‚Üí data/clean ‚Üí data/pseudo-label ‚Üí data/training/testing\")\n",
        "print(f\"‚úÖ HuggingFace training with Gemini feedback loop\")\n",
        "print(f\"‚úÖ models/saved_models for production deployment\")\n",
        "print(f\"‚úÖ data/actual ‚Üí 01_ipynb ‚Üí results/inference\")\n",
        "print(f\"‚úÖ Spam detection integration ready\")\n",
        "print(f\"‚úÖ Complete separation of training and inference phases\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}