{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b1bbdd",
   "metadata": {},
   "source": [
    "# Review Classification Pipeline - Google Colab Setup\n",
    "\n",
    "This notebook sets up and runs the complete review classification pipeline in Google Colab.\n",
    "\n",
    "**Features:**\n",
    "- Ollama-style prompt classification (using OpenAI API)\n",
    "- HuggingFace transformer models\n",
    "- Ensemble classification\n",
    "- GPT pseudo-labeling\n",
    "- Complete evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462d932",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f35169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch pandas scikit-learn openai tqdm\n",
    "!pip install -q datasets accelerate\n",
    "\n",
    "print(\"Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d42837",
   "metadata": {},
   "source": [
    "## 2. Project Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0558d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create all necessary directories\n",
    "directories = [\n",
    "    'src/config', 'src/core', 'src/pseudo_labelling', 'src/utils',\n",
    "    'data/sample', 'data/raw', 'data/processed',\n",
    "    'results/predictions', 'results/evaluations', 'results/reports',\n",
    "    'logs/pipeline_logs', 'models/cache', 'prompts'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    # Create __init__.py files for Python packages\n",
    "    if directory.startswith('src/'):\n",
    "        with open(f'{directory}/__init__.py', 'w') as f:\n",
    "            f.write('# Package init\\n')\n",
    "\n",
    "print(\"Directory structure created!\")\n",
    "print(\"Created directories:\", directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18cdbb",
   "metadata": {},
   "source": [
    "## 3. Sample Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample review data\n",
    "sample_data = {\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'text': [\n",
    "        \"Great product, highly recommend!\",\n",
    "        \"Terrible service, waste of money\", \n",
    "        \"Check out this amazing deal at example.com\",\n",
    "        \"The staff was rude and unprofessional\",\n",
    "        \"Overpriced scammers. Society is doomed.\"\n",
    "    ],\n",
    "    'gold_label': ['APPROVE', 'REJECT', 'REJECT', 'REJECT', 'REJECT'],\n",
    "    'gold_category': ['None', 'Irrelevant', 'No_Ads', 'Irrelevant', 'Rant_No_Visit']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "df.to_csv('data/sample/sample_reviews.csv', index=False)\n",
    "\n",
    "print(\"Sample data created!\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014facd",
   "metadata": {},
   "source": [
    "## 4. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97baccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration classes\n",
    "config_code = '''\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model settings\"\"\"\n",
    "    openai_model: str = \"gpt-3.5-turbo\"\n",
    "    hf_sentiment_model: str = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    hf_toxicity_model: str = \"unitary/toxic-bert\"\n",
    "    hf_zero_shot_model: str = \"facebook/bart-large-mnli\"\n",
    "    \n",
    "    sentiment_threshold: float = 0.7\n",
    "    toxicity_threshold: float = 0.5\n",
    "    zero_shot_threshold: float = 0.7\n",
    "    ensemble_tau: float = 0.55\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Main pipeline configuration\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    openai_api_key: str = \"\"\n",
    "    max_gpt_calls_per_day: int = 1000\n",
    "    gpt_cost_limit: float = 10.0\n",
    "    \n",
    "config = PipelineConfig()\n",
    "'''\n",
    "\n",
    "with open('src/config/pipeline_config.py', 'w') as f:\n",
    "    f.write(config_code)\n",
    "\n",
    "print(\"Configuration created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf61f4",
   "metadata": {},
   "source": [
    "## 5. Constants and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b648a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constants\n",
    "constants_code = '''\n",
    "POLICY_CATEGORIES = {\n",
    "    'NO_ADS': 'No_Ads',\n",
    "    'IRRELEVANT': 'Irrelevant', \n",
    "    'RANT_NO_VISIT': 'Rant_No_Visit',\n",
    "    'NONE': 'None'\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    'APPROVE': 'APPROVE',\n",
    "    'REJECT': 'REJECT'\n",
    "}\n",
    "\n",
    "GPT_PRICING = {\n",
    "    \"gpt-3.5-turbo\": 0.002,\n",
    "    \"gpt-4\": 0.03,\n",
    "    \"gpt-4-turbo\": 0.01,\n",
    "    \"gpt-4o\": 0.005\n",
    "}\n",
    "\n",
    "ZERO_SHOT_LABELS = [\n",
    "    \"an advertisement or promotional solicitation\",\n",
    "    \"off-topic or unrelated content\", \n",
    "    \"a generic negative rant without evidence\",\n",
    "    \"a relevant review of an actual visit\"\n",
    "]\n",
    "'''\n",
    "\n",
    "with open('src/core/constants.py', 'w') as f:\n",
    "    f.write(constants_code)\n",
    "\n",
    "# Create prompt templates\n",
    "prompts_code = '''\n",
    "NO_ADS_SYSTEM = \"\"\"You are analyzing Google reviews to detect advertising/promotional content.\n",
    "Classify as REJECT if the review contains promotional solicitations, referral codes, or advertising.\n",
    "Classify as APPROVE if it appears to be a genuine review experience.\"\"\"\n",
    "\n",
    "IRRELEVANT_SYSTEM = \"\"\"You are analyzing Google reviews to detect off-topic content.\n",
    "Classify as REJECT if the review is unrelated to the business or contains irrelevant content.\n",
    "Classify as APPROVE if it discusses the actual business or service.\"\"\"\n",
    "\n",
    "RANT_NO_VISIT_SYSTEM = \"\"\"You are analyzing Google reviews to detect generic negative rants.\n",
    "Classify as REJECT if the review appears to be a generic complaint without evidence of visiting.\n",
    "Classify as APPROVE if it shows evidence of an actual experience.\"\"\"\n",
    "\n",
    "def build_prompt(system_prompt, review_text, few_shots=None):\n",
    "    \"\"\"Build a complete prompt for classification\"\"\"\n",
    "    prompt = f\"{system_prompt}\\n\\nReview: {review_text}\\n\\nRespond with JSON: {{\\\"label\\\": \\\"APPROVE/REJECT\\\", \\\"rationale\\\": \\\"reason\\\", \\\"confidence\\\": 0.8}}\"\n",
    "    return prompt\n",
    "'''\n",
    "\n",
    "with open('prompts/policy_prompts.py', 'w') as f:\n",
    "    f.write(prompts_code)\n",
    "\n",
    "print(\"Constants and prompts created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015eaed",
   "metadata": {},
   "source": [
    "## 6. API Key Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API key (replace with your actual key)\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Option 1: Use Colab secrets (recommended)\n",
    "try:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"âœ… OpenAI API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Option 2: Manual input (less secure)\n",
    "    import getpass\n",
    "    OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    print(\"âœ… OpenAI API key entered manually\")\n",
    "\n",
    "# Set environment variable\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print(\"\\n To add API key to Colab secrets:\")\n",
    "print(\"1. Click the ðŸ”‘ key icon in the left sidebar\")\n",
    "print(\"2. Add new secret: OPENAI_API_KEY\")\n",
    "print(\"3. Restart runtime and run this cell again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255cad8",
   "metadata": {},
   "source": [
    "## 7. HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace-based classification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "def run_hf_pipeline(df, output_file=\"results/predictions/predictions_hf.csv\"):\n",
    "    \"\"\"Run HuggingFace pipeline classification\"\"\"\n",
    "    print(\"Loading HuggingFace models...\")\n",
    "    \n",
    "    # Load models\n",
    "    sentiment_classifier = pipeline(\n",
    "        \"sentiment-analysis\", \n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    zero_shot_classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"facebook/bart-large-mnli\", \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        \n",
    "        # Zero-shot classification\n",
    "        labels = [\n",
    "            \"advertisement or promotion\",\n",
    "            \"off-topic content\",\n",
    "            \"negative rant without visit\",\n",
    "            \"genuine review\"\n",
    "        ]\n",
    "        \n",
    "        zs_result = zero_shot_classifier(text, labels)\n",
    "        top_label = zs_result['labels'][0]\n",
    "        confidence = zs_result['scores'][0]\n",
    "        \n",
    "        # Determine final classification\n",
    "        if top_label == \"genuine review\" and confidence > 0.7:\n",
    "            pred_label = \"APPROVE\"\n",
    "            pred_category = \"None\"\n",
    "        else:\n",
    "            pred_label = \"REJECT\"\n",
    "            if \"advertisement\" in top_label:\n",
    "                pred_category = \"No_Ads\"\n",
    "            elif \"off-topic\" in top_label:\n",
    "                pred_category = \"Irrelevant\"\n",
    "            else:\n",
    "                pred_category = \"Rant_No_Visit\"\n",
    "        \n",
    "        results.append({\n",
    "            'id': row['id'],\n",
    "            'text': text,\n",
    "            'pred_label': pred_label,\n",
    "            'pred_category': pred_category,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"HuggingFace results saved to {output_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Load sample data and run HF pipeline\n",
    "df = pd.read_csv('data/sample/sample_reviews.csv')\n",
    "hf_results = run_hf_pipeline(df)\n",
    "print(\"\\nðŸ“Š HuggingFace Results:\")\n",
    "print(hf_results[['id', 'text', 'pred_label', 'pred_category']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351199ee",
   "metadata": {},
   "source": [
    "## 8. GPT-Based Classification (Pseudo-Labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-based classification and pseudo-labeling\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "def classify_with_gpt(text, system_prompt):\n",
    "    \"\"\"Classify a single review using GPT\"\"\"\n",
    "    prompt = f\"{system_prompt}\\n\\nReview: {text}\\n\\nRespond with JSON: {{\\\"label\\\": \\\"APPROVE/REJECT\\\", \\\"rationale\\\": \\\"reason\\\", \\\"confidence\\\": 0.8}}\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes reviews.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            return json.loads(result_text)\n",
    "        except:\n",
    "            # Fallback parsing\n",
    "            if \"REJECT\" in result_text.upper():\n",
    "                return {\"label\": \"REJECT\", \"rationale\": \"GPT classified as reject\", \"confidence\": 0.8}\n",
    "            else:\n",
    "                return {\"label\": \"APPROVE\", \"rationale\": \"GPT classified as approve\", \"confidence\": 0.8}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with GPT API: {e}\")\n",
    "        return {\"label\": \"APPROVE\", \"rationale\": \"API error\", \"confidence\": 0.0}\n",
    "\n",
    "def run_gpt_pipeline(df, output_file=\"results/predictions/predictions_gpt.csv\"):\n",
    "    \"\"\"Run GPT-based classification pipeline\"\"\"\n",
    "    print(\"Running GPT classification...\")\n",
    "    \n",
    "    # Define policy prompts\n",
    "    prompts = {\n",
    "        \"No_Ads\": \"\"\"Analyze this review for advertising/promotional content. \n",
    "        Classify as REJECT if it contains promotional solicitations, referral codes, or advertising.\n",
    "        Classify as APPROVE if it appears to be a genuine review.\"\"\",\n",
    "        \n",
    "        \"Irrelevant\": \"\"\"Analyze this review for relevance to the business.\n",
    "        Classify as REJECT if it's off-topic or unrelated to the business.\n",
    "        Classify as APPROVE if it discusses the actual business.\"\"\",\n",
    "        \n",
    "        \"Rant_No_Visit\": \"\"\"Analyze this review for evidence of an actual visit.\n",
    "        Classify as REJECT if it appears to be a generic rant without visiting.\n",
    "        Classify as APPROVE if it shows evidence of an actual experience.\"\"\"\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"GPT Classification\"):\n",
    "        text = row['text']\n",
    "        violations = []\n",
    "        \n",
    "        # Check each policy\n",
    "        for policy, prompt in prompts.items():\n",
    "            result = classify_with_gpt(text, prompt)\n",
    "            if result['label'] == 'REJECT':\n",
    "                violations.append((policy, result['confidence']))\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "        \n",
    "        # Determine final classification\n",
    "        if violations:\n",
    "            # Choose violation with highest confidence\n",
    "            best_violation = max(violations, key=lambda x: x[1])\n",
    "            pred_label = \"REJECT\"\n",
    "            pred_category = best_violation[0]\n",
    "        else:\n",
    "            pred_label = \"APPROVE\"\n",
    "            pred_category = \"None\"\n",
    "        \n",
    "        results.append({\n",
    "            'id': row['id'],\n",
    "            'text': text,\n",
    "            'pred_label': pred_label,\n",
    "            'pred_category': pred_category,\n",
    "            'confidence': 0.8\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"GPT results saved to {output_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run GPT pipeline\n",
    "gpt_results = run_gpt_pipeline(df)\n",
    "print(\"\\nGPT Results:\")\n",
    "print(gpt_results[['id', 'text', 'pred_label', 'pred_category']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d961c",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e21722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate_predictions(gold_df, pred_df, pipeline_name):\n",
    "    \"\"\"Evaluate predictions against gold standard\"\"\"\n",
    "    merged = gold_df.merge(pred_df[['id', 'pred_label', 'pred_category']], on='id')\n",
    "    \n",
    "    print(f\"\\n {pipeline_name} Evaluation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Binary classification (APPROVE/REJECT)\n",
    "    print(\"\\n APPROVE/REJECT Classification:\")\n",
    "    accuracy = accuracy_score(merged['gold_label'], merged['pred_label'])\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(merged['gold_label'], merged['pred_label']))\n",
    "    \n",
    "    # Category classification (for REJECT cases)\n",
    "    reject_cases = merged[merged['gold_label'] == 'REJECT']\n",
    "    if len(reject_cases) > 0:\n",
    "        print(\"\\n Category Classification (REJECT cases only):\")\n",
    "        cat_accuracy = accuracy_score(reject_cases['gold_category'], reject_cases['pred_category'])\n",
    "        print(f\"Category Accuracy: {cat_accuracy:.2%}\")\n",
    "        print(\"\\nCategory Report:\")\n",
    "        print(classification_report(reject_cases['gold_category'], reject_cases['pred_category']))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Load gold standard\n",
    "gold_df = pd.read_csv('data/sample/sample_reviews.csv')\n",
    "\n",
    "# Evaluate both pipelines\n",
    "hf_accuracy = evaluate_predictions(gold_df, hf_results, \"HuggingFace\")\n",
    "gpt_accuracy = evaluate_predictions(gold_df, gpt_results, \"GPT\")\n",
    "\n",
    "print(f\"\\n FINAL COMPARISON:\")\n",
    "print(f\"HuggingFace Accuracy: {hf_accuracy:.2%}\")\n",
    "print(f\"GPT Accuracy: {gpt_accuracy:.2%}\")\n",
    "print(f\"Best Pipeline: {'HuggingFace' if hf_accuracy > gpt_accuracy else 'GPT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ac73c",
   "metadata": {},
   "source": [
    "## 10. Ensemble Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble classification combining HF and GPT\n",
    "def run_ensemble_pipeline(df, hf_results, gpt_results, tau=0.55):\n",
    "    \"\"\"Combine HuggingFace and GPT predictions\"\"\"\n",
    "    print(f\"Running Ensemble with tau={tau}\")\n",
    "    \n",
    "    ensemble_results = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        review_id = row['id']\n",
    "        \n",
    "        # Get predictions from both pipelines\n",
    "        hf_pred = hf_results[hf_results['id'] == review_id].iloc[0]\n",
    "        gpt_pred = gpt_results[gpt_results['id'] == review_id].iloc[0]\n",
    "        \n",
    "        # Agreement check\n",
    "        if hf_pred['pred_label'] == gpt_pred['pred_label']:\n",
    "            # Both agree\n",
    "            final_label = hf_pred['pred_label']\n",
    "            final_category = hf_pred['pred_category']\n",
    "            confidence = 0.9  # High confidence when both agree\n",
    "        else:\n",
    "            # Disagreement - use HF model with high confidence threshold\n",
    "            if hf_pred['confidence'] > tau:\n",
    "                final_label = hf_pred['pred_label']\n",
    "                final_category = hf_pred['pred_category']\n",
    "            else:\n",
    "                final_label = gpt_pred['pred_label']\n",
    "                final_category = gpt_pred['pred_category']\n",
    "            confidence = 0.6  # Lower confidence on disagreement\n",
    "        \n",
    "        ensemble_results.append({\n",
    "            'id': review_id,\n",
    "            'text': row['text'],\n",
    "            'pred_label': final_label,\n",
    "            'pred_category': final_category,\n",
    "            'confidence': confidence,\n",
    "            'hf_label': hf_pred['pred_label'],\n",
    "            'gpt_label': gpt_pred['pred_label'],\n",
    "            'agreement': hf_pred['pred_label'] == gpt_pred['pred_label']\n",
    "        })\n",
    "    \n",
    "    ensemble_df = pd.DataFrame(ensemble_results)\n",
    "    ensemble_df.to_csv('results/predictions/predictions_ensemble.csv', index=False)\n",
    "    \n",
    "    print(f\"Ensemble results saved!\")\n",
    "    print(f\"Agreement rate: {ensemble_df['agreement'].mean():.2%}\")\n",
    "    \n",
    "    return ensemble_df\n",
    "\n",
    "# Run ensemble\n",
    "ensemble_results = run_ensemble_pipeline(df, hf_results, gpt_results)\n",
    "ensemble_accuracy = evaluate_predictions(gold_df, ensemble_results, \"Ensemble\")\n",
    "\n",
    "print(\"\\nEnsemble Results:\")\n",
    "print(ensemble_results[['id', 'text', 'pred_label', 'agreement']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4fdaf",
   "metadata": {},
   "source": [
    "## 11. ðŸ“‹ Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa85c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and comparison\n",
    "print(\"\\nPIPELINE EXECUTION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nFINAL RESULTS SUMMARY:\")\n",
    "print(f\"HuggingFace Accuracy: {hf_accuracy:.2%}\")\n",
    "print(f\"GPT Accuracy: {gpt_accuracy:.2%}\")\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nGenerated Files:\")\n",
    "files = [\n",
    "    'results/predictions/predictions_hf.csv',\n",
    "    'results/predictions/predictions_gpt.csv', \n",
    "    'results/predictions/predictions_ensemble.csv'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file)\n",
    "        print(f\"  {file} ({size} bytes)\")\n",
    "    else:\n",
    "        print(f\"  {file} (not found)\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Download the CSV files to analyze results\")\n",
    "print(\"  2. Upload your own review data to data/sample/\")\n",
    "print(\"  3. Modify thresholds and models for better performance\")\n",
    "print(\"  4. Use this as a pseudo-labeling system for larger datasets\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
