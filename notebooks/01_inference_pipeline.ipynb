{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd1d20d",
   "metadata": {},
   "source": [
    "# Review Classification Pipeline - Inference Only\n",
    "\n",
    "This notebook uses the pre-trained model to classify new reviews without retraining.\n",
    "\n",
    "## Requirements\n",
    "1. Run the complete training pipeline first (00_colab_complete_pipeline.ipynb)\n",
    "2. Place your review data in `data/actual/` directory\n",
    "3. Data should be in CSV or JSON format with 'id' and 'text' columns\n",
    "\n",
    "## What This Does\n",
    "- Loads pre-trained models from `models/saved_models/`\n",
    "- Processes reviews from `data/actual/`\n",
    "- Outputs policy violation predictions\n",
    "- Saves results to `results/inference/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69367005",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "!pip install -q transformers torch pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"✅ Environment setup complete\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f99e80",
   "metadata": {},
   "source": [
    "## 2. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b12581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if trained model exists\n",
    "model_dir = 'models/saved_models'\n",
    "required_files = [\n",
    "    'model_config.json',\n",
    "    'constants.json', \n",
    "    'inference_pipeline.py',\n",
    "    'metadata.json'\n",
    "]\n",
    "\n",
    "print(\"CHECKING TRAINED MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(model_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"✅ {file}\")\n",
    "    else:\n",
    "        print(f\"❌ {file}\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n❌ ERROR: Missing required model files\")\n",
    "    print(f\"Missing: {missing_files}\")\n",
    "    print(f\"\\nPlease run the training notebook first:\")\n",
    "    print(f\"1. Open 00_colab_complete_pipeline.ipynb\")\n",
    "    print(f\"2. Run all cells to train the pipeline\")\n",
    "    print(f\"3. Run the model persistence cell\")\n",
    "    print(f\"4. Then return to this inference notebook\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    print(f\"\\n✅ All required model files found!\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(os.path.join(model_dir, 'metadata.json'), 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nModel Information:\")\n",
    "    print(f\"   Version: {metadata['version']}\")\n",
    "    print(f\"   Trained: {metadata['timestamp']}\")\n",
    "    print(f\"   Training size: {metadata['training_data_size']} reviews\")\n",
    "    if 'model_performance' in metadata:\n",
    "        perf = metadata['model_performance']\n",
    "        print(f\"   Performance: {perf.get('average_confidence', 'N/A')} avg confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fa2e1",
   "metadata": {},
   "source": [
    "## 3. Load Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the saved pipeline functions\n",
    "sys.path.append(model_dir)\n",
    "from inference_pipeline import process_reviews, load_constants, load_hf_pipelines\n",
    "\n",
    "print(\"LOADING INFERENCE PIPELINE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    # Test load the models to make sure everything works\n",
    "    model_config_path = os.path.join(model_dir, 'model_config.json')\n",
    "    constants_path = os.path.join(model_dir, 'constants.json')\n",
    "    \n",
    "    # Load configuration\n",
    "    constants = load_constants(constants_path)\n",
    "    \n",
    "    print(f\"✅ Constants loaded\")\n",
    "    print(f\"   Policy categories: {len(constants['POLICY_CATEGORIES'])}\")\n",
    "    print(f\"   Zero-shot labels: {len(constants['ZERO_SHOT_LABELS'])}\")\n",
    "    \n",
    "    # Load models (this might take a moment)\n",
    "    print(f\"\\nLoading HuggingFace models...\")\n",
    "    sentiment, toxic, zshot, tau = load_hf_pipelines(model_config_path)\n",
    "    \n",
    "    print(f\"✅ Models loaded successfully\")\n",
    "    print(f\"   Confidence threshold: {tau}\")\n",
    "    print(f\"   Ready for inference!\")\n",
    "    \n",
    "    models_loaded = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading models: {e}\")\n",
    "    print(f\"The training pipeline may need to be run again\")\n",
    "    models_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42dab9",
   "metadata": {},
   "source": [
    "## 4. Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for input data in data/actual directory\n",
    "input_dir = 'data/actual'\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "print(\"LOADING INPUT DATA\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Look for CSV and JSON files\n",
    "input_files = []\n",
    "if os.path.exists(input_dir):\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(('.csv', '.json')):\n",
    "            input_files.append(file)\n",
    "\n",
    "print(f\"Available input files in {input_dir}:\")\n",
    "for file in input_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"   {file} ({file_size} bytes)\")\n",
    "\n",
    "if not input_files:\n",
    "    print(f\"❌ No input files found in {input_dir}\")\n",
    "    print(f\"\\nTo add your review data:\")\n",
    "    print(f\"1. Create a CSV file with columns: 'id', 'text'\")\n",
    "    print(f\"2. Or create a JSON file with array of objects: [{'id': 1, 'text': 'review text'}, ...]\")\n",
    "    print(f\"3. Place the file in {input_dir}\")\n",
    "    print(f\"4. Re-run this cell\")\n",
    "    print(f\"\\nExample files are already created for you to test with.\")\n",
    "\n",
    "# Let user choose which file to process\n",
    "if input_files:\n",
    "    print(f\"\\nChoose a file to process:\")\n",
    "    for i, file in enumerate(input_files):\n",
    "        print(f\"   {i+1}. {file}\")\n",
    "    \n",
    "    # For demo, automatically use the first file\n",
    "    # In practice, you might want to manually specify the file\n",
    "    selected_file = input_files[0]\n",
    "    print(f\"\\nUsing: {selected_file}\")\n",
    "    \n",
    "    # Load the selected file\n",
    "    file_path = os.path.join(input_dir, selected_file)\n",
    "    \n",
    "    try:\n",
    "        if selected_file.endswith('.csv'):\n",
    "            input_data = pd.read_csv(file_path)\n",
    "        elif selected_file.endswith('.json'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            input_data = pd.DataFrame(json_data)\n",
    "        \n",
    "        # Validate required columns\n",
    "        if 'text' not in input_data.columns:\n",
    "            print(f\"❌ Missing required 'text' column\")\n",
    "            print(f\"Available columns: {list(input_data.columns)}\")\n",
    "            input_data = None\n",
    "        else:\n",
    "            # Add ID column if missing\n",
    "            if 'id' not in input_data.columns:\n",
    "                input_data['id'] = range(1, len(input_data) + 1)\n",
    "            \n",
    "            print(f\"✅ Data loaded successfully\")\n",
    "            print(f\"   Reviews to process: {len(input_data)}\")\n",
    "            print(f\"   Columns: {list(input_data.columns)}\")\n",
    "            \n",
    "            # Show preview\n",
    "            print(f\"\\nData Preview:\")\n",
    "            for idx, row in input_data.head(3).iterrows():\n",
    "                text_preview = str(row['text'])[:60] + \"...\" if len(str(row['text'])) > 60 else str(row['text'])\n",
    "                print(f\"   ID {row['id']}: {text_preview}\")\n",
    "            \n",
    "            if len(input_data) > 3:\n",
    "                print(f\"   ... and {len(input_data) - 3} more reviews\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading file: {e}\")\n",
    "        input_data = None\n",
    "else:\n",
    "    input_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63525ac",
   "metadata": {},
   "source": [
    "## 5. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_loaded and input_data is not None and len(input_data) > 0:\n",
    "    \n",
    "    print(\"RUNNING INFERENCE\")\n",
    "    print(\"=\"*20)\n",
    "    print(f\"Processing {len(input_data)} reviews...\")\n",
    "    \n",
    "    try:\n",
    "        # Run the inference\n",
    "        results = process_reviews(input_data, model_dir)\n",
    "        \n",
    "        print(f\"✅ Inference completed!\")\n",
    "        print(f\"   Processed: {len(results)} reviews\")\n",
    "        \n",
    "        # Create results directory\n",
    "        results_dir = 'results/inference'\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = f\"inference_results_{timestamp}.csv\"\n",
    "        results_path = os.path.join(results_dir, results_file)\n",
    "        \n",
    "        results.to_csv(results_path, index=False)\n",
    "        \n",
    "        print(f\"\\nRESULTS SUMMARY\")\n",
    "        print(\"=\"*20)\n",
    "        \n",
    "        # Summary statistics\n",
    "        approve_count = len(results[results['pred_label'] == 'APPROVE'])\n",
    "        reject_count = len(results[results['pred_label'] == 'REJECT'])\n",
    "        avg_confidence = results['confidence'].mean()\n",
    "        \n",
    "        print(f\"Total reviews: {len(results)}\")\n",
    "        print(f\"APPROVE: {approve_count} ({approve_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"REJECT: {reject_count} ({reject_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Average confidence: {avg_confidence:.3f}\")\n",
    "        \n",
    "        # Category breakdown for rejected reviews\n",
    "        if reject_count > 0:\n",
    "            print(f\"\\nREJECT Categories:\")\n",
    "            reject_categories = results[results['pred_label'] == 'REJECT']['pred_category'].value_counts()\n",
    "            for category, count in reject_categories.items():\n",
    "                print(f\"   {category}: {count} reviews\")\n",
    "        \n",
    "        # Show detailed results\n",
    "        print(f\"\\nDETAILED RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        display_df = results.copy()\n",
    "        # Truncate text for display\n",
    "        display_df['text'] = display_df['text'].apply(lambda x: x[:50] + \"...\" if len(x) > 50 else x)\n",
    "        \n",
    "        display_cols = ['id', 'text', 'pred_label', 'pred_category', 'confidence']\n",
    "        print(display_df[display_cols].to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n✅ Results saved to: {results_path}\")\n",
    "        print(f\"\\nSUCCESS: Inference complete!\")\n",
    "        print(f\"Your reviews have been classified for policy violations.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot run inference\")\n",
    "    if not models_loaded:\n",
    "        print(\"   Models not loaded properly\")\n",
    "    if input_data is None or len(input_data) == 0:\n",
    "        print(\"   No input data available\")\n",
    "    \n",
    "    print(f\"\\nPlease check:\")\n",
    "    print(f\"1. Training notebook was run successfully\")\n",
    "    print(f\"2. Input data is placed in data/input/ directory\")\n",
    "    print(f\"3. Input data has required 'text' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c18b38",
   "metadata": {},
   "source": [
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analysis of results (if available)\n",
    "if 'results' in locals() and len(results) > 0:\n",
    "    \n",
    "    print(\"ADVANCED RESULTS ANALYSIS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Confidence distribution\n",
    "    high_conf = results[results['confidence'] >= 0.8]\n",
    "    medium_conf = results[(results['confidence'] >= 0.6) & (results['confidence'] < 0.8)]\n",
    "    low_conf = results[results['confidence'] < 0.6]\n",
    "    \n",
    "    print(f\"Confidence Distribution:\")\n",
    "    print(f\"   High (≥0.8): {len(high_conf)} reviews ({len(high_conf)/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Medium (0.6-0.8): {len(medium_conf)} reviews ({len(medium_conf)/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Low (<0.6): {len(low_conf)} reviews ({len(low_conf)/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # Policy violations by type\n",
    "    print(f\"\\nPolicy Violation Types:\")\n",
    "    category_counts = results['pred_category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = count / len(results) * 100\n",
    "        status = \"Policy Violation\" if category != \"None\" else \"Clean Review\"\n",
    "        print(f\"   {category}: {count} reviews ({percentage:.1f}%) - {status}\")\n",
    "    \n",
    "    # Flag high-risk reviews\n",
    "    high_risk = results[\n",
    "        (results['pred_label'] == 'REJECT') & \n",
    "        (results['confidence'] >= 0.8)\n",
    "    ]\n",
    "    \n",
    "    if len(high_risk) > 0:\n",
    "        print(f\"\\nHIGH-RISK REVIEWS (High confidence violations):\")\n",
    "        for idx, row in high_risk.iterrows():\n",
    "            text_preview = row['text'][:60] + \"...\" if len(row['text']) > 60 else row['text']\n",
    "            print(f\"   ID {row['id']}: {row['pred_category']} ({row['confidence']:.3f}) - {text_preview}\")\n",
    "    \n",
    "    # Export summary report\n",
    "    summary_report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_reviews': len(results),\n",
    "        'approve_count': len(results[results['pred_label'] == 'APPROVE']),\n",
    "        'reject_count': len(results[results['pred_label'] == 'REJECT']),\n",
    "        'average_confidence': float(results['confidence'].mean()),\n",
    "        'high_confidence_count': len(high_conf),\n",
    "        'category_breakdown': category_counts.to_dict(),\n",
    "        'high_risk_reviews': len(high_risk)\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(results_dir, f\"summary_report_{timestamp}.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ Summary report saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nINFERENCE COMPLETE\")\n",
    "    print(f\"Files created:\")\n",
    "    print(f\"   {results_path} - Detailed results\")\n",
    "    print(f\"   {summary_path} - Summary report\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results available for analysis\")\n",
    "    print(\"Run the inference cell first to generate results\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
