{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd1d20d",
   "metadata": {},
   "source": [
    "# Review Classification Pipeline - Inference with HuggingFace Model\n",
    "\n",
    "This notebook uses the fine-tuned HuggingFace model to classify new reviews without retraining.\n",
    "\n",
    "## Requirements\n",
    "1. Run the complete training pipeline first (00_colab_complete_pipeline.ipynb)\n",
    "2. Ensure trained model exists in `models/saved_models/review_classifier_*/`\n",
    "3. Place your review data in `data/actual/` directory\n",
    "4. Data should be in CSV or JSON format with 'id' and 'text' columns\n",
    "\n",
    "## What This Does\n",
    "- Loads fine-tuned HuggingFace model from `models/saved_models/`\n",
    "- Loads auxiliary models (toxicity detection, zero-shot classification)\n",
    "- Combines custom ML predictions with policy violation detection\n",
    "- Processes reviews from `data/actual/`\n",
    "- Outputs comprehensive classification results\n",
    "- Saves results to `results/inference/`\n",
    "\n",
    "## Model Architecture\n",
    "- **Fine-tuned Model**: Custom HuggingFace transformer for review classification\n",
    "- **Policy Detection**: Zero-shot + toxicity analysis for specific violations\n",
    "- **Dual-layer**: Both models vote; REJECT if either flags the review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69367005",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "!pip install -q transformers torch pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f99e80",
   "metadata": {},
   "source": [
    "## 2. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b12581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if trained HuggingFace model exists\n",
    "model_dir = 'models/saved_models'\n",
    "\n",
    "print(\"CHECKING TRAINED MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Look for HuggingFace model directories (timestamped folders)\n",
    "model_folders = []\n",
    "if os.path.exists(model_dir):\n",
    "    for item in os.listdir(model_dir):\n",
    "        item_path = os.path.join(model_dir, item)\n",
    "        if os.path.isdir(item_path) and item.startswith('review_classifier_'):\n",
    "            # Check if it contains HuggingFace model files\n",
    "            hf_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json']\n",
    "            model_files = ['model.safetensors', 'pytorch_model.bin']  # Either format\n",
    "            \n",
    "            has_config = all(os.path.exists(os.path.join(item_path, f)) for f in hf_files)\n",
    "            has_model = any(os.path.exists(os.path.join(item_path, f)) for f in model_files)\n",
    "            \n",
    "            if has_config and has_model:\n",
    "                model_folders.append(item)\n",
    "\n",
    "if not model_folders:\n",
    "    print(f\"‚ùå ERROR: No trained HuggingFace model found in {model_dir}\")\n",
    "    print(f\"\\nPlease run the training notebook first:\")\n",
    "    print(f\"1. Open 00_colab_complete_pipeline.ipynb\")\n",
    "    print(f\"2. Run all cells to train the model\")\n",
    "    print(f\"3. Ensure model is saved to models/saved_models/\")\n",
    "    print(f\"4. Then return to this inference notebook\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    # Use the most recent model (last in alphabetical order = most recent timestamp)\n",
    "    latest_model = sorted(model_folders)[-1]\n",
    "    model_path = os.path.join(model_dir, latest_model)\n",
    "    \n",
    "    print(f\"‚úÖ Found trained model: {latest_model}\")\n",
    "    print(f\"   Model path: {model_path}\")\n",
    "    \n",
    "    # Check model files\n",
    "    model_files = os.listdir(model_path)\n",
    "    print(f\"   Model files: {', '.join(model_files)}\")\n",
    "    \n",
    "    # Load latest_config.json if it exists for metadata\n",
    "    config_file = os.path.join(model_dir, 'latest_config.json')\n",
    "    if os.path.exists(config_file):\n",
    "        with open(config_file, 'r') as f:\n",
    "            config_metadata = json.load(f)\n",
    "        print(f\"\\nModel Information:\")\n",
    "        print(f\"   Training mode: {config_metadata.get('training_mode', 'Unknown')}\")\n",
    "        print(f\"   Timestamp: {config_metadata.get('timestamp', 'Unknown')}\")\n",
    "        print(f\"   Confidence threshold: {config_metadata.get('confidence_threshold', 0.55)}\")\n",
    "    else:\n",
    "        print(f\"\\nUsing model: {latest_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fa2e1",
   "metadata": {},
   "source": [
    "## 3. Load Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace trained model and constants\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"LOADING TRAINED MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    # Load the fine-tuned HuggingFace model\n",
    "    print(f\"Loading fine-tuned model from: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    print(f\"‚úÖ Fine-tuned model loaded successfully\")\n",
    "    print(f\"   Model type: {model.config.model_type}\")\n",
    "    print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Load auxiliary models for policy detection\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"   Device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "    \n",
    "    print(f\"\\nLoading auxiliary models...\")\n",
    "    toxic_pipeline = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", top_k=None, device=device)\n",
    "    zshot_pipeline = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "    \n",
    "    print(f\"‚úÖ Auxiliary models loaded\")\n",
    "    \n",
    "    # Define constants (from training pipeline)\n",
    "    POLICY_CATEGORIES = {\n",
    "        'NO_ADS': 'No_Ads',\n",
    "        'IRRELEVANT': 'Irrelevant', \n",
    "        'RANT_NO_VISIT': 'Rant_No_Visit',\n",
    "        'NONE': 'None'\n",
    "    }\n",
    "    \n",
    "    LABELS = {\n",
    "        'APPROVE': 'APPROVE',\n",
    "        'REJECT': 'REJECT'\n",
    "    }\n",
    "    \n",
    "    ZERO_SHOT_LABELS = [\n",
    "        \"an advertisement or promotional solicitation for this business (promo code, referral, links, contact to buy)\",\n",
    "        \"off-topic or unrelated to this business (e.g., politics, crypto, chain messages, personal stories not about this place)\",\n",
    "        \"a generic negative rant about this business without evidence of a visit (short insults, 'scam', 'overpriced', 'worst ever')\",\n",
    "        \"a relevant on-topic description of a visit or experience at this business\"\n",
    "    ]\n",
    "    \n",
    "    ZERO_SHOT_TO_POLICY = {\n",
    "        ZERO_SHOT_LABELS[0]: POLICY_CATEGORIES['NO_ADS'],\n",
    "        ZERO_SHOT_LABELS[1]: POLICY_CATEGORIES['IRRELEVANT'],\n",
    "        ZERO_SHOT_LABELS[2]: POLICY_CATEGORIES['RANT_NO_VISIT'],\n",
    "        ZERO_SHOT_LABELS[3]: POLICY_CATEGORIES['NONE']\n",
    "    }\n",
    "    \n",
    "    # Get confidence threshold from config or use default\n",
    "    confidence_threshold = config_metadata.get('confidence_threshold', 0.55) if 'config_metadata' in locals() else 0.55\n",
    "    \n",
    "    print(f\"‚úÖ Constants loaded\")\n",
    "    print(f\"   Policy categories: {len(POLICY_CATEGORIES)}\")\n",
    "    print(f\"   Zero-shot labels: {len(ZERO_SHOT_LABELS)}\")\n",
    "    print(f\"   Confidence threshold: {confidence_threshold}\")\n",
    "    print(f\"\\nüöÄ Ready for inference!\")\n",
    "    \n",
    "    models_loaded = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading models: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"1. Ensure the training notebook completed successfully\")\n",
    "    print(f\"2. Check that model files exist in: {model_path}\")\n",
    "    print(f\"3. Verify internet connection for downloading auxiliary models\")\n",
    "    models_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42dab9",
   "metadata": {},
   "source": [
    "## 4. Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for input data in data/actual directory\n",
    "input_dir = 'data/actual'\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "print(\"LOADING INPUT DATA\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Look for CSV and JSON files\n",
    "input_files = []\n",
    "if os.path.exists(input_dir):\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(('.csv', '.json')):\n",
    "            input_files.append(file)\n",
    "\n",
    "print(f\"Available input files in {input_dir}:\")\n",
    "for file in input_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"   {file} ({file_size} bytes)\")\n",
    "\n",
    "if not input_files:\n",
    "    print(f\"‚ùå No input files found in {input_dir}\")\n",
    "    print(f\"\\nTo add your review data:\")\n",
    "    print(f\"1. Create a CSV file with columns: 'id', 'text'\")\n",
    "    print(f\"2. Or create a JSON file with array of objects: [{'id': 1, 'text': 'review text'}, ...]\")\n",
    "    print(f\"3. Place the file in {input_dir}\")\n",
    "    print(f\"4. Re-run this cell\")\n",
    "    print(f\"\\nExample files are already created for you to test with.\")\n",
    "\n",
    "# Let user choose which file to process\n",
    "if input_files:\n",
    "    print(f\"\\nChoose a file to process:\")\n",
    "    for i, file in enumerate(input_files):\n",
    "        print(f\"   {i+1}. {file}\")\n",
    "    \n",
    "    # For demo, automatically use the first file\n",
    "    # In practice, you might want to manually specify the file\n",
    "    selected_file = input_files[0]\n",
    "    print(f\"\\nUsing: {selected_file}\")\n",
    "    \n",
    "    # Load the selected file\n",
    "    file_path = os.path.join(input_dir, selected_file)\n",
    "    \n",
    "    try:\n",
    "        if selected_file.endswith('.csv'):\n",
    "            input_data = pd.read_csv(file_path)\n",
    "        elif selected_file.endswith('.json'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            input_data = pd.DataFrame(json_data)\n",
    "        \n",
    "        # Validate required columns\n",
    "        if 'text' not in input_data.columns:\n",
    "            print(f\"‚ùå Missing required 'text' column\")\n",
    "            print(f\"Available columns: {list(input_data.columns)}\")\n",
    "            input_data = None\n",
    "        else:\n",
    "            # Add ID column if missing\n",
    "            if 'id' not in input_data.columns:\n",
    "                input_data['id'] = range(1, len(input_data) + 1)\n",
    "            \n",
    "            print(f\"‚úÖ Data loaded successfully\")\n",
    "            print(f\"   Reviews to process: {len(input_data)}\")\n",
    "            print(f\"   Columns: {list(input_data.columns)}\")\n",
    "            \n",
    "            # Show preview\n",
    "            print(f\"\\nData Preview:\")\n",
    "            for idx, row in input_data.head(3).iterrows():\n",
    "                text_preview = str(row['text'])[:60] + \"...\" if len(str(row['text'])) > 60 else str(row['text'])\n",
    "                print(f\"   ID {row['id']}: {text_preview}\")\n",
    "            \n",
    "            if len(input_data) > 3:\n",
    "                print(f\"   ... and {len(input_data) - 3} more reviews\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        input_data = None\n",
    "else:\n",
    "    input_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63525ac",
   "metadata": {},
   "source": [
    "## 5. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inference functions\n",
    "def predict_with_custom_model(text, model, tokenizer):\n",
    "    \"\"\"Predict using fine-tuned model\"\"\"\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        conf = float(probs.max())\n",
    "        pred = int(probs.argmax())\n",
    "    return (\"REJECT\" if pred == 1 else \"APPROVE\"), conf\n",
    "\n",
    "def tox_top_label(tox_output):\n",
    "    \"\"\"Extract top toxicity label and score\"\"\"\n",
    "    try:\n",
    "        if isinstance(tox_output, dict):\n",
    "            candidates = [tox_output]\n",
    "        elif isinstance(tox_output, list):\n",
    "            if len(tox_output) and isinstance(tox_output[0], dict):\n",
    "                candidates = tox_output\n",
    "            elif len(tox_output) and isinstance(tox_output[0], list):\n",
    "                candidates = tox_output[0]\n",
    "            else:\n",
    "                candidates = []\n",
    "        else:\n",
    "            candidates = []\n",
    "        if not candidates:\n",
    "            return \"NONE\", 0.0\n",
    "        best = max(candidates, key=lambda d: float(d.get(\"score\", 0.0)))\n",
    "        return best.get(\"label\", \"NONE\"), float(best.get(\"score\", 0.0))\n",
    "    except Exception:\n",
    "        return \"NONE\", 0.0\n",
    "\n",
    "def policy_zero_shot_fused(zshot, toxic, text, tau_irrelevant=0.55, tau_rant=0.55, tau_ads=0.70, tox_tau=0.50):\n",
    "    \"\"\"Unified policy detection using zero-shot + toxicity\"\"\"\n",
    "    # Zero-shot classification\n",
    "    zs_res = zshot(text, candidate_labels=ZERO_SHOT_LABELS, \n",
    "                   hypothesis_template=\"This review is {}.\", multi_label=True)\n",
    "    zs = {lab: float(scr) for lab, scr in zip(zs_res[\"labels\"], zs_res[\"scores\"])}\n",
    "    \n",
    "    ads = zs.get(ZERO_SHOT_LABELS[0], 0.0)\n",
    "    irr = zs.get(ZERO_SHOT_LABELS[1], 0.0)\n",
    "    rant = zs.get(ZERO_SHOT_LABELS[2], 0.0)\n",
    "    \n",
    "    # Toxicity gate\n",
    "    tox_label, tox_score = tox_top_label(toxic(text))\n",
    "    \n",
    "    TOX_TO_RANT = {\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\"}\n",
    "    TOX_TO_IRRELEVANT = {\"identity_hate\"}\n",
    "    \n",
    "    if tox_label and tox_score >= tox_tau:\n",
    "        if tox_label in TOX_TO_RANT:\n",
    "            return LABELS['REJECT'], POLICY_CATEGORIES['RANT_NO_VISIT']\n",
    "        if tox_label in TOX_TO_IRRELEVANT:\n",
    "            return LABELS['REJECT'], POLICY_CATEGORIES['IRRELEVANT']\n",
    "    \n",
    "    # Policy thresholds\n",
    "    if max(irr, rant) >= min(tau_irrelevant, tau_rant):\n",
    "        return LABELS['REJECT'], (POLICY_CATEGORIES['IRRELEVANT'] if irr >= rant else POLICY_CATEGORIES['RANT_NO_VISIT'])\n",
    "    \n",
    "    # Ads detection with evidence\n",
    "    import re\n",
    "    AD_PATTERNS = [r\"https?://\", r\"\\bwww\\.\", r\"\\.[a-z]{2,6}\\b\", r\"\\b(?:\\+?\\d[\\s\\-()]*){7,}\\b\",\n",
    "                   r\"\\bpromo(?:\\s*code)?\\b\", r\"\\bdiscount\\b\", r\"\\bcoupon\\b\", r\"\\breferral\\b\",\n",
    "                   r\"\\buse\\s*code\\b\", r\"\\bwhatsapp\\b\", r\"\\bdm\\s+(?:me|us)\\b\"]\n",
    "    AD_REGEX = re.compile(\"|\".join(AD_PATTERNS), flags=re.IGNORECASE)\n",
    "    \n",
    "    has_ads = bool(AD_REGEX.search(text))\n",
    "    ads_margin = 0.10\n",
    "    \n",
    "    if has_ads and (ads >= tau_ads) and (ads >= max(irr, rant) + ads_margin):\n",
    "        return LABELS['REJECT'], POLICY_CATEGORIES['NO_ADS']\n",
    "    \n",
    "    return LABELS['APPROVE'], POLICY_CATEGORIES['NONE']\n",
    "\n",
    "def process_reviews(input_df):\n",
    "    \"\"\"Process reviews using trained model + policy detection\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _, row in input_df.iterrows():\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        # Use fine-tuned model for custom classification\n",
    "        custom_label, custom_conf = predict_with_custom_model(text, model, tokenizer)\n",
    "        \n",
    "        # Use policy detection for specific violations\n",
    "        policy_label, policy_category = policy_zero_shot_fused(\n",
    "            zshot_pipeline, toxic_pipeline, text,\n",
    "            tau_irrelevant=0.55, tau_rant=0.55, tau_ads=0.70, tox_tau=0.50\n",
    "        )\n",
    "        \n",
    "        # Final decision: if either flags as REJECT, it's rejected\n",
    "        if custom_label == 'REJECT' or policy_label == 'REJECT':\n",
    "            final_label = 'REJECT'\n",
    "            if policy_label == 'REJECT':\n",
    "                final_category = policy_category\n",
    "                final_confidence = 0.8  # High confidence for policy violations\n",
    "            else:\n",
    "                final_category = 'Custom_ML_Detected'\n",
    "                final_confidence = custom_conf\n",
    "        else:\n",
    "            final_label = 'APPROVE'\n",
    "            final_category = 'None'\n",
    "            final_confidence = max(custom_conf, 0.7)\n",
    "        \n",
    "        results.append({\n",
    "            'id': row['id'],\n",
    "            'text': text,\n",
    "            'pred_label': final_label,\n",
    "            'pred_category': final_category,\n",
    "            'confidence': round(final_confidence, 4),\n",
    "            'custom_ml_label': custom_label,\n",
    "            'custom_ml_confidence': round(custom_conf, 4),\n",
    "            'policy_label': policy_label,\n",
    "            'policy_category': policy_category\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run inference if everything is ready\n",
    "if models_loaded and input_data is not None and len(input_data) > 0:\n",
    "    \n",
    "    print(\"RUNNING INFERENCE\")\n",
    "    print(\"=\"*20)\n",
    "    print(f\"Processing {len(input_data)} reviews...\")\n",
    "    \n",
    "    try:\n",
    "        # Run the inference\n",
    "        results = process_reviews(input_data)\n",
    "        \n",
    "        print(f\"‚úÖ Inference completed!\")\n",
    "        print(f\"   Processed: {len(results)} reviews\")\n",
    "        \n",
    "        # Create results directory\n",
    "        results_dir = 'results/inference'\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = f\"inference_results_{timestamp}.csv\"\n",
    "        results_path = os.path.join(results_dir, results_file)\n",
    "        \n",
    "        results.to_csv(results_path, index=False)\n",
    "        \n",
    "        print(f\"\\nRESULTS SUMMARY\")\n",
    "        print(\"=\"*20)\n",
    "        \n",
    "        # Summary statistics\n",
    "        approve_count = len(results[results['pred_label'] == 'APPROVE'])\n",
    "        reject_count = len(results[results['pred_label'] == 'REJECT'])\n",
    "        avg_confidence = results['confidence'].mean()\n",
    "        \n",
    "        print(f\"Total reviews: {len(results)}\")\n",
    "        print(f\"APPROVE: {approve_count} ({approve_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"REJECT: {reject_count} ({reject_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Average confidence: {avg_confidence:.3f}\")\n",
    "        \n",
    "        # Category breakdown for rejected reviews\n",
    "        if reject_count > 0:\n",
    "            print(f\"\\nREJECT Categories:\")\n",
    "            reject_categories = results[results['pred_label'] == 'REJECT']['pred_category'].value_counts()\n",
    "            for category, count in reject_categories.items():\n",
    "                print(f\"   {category}: {count} reviews\")\n",
    "        \n",
    "        # Show detailed results\n",
    "        print(f\"\\nDETAILED RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        display_df = results.copy()\n",
    "        # Truncate text for display\n",
    "        display_df['text'] = display_df['text'].apply(lambda x: x[:50] + \"...\" if len(x) > 50 else x)\n",
    "        \n",
    "        display_cols = ['id', 'text', 'pred_label', 'pred_category', 'confidence']\n",
    "        print(display_df[display_cols].to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n‚úÖ Results saved to: {results_path}\")\n",
    "        print(f\"\\nSUCCESS: Inference complete!\")\n",
    "        print(f\"Your reviews have been classified for policy violations.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot run inference\")\n",
    "    if not models_loaded:\n",
    "        print(\"   Models not loaded properly\")\n",
    "    if input_data is None or len(input_data) == 0:\n",
    "        print(\"   No input data available\")\n",
    "    \n",
    "    print(f\"\\nPlease check:\")\n",
    "    print(f\"1. Training notebook was run successfully\")\n",
    "    print(f\"2. Input data is placed in data/actual/ directory\")\n",
    "    print(f\"3. Input data has required 'text' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c18b38",
   "metadata": {},
   "source": [
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analysis of results (if available)\n",
    "if 'results' in locals() and len(results) > 0:\n",
    "    \n",
    "    print(\"ADVANCED RESULTS ANALYSIS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Confidence distribution\n",
    "    high_conf = results[results['confidence'] >= 0.8]\n",
    "    medium_conf = results[(results['confidence'] >= 0.6) & (results['confidence'] < 0.8)]\n",
    "    low_conf = results[results['confidence'] < 0.6]\n",
    "    \n",
    "    print(f\"Confidence Distribution:\")\n",
    "    print(f\"   High (‚â•0.8): {len(high_conf)} reviews ({len(high_conf)/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Medium (0.6-0.8): {len(medium_conf)} reviews ({len(medium_conf)/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Low (<0.6): {len(low_conf)} reviews ({len(low_conf)/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # Policy violations by type\n",
    "    print(f\"\\nPolicy Violation Types:\")\n",
    "    category_counts = results['pred_category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = count / len(results) * 100\n",
    "        status = \"Policy Violation\" if category != \"None\" else \"Clean Review\"\n",
    "        print(f\"   {category}: {count} reviews ({percentage:.1f}%) - {status}\")\n",
    "    \n",
    "    # Flag high-risk reviews\n",
    "    high_risk = results[\n",
    "        (results['pred_label'] == 'REJECT') & \n",
    "        (results['confidence'] >= 0.8)\n",
    "    ]\n",
    "    \n",
    "    if len(high_risk) > 0:\n",
    "        print(f\"\\nHIGH-RISK REVIEWS (High confidence violations):\")\n",
    "        for idx, row in high_risk.iterrows():\n",
    "            text_preview = row['text'][:60] + \"...\" if len(row['text']) > 60 else row['text']\n",
    "            print(f\"   ID {row['id']}: {row['pred_category']} ({row['confidence']:.3f}) - {text_preview}\")\n",
    "    \n",
    "    # Export summary report\n",
    "    summary_report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_reviews': len(results),\n",
    "        'approve_count': len(results[results['pred_label'] == 'APPROVE']),\n",
    "        'reject_count': len(results[results['pred_label'] == 'REJECT']),\n",
    "        'average_confidence': float(results['confidence'].mean()),\n",
    "        'high_confidence_count': len(high_conf),\n",
    "        'category_breakdown': category_counts.to_dict(),\n",
    "        'high_risk_reviews': len(high_risk)\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(results_dir, f\"summary_report_{timestamp}.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Summary report saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nINFERENCE COMPLETE\")\n",
    "    print(f\"Files created:\")\n",
    "    print(f\"   {results_path} - Detailed results\")\n",
    "    print(f\"   {summary_path} - Summary report\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results available for analysis\")\n",
    "    print(\"Run the inference cell first to generate results\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
